{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Latex match Grobid\n",
    "\n",
    "import os\n",
    "import random\n",
    "\n",
    "# fetch data\n",
    "tex_root_path = \"/home/jihuawei/jhw_pdf_parser/grobid_base/datasetgenius/formula/tmp/tex\"\n",
    "tex_folds_rand_sample = random.sample(os.listdir(tex_root_path), 1)\n",
    "\n",
    "\n",
    "xmls = [os.path.join(\"/home/jihuawei/jhw_pdf_parser/grobid_base/datasetgenius/formula/tmp/xmls\", f\"{i}.tei.xml\") for i in tex_folds_rand_sample]\n",
    "# latex = [[os.path.join(f\"/home/jihuawei/jhw_pdf_parser/grobid_base/datasetgenius/formula/tmp/tex/{i}\", j) for j in os.listdir(f\"/home/jihuawei/jhw_pdf_parser/grobid_base/datasetgenius/formula/tmp/tex/{i}\")]for i in tex_folds_rand_sample]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['2306.09649']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tex_folds_rand_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 提取latex种包含公式的句子\n",
    "# 思路：\n",
    "# 获得公式及位于的index\n",
    "# 获得段落句点的index\n",
    "# 看公式落在哪两个句点之间\n",
    "# 提取出来\n",
    "import regex as re\n",
    "from collections import OrderedDict\n",
    "\"\"\"\n",
    "# 找出句点\n",
    "(?<!No)(?<!\\.g)(?<!\\.e)(?<!\\{)\\.(?!g\\.)(?!e\\.)(?!\\d)(?=\\s)\n",
    "# 提取公式\n",
    "(?<!\\\\)(?:((?<!\\$)\\${1,2}(?!\\$))|(\\\\begin\\{equation\\})|(\\\\\\()|(\\\\\\[)|(\\\\begin\\{multline\\}))(?(1)(.*?)(?<!\\\\)(?<!\\$)\\1(?!\\$)|(?:([\\s\\S]*?)(?<!\\\\)(?:(?(2)\\\\end\\{equation\\}|(?(3)\\\\\\)|(?(4)\\\\\\]|(?(5)\\\\end\\{multline\\})))))))\n",
    "# 清洗公式\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "for i in tex_folds_rand_sample:\n",
    "    for j in os.listdir(os.path.join(\"/home/jihuawei/jhw_pdf_parser/grobid_base/datasetgenius/formula/tmp/tex/\", i)):\n",
    "        with open(os.path.join(\"/home/jihuawei/jhw_pdf_parser/grobid_base/datasetgenius/formula/tmp/tex/\", i, j), encoding='ISO-8859-1') as fp:\n",
    "            data = fp.read()\n",
    "        # 注释去掉\n",
    "        clean_data = re.sub(r'(?<!\\\\)%.*\\n?', '', data)\n",
    "        # \\begin{figure}...\\end{figure}去掉\n",
    "        clean_data = re.sub(r'\\\\begin\\{figure\\}[\\s\\S]*?\\\\end\\{figure\\}', '', clean_data)\n",
    "        # \\begin{algorithm}...\\end{algorithm}去掉\n",
    "        clean_data = re.sub(r'\\\\begin\\{algorithm\\}[\\s\\S]*?\\\\end\\{algorithm\\}', '', clean_data)\n",
    "        # \\begin{lstlisting}...\\end{lstlisting}去掉\n",
    "        clean_data = re.sub(r'\\\\begin\\{lstlisting\\}[\\s\\S]*?\\\\end\\{lstlisting\\}', '', clean_data)\n",
    "        clean_data = clean_data.replace('\\n', ' ')\n",
    "        # 提取公式\n",
    "        reg1 = r\"(?<!\\\\)(?:((?<!\\$)\\${1,2}(?!\\$))|(\\\\begin\\{equation\\})|(\\\\\\()|(\\\\\\[)|(\\\\begin\\{multline\\}))(?(1)(.*?)(?<!\\\\)(?<!\\$)\\1(?!\\$)|(?:([\\s\\S]*?)(?<!\\\\)(?:(?(2)\\\\end\\{equation\\}|(?(3)\\\\\\)|(?(4)\\\\\\]|(?(5)\\\\end\\{multline\\})))))))\"\n",
    "        pattern1 = re.compile(reg1, re.MULTILINE)\n",
    "        matches1 = pattern1.finditer(clean_data)   \n",
    "        equ_spans = [match.span() for match in matches1]\n",
    "        # 提取句点\n",
    "        reg2 = r\"(?<!No)(?<!\\.g)(?<!\\.e)(?<!\\{)\\.(?!g\\.)(?!e\\.)(?!\\d)(?=\\s)\"\n",
    "        pattern2 = re.compile(reg2, re.MULTILINE)\n",
    "        matches2 = pattern2.finditer(clean_data)\n",
    "        period_spans = [match.span() for match in matches2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 查询一段区间是否完全在另一段区间里面\n",
    "def if_inside(span1, span2, strict=False):\n",
    "    if strict:\n",
    "        if span1[0] > span2[0] and span1[1] < span2[1]:\n",
    "            return True\n",
    "        else:\n",
    "            return False   \n",
    "    else:   \n",
    "        if span1[0] >= span2[0] and span1[1] <= span2[1]:\n",
    "            return True\n",
    "        else:\n",
    "            return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 对period_spans进行筛选，先把跨equation的span合起来\n",
    "drop_per_lst = []\n",
    "for equ_span in equ_spans:\n",
    "    equ_right = equ_span[1]\n",
    "    for period_span in period_spans:\n",
    "        per_right = period_span[1]\n",
    "        if per_right <= equ_right:\n",
    "            if if_inside((per_right,per_right), equ_span):\n",
    "                # print(clean_data[equ_span[0]:equ_span[1]])\n",
    "                # print(period_span)\n",
    "                # print(\"=================================================\")\n",
    "                drop_per_lst.append(period_span)\n",
    "                \n",
    "        else:\n",
    "            break\n",
    "for drop_per in drop_per_lst:\n",
    "    period_spans.remove(drop_per)# 去掉这个句点就是合起来了\n",
    "# print(period_spans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 把包含公式的句子找出来\n",
    "sentence_spans = []\n",
    "for i in range(len(period_spans)):\n",
    "    if i == 0:\n",
    "        sentence_spans.append((0, period_spans[i][1]))\n",
    "    elif i == len(period_spans):\n",
    "        equ_spans.append((period_spans[i][1], len(period_spans)))\n",
    "    else:\n",
    "        sentence_spans.append((period_spans[i-1][1], period_spans[i][1]))\n",
    "equ_sentence_spans = []\n",
    "for equ_span in equ_spans:\n",
    "    for sentence_span in sentence_spans:\n",
    "        if if_inside(equ_span, sentence_span):\n",
    "            equ_sentence_spans.append(sentence_span)\n",
    "            break\n",
    "equ_sentence_spans = list(OrderedDict.fromkeys(equ_sentence_spans).keys())\n",
    "equ_sentence_spans = [equ_sentence for equ_sentence in equ_sentence_spans if equ_sentence[1] - equ_sentence[0]<= 800]# 去掉极长的"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_equ_sentences(equ_sentence_spans, clean_data):\n",
    "    equ_sentence_lst = []\n",
    "    for equ_sentence_span in equ_sentence_spans:\n",
    "        equ_sentence = clean_data[equ_sentence_span[0]:equ_sentence_span[1]].replace('\\n',' ').strip()# 初步清洗\n",
    "        equ_sentence = re.sub(r'\\s+', ' ', equ_sentence)\n",
    "        equ_sentence = re.sub(r'(?!\\\\begin\\{equation\\})(?!\\\\end\\{equation\\})(?!\\\\begin\\{multline\\})(?!\\\\end\\{multline\\})((\\\\begin\\{.*?\\})|(\\\\end\\{.*?\\})|(\\\\label\\{.*?\\})|\\\\item|\\\\qqq|\\\\ee|\\\\ref{.*?}|\\\\eqref{.*?}|\\\\footnote{.*?}|\\\\cite{.*?})', '', equ_sentence)\n",
    "        equ_sentence = equ_sentence.replace('\\n',' ').strip()# again\n",
    "        equ_sentence = re.sub(r'\\s+', ' ', equ_sentence)\n",
    "        equ_sentence_lst.append(equ_sentence)\n",
    "    return equ_sentence_lst\n",
    "equ_sentence_lst = clean_equ_sentences(equ_sentence_spans, clean_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We found in a user study ($N$=16) that the app generated with \\mr{} significantly reduced cognitive load and task completion time compared to a GUI-only app.\n",
      "==================================================\n",
      "The \\mr{} framework ships with one \\texttt{Helper class} called \\texttt{DateTime} and a set of \\texttt{GenieComponent} that can be used to represent the \\texttt{DateTime} instance, like one that shows the data in a convenient ``$n$ seconds/minutes/hours/days ago'' format.\n",
      "==================================================\n",
      "We used a paired t-test and found that the difference is statistically significant ($p=0.0004, t=3.955$).\n",
      "==================================================\n",
      "We used a Wilcoxon test and found that the difference is statistically significant ($p=0.013, z=21$).\n",
      "==================================================\n",
      "We used a Wilcoxon test and found that the difference is statistically significant ($p=0.031, z=22$).\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "for i in equ_sentence_lst:\n",
    "    print(i)\n",
    "    print(\"==================================================\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from grobid_parser import GParser\n",
    "import xml.etree.ElementTree as ET\n",
    "\n",
    "clean_data = \"\"\n",
    "pdf_parser = GParser(parse_imgs=False)\n",
    "for xml_path in xmls:\n",
    "    pdf_name = os.path.basename(xml_path).replace(\".tei.xml\", \".pdf\")\n",
    "    pdf_parser.pdf_name = pdf_name\n",
    "    tree = ET.parse(xml_path)\n",
    "    root = tree.getroot()\n",
    "    result = pdf_parser.parse_all(root, pdf_name, f\"{pdf_name.strip('.pdf')}.json\")\n",
    "    result.pop('back')\n",
    "    for name in result.keys():\n",
    "        top_el = result[name]\n",
    "        for el in top_el:\n",
    "            if 'tail' not in el.keys():\n",
    "                clean_data = clean_data + el['txt'] + ' '\n",
    "            else:\n",
    "                clean_data = clean_data + el['txt'] + ' '\n",
    "                clean_data = clean_data + el['tail'] + ' '\n",
    "clean_data= clean_data.replace('\\n', ' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_span(span1, span2):\n",
    "    return (min(span1[0], span1[1], span2[0], span2[1]), max(span1[0], span1[1], span2[0], span2[1]))\n",
    "\n",
    "# 提取句点\n",
    "reg2 = r\"(?<!No)(?<!\\.g)(?<!\\.e)(?<!\\{)\\.(?!g\\.)(?!e\\.)(?!\\d)(?=\\s)\"\n",
    "pattern2 = re.compile(reg2, re.MULTILINE)\n",
    "matches2 = pattern2.finditer(clean_data)\n",
    "period_spans = [match.span() for match in matches2]\n",
    "# 把包含公式的句子找出来\n",
    "sentence_spans = []\n",
    "for i in range(len(period_spans)):\n",
    "    if i == 0:\n",
    "        sentence_spans.append((0, period_spans[i][1]))\n",
    "    elif i == len(period_spans):\n",
    "        equ_spans.append((period_spans[i][1], len(period_spans)))\n",
    "    else:\n",
    "        sentence_spans.append((period_spans[i-1][1], period_spans[i][1]))\n",
    "\n",
    "sentence_spans_clean = []\n",
    "for i in range(len(sentence_spans)):\n",
    "    if i == len(sentence_spans)-1:\n",
    "        sentence_spans_clean.append(sentence_spans[i])\n",
    "    else:\n",
    "        if sentence_spans[i][1]-sentence_spans[i][0] <= 12:\n",
    "            sentence_spans[i+1] = merge_span(sentence_spans[i], sentence_spans[i+1])\n",
    "        else:\n",
    "            sentence_spans_clean.append(sentence_spans[i])\n",
    "sentence_spans = [sentence_span for sentence_span in sentence_spans_clean if sentence_span[1]-sentence_span[0] <= 800]\n",
    "xml_sentence_lst = clean_equ_sentences(sentence_spans, clean_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ReactGenie: An Object-Oriented State Abstraction for Complex Multimodal Interactions Using Large Language Models In this paper, we present ReactGenie, a programming framework that uses a shared object-oriented state abstraction to support building complex multimodal mobile applications.\n",
      "=================================================================\n",
      "Having different modalities share the same state abstraction allows developers using ReactGenie to seamlessly integrate and compose these modalities to deliver multimodal interaction.\n",
      "=================================================================\n",
      "ReactGenie is a natural extension to the existing workflow of building a graphical app, like the workflow with React-Redux.\n",
      "=================================================================\n",
      "Developers only have to add a few annotations and examples to indicate how natural language is mapped to the user-accessible functions in 1 INTRODUCTION Multimodal interactions allow users to use a combination of input and output modalities, such as touch, voice, and graphical user interfaces (GUIs).\n",
      "=================================================================\n",
      "They have been shown to offer more flexibility, efficiency, and adaptability for diverse users and tasks [47] .\n",
      "=================================================================\n",
      "However, developing multimodal applications remains a challenging task for developers.\n",
      "=================================================================\n",
      "Existing frameworks [10] [12] [28] [37] [38] [44] [45] often require developers to manually handle the low-level control logic for voice interactions and manage the complexity of multimodal commands.\n",
      "=================================================================\n",
      "This manual process significantly increases development costs and time and often limits the expressiveness of voice and multimodal commands.\n",
      "=================================================================\n",
      "There are systems [24] [36] [48] that can automatically handle voice commands by converting them to UI actions, but they are prone to error and do not allow developers to fully control the behavior of the app.\n",
      "=================================================================\n",
      "The goal of this research is to give developers a simple abstraction, while automatically handling the complexity of natural language understanding and composition of different modalities.\n",
      "=================================================================\n",
      "We focus on the smartphone, the most popular device with millions of existing apps.\n",
      "=================================================================\n",
      "We focus on letting users access off-screen content and actions, and complete tasks involving multiple taps in a single multimodal command, as illustrated in Figure [2] .\n",
      "=================================================================\n",
      "We aim to reuse the existing declarative GUI development workflow with little added workload to encourage the adoption of multimodal interactions and make multimodal interactions more accessible to end-users.\n",
      "=================================================================\n",
      "This paper presents ReactGenie 1 , a declarative programming framework for developing multimodal applications.\n",
      "=================================================================\n",
      "ReactGenie 1 source code: https://jya.ng/reactgenie uses an object-oriented state abstraction to represent the data and logic of the app, and uses declarative UI components to represent the UI.\n",
      "=================================================================\n",
      "Complex multimodal commands usually require multiple state changes.\n",
      "=================================================================\n",
      "Existing declarative UI state management frameworks [5] use a monolithic object to represent the state of the UI.\n",
      "=================================================================\n",
      "In other words, how the state changes for each step of a complex command is hidden within the monolithic object state, making it difficult for the framework to compose steps using different modalities.\n",
      "=================================================================\n",
      "The object-oriented state abstraction in ReactGenie, on the other hand, encapsulates state changes in each step as methods with strictly typed parameters and return values.\n",
      "=================================================================\n",
      "This allows easy and accurate composition of methods and properties of existing states for executing complex multimodal commands.\n",
      "=================================================================\n",
      "One example of a complex multimodal interaction is shown in the center of Figure 1 : the user says, \"Reorder my last meal from this restaurant\" while touching the display of a restaurant on the screen.\n",
      "=================================================================\n",
      "These commands are intuitive for human communication, but they actually involve multiple steps (retrieving the history of orders from the restaurant, creating an order, and adding food to the order) for the app and require combining inputs from both modalities, hence being \"complex\".\n",
      "=================================================================\n",
      "With ReactGenie, developers build graphical interfaces in a similar development workflow to a typical React + Redux application.\n",
      "=================================================================\n",
      "To add multimodality, the developer supplies a few annotations and examples that indicate what methods/properties can be used in voice and how they can be used.\n",
      "=================================================================\n",
      "By supplying the extracted class and function definitions from the developer's state code as well as few-shot examples, ReactGenie creates a parser that leverages large language models (LLM) [15] to translate the natural language to our new domain-specific language (DSL), ReactGenieDSL.\n",
      "=================================================================\n",
      "Combined with our custom-designed interpreter, ReactGenie can seamlessly handle multimodal commands and present the result in the graphical UI that the developer built.\n",
      "=================================================================\n",
      "As shown in Figure 1 left, developers can build object-oriented state abstraction classes dealing with data changes and UI components containing an explicit mapping between the state and the UI.\n",
      "=================================================================\n",
      "Similar to React, when the user interacts with the app, the state of the app will be updated and the UI will be re-rendered.\n",
      "=================================================================\n",
      "What is unique about ReactGenie is its ability to support complex multimodal input (shown in Figure 1 right).\n",
      "=================================================================\n",
      "To process the example mentioned above: (1) ReactGenie first translates the user's voice command to the ReactGenieDSL code.\n",
      "=================================================================\n",
      "Here, the user referred to an element on the UI by voice (\"this restaurant\") and the language parser generates a special reference Restaurant.current().\n",
      "=================================================================\n",
      "(2) ReactGenie extracts the tap point from the UI and uses the UI component code to map the tap point back to a state object Restaurant(name:\"TacoBell\").\n",
      "=================================================================\n",
      "(3) With both the parsed DSL and UI context, ReactGenie's interpreter can then execute the generated ReactGenieDSL using developer-defined states.\n",
      "=================================================================\n",
      "It first retrieves the most recent order from \"Taco Bell\", designated as \"Taco 3/3\".\n",
      "=================================================================\n",
      "Then, it creates a new order, designated as \"New Taco\".\n",
      "=================================================================\n",
      "Finally, the interpreter adds all the food items from \"Taco 3/3\" to \"New Taco\" and returns the new order.\n",
      "=================================================================\n",
      "(4) ReactGenie passes the return value of the ReactGenieDSL statement to the output UI mapping module.\n",
      "=================================================================\n",
      "Because the return value is an Order object, ReactGenie searches in the developer's UI component code to find a corresponding representation (Output UI Mapping) to present the result to the user.\n",
      "=================================================================\n",
      "ReactGenie also generates a text response using the LLM based on the user's input, parsed ReactGenieDSL, and the return value: \"Your cart is updated with the same order from this restaurant as the last time.\n",
      "=================================================================\n",
      "\" Our main contributions to this paper are as follows: • ReactGenie, a new multimodal app development framework based on object-oriented state abstraction that is easy on developers and generates apps that support complex multimodal interactions.\n",
      "=================================================================\n",
      "• Automatic handling of natural language understanding in a multimodal app framework.\n",
      "=================================================================\n",
      "This involves the design of the high-level annotation of user-accessible functions, the automatic generation of a natural language parser using LLMs that targets ReactGenieDSL, a new DSL for complex multimodal commands, and an interpreter that executes Re-actGenieDSL.\n",
      "=================================================================\n",
      "<b>• Evaluations of ReactGenie:</b> -We demonstrated the expressiveness of the ReactGenie framework by building three demo apps.\n",
      "=================================================================\n",
      "-We elicited commands from crowd workers and found the ReactGenie language parser can achieve 90% accuracy on supported tasks for one of the example apps.\n",
      "=================================================================\n",
      "-We found in a user study ( =16) that the app generated with ReactGenie significantly reduced cognitive load and task completion time compared to a GUI-only app.\n",
      "=================================================================\n",
      "2 RELATED WORK In this section, we review related work on multimodal interaction systems, Graphical and Voice UI frameworks, and multimodal interaction frameworks.\n",
      "=================================================================\n",
      "2.1 Multimodal Interaction Systems Many researchers have proposed multimodal interaction systems.\n",
      "=================================================================\n",
      "The earliest multimodal interaction systems were developed in the 1980s [13] .\n",
      "=================================================================\n",
      "They demonstrated that users can interact with a computer using a combination of voice and gestures.\n",
      "=================================================================\n",
      "QuickSet [18] further demonstrated use cases of multimodal interaction on a mobile device and showed military and medical applications.\n",
      "=================================================================\n",
      "Recent work has explored different applications of multimodal interaction, including care-taking of older adults [43] [46] , photo editing [33] , and digital virtual assistants [29] .\n",
      "=================================================================\n",
      "Researchers have also explored different devices and environments for multimodal interaction, including augmented reality [54] , virtual reality [35] [51] , wearables [14] , and the Internet of Things [21] [30] [50] [53] .\n",
      "=================================================================\n",
      "These papers have demonstrated the great potential of multimodal interaction systems.\n",
      "=================================================================\n",
      "However, multimodal systems still have limited adoption in the real world due to their added development complexity.\n",
      "=================================================================\n",
      "2.2 Graphical UI frameworks ReactGenie is built on top of existing graphical UI frameworks to provide a familiar development experience.\n",
      "=================================================================\n",
      "Model-view-controller (MVC) [31] is a traditional approach to UI development, including frameworks such as Microsoft's Windows Forms [25] , and Apple's UIKit [1] .\n",
      "=================================================================\n",
      "The model stores data while the controller manages GUI input and updates the GUI based on data changes.\n",
      "=================================================================\n",
      "Typically implemented in object-oriented programming languages, MVC can be compared to a shadow play, where objects (controllers) manipulate GUIs and data to maintain synchronization.\n",
      "=================================================================\n",
      "However, updating the model with alternative modalities, such as voice, is not feasible due to the strong entanglement between models and GUI updates.\n",
      "=================================================================\n",
      "Garnet [40] , a user interface development environment introduced in the late 1980s, is another notable approach to GUI development.\n",
      "=================================================================\n",
      "Garnet introduced concepts like data binding, which allows the GUI to be updated automatically when the data changes.\n",
      "=================================================================\n",
      "It also tries to abstract the GUI state from the presentation using designs like interactors [39] .\n",
      "=================================================================\n",
      "While interactors allow the UI state to be rewired and thus to be updated from another modality like voice or gesture [32] , they still lack the capability to enable manipulation of more abstract states (like an entire query) that are not directly mapped to a single UI control.\n",
      "=================================================================\n",
      "Declarative UI frameworks, such as React [2] , Flutter [3] , and SwiftUI [6], are a more recent approach to UI development.\n",
      "=================================================================\n",
      "With declarative UI frameworks, programmers write functions to transform data into UI interfaces, and the system automatically manages updates.\n",
      "=================================================================\n",
      "To ease the management of states that may be updated by and reflected on multiple UI interfaces, centralized state management frameworks, such as Redux [5], Flux [8] , and Pinia [4], are often used.\n",
      "=================================================================\n",
      "They provide a single source of truth for the application state and allow state updates to be reflected across all presented UIs.\n",
      "=================================================================\n",
      "This approach can be likened to an overhead projector, where the centralized state represents the writing and the transform functions represent the lens projecting the UI to the user.\n",
      "=================================================================\n",
      "While this approach improves separation and UI updating, it sacrifices the object-oriented nature of the data model.\n",
      "=================================================================\n",
      "ReactGenie reintroduces object-orientedness to centralized state management systems by representing the state as a sum of all class instances in the memory.\n",
      "=================================================================\n",
      "Developers can declare classes and describe actions as member functions of the classes.\n",
      "=================================================================\n",
      "ReactGenie captures all instantiated classes and stores them in a central state.\n",
      "=================================================================\n",
      "This model can be compared to children (class instances) playing in a playground, with views (UI components) acting as cameras capturing different angles of the centralized state.\n",
      "=================================================================\n",
      "In this way, React-Genie enables complex action composition through type-checked functional calls.\n",
      "=================================================================\n",
      "Furthermore, developers can tag specific cameras to point at certain objects, enabling automatic UI updates from state changes.\n",
      "=================================================================\n",
      "These features allow ReactGenie apps to easily support the compositionality of multimodal input and enable the interleaving of multimodal input with other graphical UI actions.\n",
      "=================================================================\n",
      "2.3 Voice UI frameworks Commercial voice or chatbot frameworks, such as Amazon Lex, Dialogflow, and Rasa, are designed to handle natural language understanding and generation.\n",
      "=================================================================\n",
      "They allow developers to define intents and entities, and then train the model to recognize the intents and entities from the user's input.\n",
      "=================================================================\n",
      "These frameworks require complete redevelopment of the application to support voice input only.\n",
      "=================================================================\n",
      "Frameworks such as Alexa Skills Kit and Google Actions allow developers to extend existing applications to support voice input.\n",
      "=================================================================\n",
      "However, they still require manual work to build functions only for voice, and their UI updates are limited to simple text and a few pre-defined UI elements.\n",
      "=================================================================\n",
      "Additionally, the intent-based nature limits the compositionality of the voice commands.\n",
      "=================================================================\n",
      "Research-focused voice/natural language frameworks, such as Genie [16] [49] and other semantic parsers [11] [41] , are designed to support better compositionality of voice commands.\n",
      "=================================================================\n",
      "However, given that app development is primarily geared toward mobile and graphical interfaces, these frameworks require extra work from the developer and do not support multimodal features.\n",
      "=================================================================\n",
      "ReactGenie improves this experience by integrating the development process of voice and graphical UIs, allowing developers to extensively reuse existing code and support multimodal interactions.\n",
      "=================================================================\n",
      "2.4 Multimodal Interaction Frameworks Prior work has also proposed multimodal interaction frameworks that allow developers to build multimodal applications.\n",
      "=================================================================\n",
      "One of the earliest works is presented by Cohen et al.\n",
      "=================================================================\n",
      "[17] . It includes ideas like forming the user's voice command as a function call and using the user's touch point as a parameter to the function call.\n",
      "=================================================================\n",
      "Later, researchers created standards [19] [20] and frameworks [10] [12] [28] [37] [38] [44] [45] to help developers build apps that can handle multiple inputs across different devices.\n",
      "=================================================================\n",
      "Although these frameworks provide scaffolding for developers to build multimodal applications, they mostly treated voice as an event source that can trigger functions the developer has to explicitly implement for voice.\n",
      "=================================================================\n",
      "Developers also have to manually update the UI to reflect the result of the voice command.\n",
      "=================================================================\n",
      "This manual process limits voice commands to simple single-action commands and makes it difficult for developers to build complex multimodal applications.\n",
      "=================================================================\n",
      "Recently, there have been research papers about generating voice commands by learning from demonstration [23] [42] , extracting from graphical user interfaces with large language models [24] [36] [48] , or building multimodal applications using existing voice skills [52] .\n",
      "=================================================================\n",
      "The first approach still requires developers to manually create demonstrations for each action and limits the compositionality of the voice commands.\n",
      "=================================================================\n",
      "The second approach is useful for accessibility purposes, but it relies on the features being easily extractable from the GUI.\n",
      "=================================================================\n",
      "It is uncertain how well the first two approaches can generalize to more complex UI tasks that require multiple UI actions.\n",
      "=================================================================\n",
      "The third approach is limited by what is provided by the voice skills and, traditionally, these have been very limited due to the added development effort.\n",
      "=================================================================\n",
      "In comparison, ReactGenie leverages the existing GUI development workflow and requires only minimal annotations to the code to generate multimodal applications.\n",
      "=================================================================\n",
      "Having access to the full object-oriented state programming codebase, ReactGenie can handle the natural complexity of multimodal input, compose the right series of function calls, and update the UI to reflect the result automatically.\n",
      "=================================================================\n",
      "3 SYSTEM DESIGN In this section, we first define the design goals of the framework.\n",
      "=================================================================\n",
      "Then, we describe the theory of operation that addresses the design goals.\n",
      "=================================================================\n",
      "Finally, we discuss the implementation of the system components and workflow.\n",
      "=================================================================\n",
      "3.1 Design goals 3.1.1 Interaction Design.\n",
      "=================================================================\n",
      "ReactGenie is designed to enhance the interaction of mobile applications.\n",
      "=================================================================\n",
      "Today, mobile applications are well-optimized for touch and graphical interactions.\n",
      "=================================================================\n",
      "Users can use the graphical interface to see content on the screen and use touch to access actions on the screen.\n",
      "=================================================================\n",
      "To further enhance the user's performance and reduce cognitive load, ReactGenie focuses on supporting interactions that often involve touch actions used together with a voice command.\n",
      "=================================================================\n",
      "These interactions can be categorized into three interaction design goals (see Figure [2] ): Data is stored in the state variable and the state can be mutated by the actions defined.\n",
      "=================================================================\n",
      "These actions do not have explicit types and they directly manipulate the state so no return values are defined.\n",
      "=================================================================\n",
      "Due to its monolithic design, it is hard to compose functions together to achieve some multimodal actions.\n",
      "=================================================================\n",
      "(Right) Part of an example state code in ReactGenie.\n",
      "=================================================================\n",
      "Automatically managed by ReactGenie, the state is composed of all the instantiated objects DataClasses.\n",
      "=================================================================\n",
      "Actions in the state are defined as methods of the class.\n",
      "=================================================================\n",
      "All the methods have explicit parameter types and return types.\n",
      "=================================================================\n",
      "These functions can be composed together to achieve multimodal actions.\n",
      "=================================================================\n",
      "I1 in the same room.\" This interaction would usually require multiple GUI navigation steps to access the content.\n",
      "=================================================================\n",
      "I2 Access off-screen actions: For example, the user says, \"Share this creator/comment\" while watching a YouTube video.\n",
      "=================================================================\n",
      "Some actions are hidden behind a menu or a button, and some are not accessible on mobile devices.\n",
      "=================================================================\n",
      "I3 Combine multiple actions/content: For example, the user says, \"Order what I ordered last time\" while looking at a food delivery app.\n",
      "=================================================================\n",
      "This usually requires the user to go back and forth between an order detail page and a menu page.\n",
      "=================================================================\n",
      "The common theme of these interactions is that they require the multimodal interaction framework to understand the content and actions available in the app.\n",
      "=================================================================\n",
      "For the content, the framework needs to know what the content on the screen is, how to access it, and how to represent it to show the retrieved content.\n",
      "=================================================================\n",
      "For the actions, the framework needs to know the list of available actions and how to reflect changes back to the user after the action is triggered.\n",
      "=================================================================\n",
      "Finally, the framework needs to understand the user's complex intent and potentially represent it as a series of actions and content.\n",
      "=================================================================\n",
      "With ReactGenie, apps will have a microphone button on the screen.\n",
      "=================================================================\n",
      "When the user taps on the button, the user can say their command and refer to the content on the screen by tapping on it.\n",
      "=================================================================\n",
      "The app will then parse the voice command and touch input and execute the corresponding actions to help the user with the types of scenarios described above.\n",
      "=================================================================\n",
      "3.1.2 Framework Design.\n",
      "=================================================================\n",
      "To understand the content, the actions, and the user's complex intent, ReactGenie needs to seek information from the app's code.\n",
      "=================================================================\n",
      "However, the design goal for ReactGenie is to do this in a way that causes minimal disruption for the application developer.\n",
      "=================================================================\n",
      "Without a proper framework for multimodal apps, the developer needs to design mechanisms to handle voice, deal with the complexity of multimodal commands, and maintain control of the app's behavior.\n",
      "=================================================================\n",
      "Instead, ReactGenie's framework design goals include handling these issues: F1 Ease the learning curve by providing a similar programming experience to existing GUI frameworks.\n",
      "=================================================================\n",
      "F2 Maximize the reuse of existing GUI code and alleviate the developer from handling the complexity of multimodal commands.\n",
      "=================================================================\n",
      "F3 Allow developers to have full control over the graphical UI appearance and app behavior.\n",
      "=================================================================\n",
      "3.2 Theory of Operation ReactGenie presents an object-oriented state programming model to the developer.\n",
      "=================================================================\n",
      "As mentioned in Section 2.2, in frameworks like React, UI development is moving towards separating the UI from the state, so that there is a unidirectional data flow from the state to the UI.\n",
      "=================================================================\n",
      "This trend allows ReactGenie to use multimodal commands to change the same state and update the UI accordingly, maximizing the reuse of existing GUI code (F2).\n",
      "=================================================================\n",
      "However, in these existing frameworks, the state is simplified to a single data store with functional transforms (actions) on the data store.\n",
      "=================================================================\n",
      "The monolithic state makes it difficult to compose multiple actions involving different modalities in one command (I3).\n",
      "=================================================================\n",
      "ReactGenie introduces the object-oriented programming model to the state of a declarative UI app.\n",
      "=================================================================\n",
      "The object-oriented state model naturally separates the content and actions in the app.\n",
      "=================================================================\n",
      "The content is usually defined as properties of the state objects, and the actions are defined as methods of the state objects.\n",
      "=================================================================\n",
      "4 : Overview of the ReactGenie system: Developers write object-oriented state code for programming content and actions and define the UI as cascading components.\n",
      "=================================================================\n",
      "ReactGenie operates at transpilation and initialization time to generate runtime modules.\n",
      "=================================================================\n",
      "Developer modules, generated modules, and ReactGenie modules come together to process complex multimodal commands from the user.\n",
      "=================================================================\n",
      "This workflow is similar to regular GUI development, maximizes code reuse, and allows full control of the app behavior.\n",
      "=================================================================\n",
      "The developer identifies the user-accessible content and actions with the GenieProperty and GenieFunction annotations, along with an example of how it may be referred to in English as shown in Figure [3] right.\n",
      "=================================================================\n",
      "The GUI is defined as cascading components that are rendered from the user-accessible state.\n",
      "=================================================================\n",
      "Components are functions that developers define, which convert data in the state to HTML, UI elements, or a combination of other components.\n",
      "=================================================================\n",
      "This allows the internal state to be rendered to the user in a GUI.\n",
      "=================================================================\n",
      "The highlevel model of ReactGenie resembles popular GUI development frameworks (React + Redux), which makes it easy for developers to learn and use (F1).\n",
      "=================================================================\n",
      "ReactGenie automatically handles the retrieval of content off screen (I1), the execution of actions off screen (I2), and combinations of the two (I3).\n",
      "=================================================================\n",
      "We contrast ReactGenie's object-oriented approach with React in Figure [3] .\n",
      "=================================================================\n",
      "In React, if the user says \"Add a food to a new order\", this is implemented as a code sequence that calls the CREATE_ORDER action, indexes into the last of the orders in the state, and calls the ADD_FOOD_TO_ORDER action.\n",
      "=================================================================\n",
      "Due to the nature of these actions, there is no explicit connection linking them together.\n",
      "=================================================================\n",
      "Whereas in ReactGenie, each step corresponds to a method invocable by multimodal input.\n",
      "=================================================================\n",
      "These methods are all strictly typed, which helps the natural language parser come up with the correct combination of methods.\n",
      "=================================================================\n",
      "The entire natural language command can be represented succinctly with a single-line command: Order.CreateOrder ().addItem(foodItem:food).\n",
      "=================================================================\n",
      "This succinctness improves the accuracy of the natural language parser that translates the natural language command to this code.\n",
      "=================================================================\n",
      "Furthermore, as the same content and state are shared by the GUI and voice modality, this representation supports interchangeability in the modality of user input for each step.\n",
      "=================================================================\n",
      "Due to the complexity of the user's multimodal commands, we cannot use a simple intent classifier like that used by traditional voice assistants.\n",
      "=================================================================\n",
      "Instead, we use a sequence-to-sequence neural semantic parser to translate the user's natural language command into ReactGenieDSL, which is executed using the developer's code.\n",
      "=================================================================\n",
      "We do not generate JavaScript directly, because the expressiveness of JavaScript may cause unintended changes to the app's behavior (contradicting F3).\n",
      "=================================================================\n",
      "3.3 System Components As shown in Figure 4 , a ReactGenie app consists of developersupplied code, MultiReact-supplied runtime modules, and modules generated from the developer's code.\n",
      "=================================================================\n",
      "3.3.1 Developer Modules.\n",
      "=================================================================\n",
      "Developers provide the content and actions in an app through ReactGenie's object-oriented state model, implemented in TypeScript 2 .\n",
      "=================================================================\n",
      "As with all object-oriented programming models, ReactGenie's state includes the definition of classes and instances.\n",
      "=================================================================\n",
      "Classes have methods and properties, which can be labeled as GenieFunction and GenieProperty using annotations to indicate that they are user-accessible via multimodal commands.\n",
      "=================================================================\n",
      "These code annotations or decorators in TypeScript are tags written before the class, method, and property declarations.\n",
      "=================================================================\n",
      "This allows the relevant annotation code to be executed at initialization time to transform the capabilities of the annotated classes or methods.\n",
      "=================================================================\n",
      "Class Definitions.\n",
      "=================================================================\n",
      "ReactGenie provides developer-accessible classes for developers to implement the state of their app: DataClass and HelperClass.\n",
      "=================================================================\n",
      "The former stores data of the app, and the latter provides definitions to ease the user's interaction with the data.\n",
      "=================================================================\n",
      "Examples of each type of class are shown in the top left of Figure 4 .\n",
      "=================================================================\n",
      "All DataClass instances need to implement two methods and one property: (1) constructor method: The constructor of the class initializes the instance with all the required data.\n",
      "=================================================================\n",
      "(2) All method: A static method that either returns a list of instances of the class or returns a Query object that can be used to retrieve the instances.\n",
      "=================================================================\n",
      "Should be annotated with GenieFunction.\n",
      "=================================================================\n",
      "(3) id property: A unique identifier of the instance.\n",
      "=================================================================\n",
      "With this information, ReactGenie can automatically generate two methods for each DataClass: (1) Get method: A static method that takes an id as input and returns the instance in memory with the corresponding id.\n",
      "=================================================================\n",
      "(2) Current property: A static method that returns the instance that is being referred to by the user.\n",
      "=================================================================\n",
      "This is automatically annotated with GenieProperty.\n",
      "=================================================================\n",
      "A DataClass is like a table in a database.\n",
      "=================================================================\n",
      "The constructor, All, id, and Get are basically insertion, selection, primary key, and retrieval of a row using its primary key.\n",
      "=================================================================\n",
      "ReactGenie automatically maintains the instances of DataClass in memory to form the state of the app.\n",
      "=================================================================\n",
      "Similar to many of the common state management frameworks, when any of the properties of a DataClass instance is changed, UI components that refer to that property will be automatically updated.\n",
      "=================================================================\n",
      "This ensures that the UI is always in sync with what is being represented in the state.\n",
      "=================================================================\n",
      "DataClass can be backed by a remote database, which is common in modern app development.\n",
      "=================================================================\n",
      "This database is automatically managed by ReactGenie.\n",
      "=================================================================\n",
      "When the data is backed by a remote database, All can be implemented as a query to the original database, and it can instantiate the DataClass with the data retrieved from the database when needed.\n",
      "=================================================================\n",
      "The HelperClass allows developers to define new types that can be used in the DataClass.\n",
      "=================================================================\n",
      "For example, ReactGenie already provides a basic DateTime HelperClass that can be used to represent a date.\n",
      "=================================================================\n",
      "It can support operations like offsetting the time by a certain amount of time or setting the 2 https://www.typescriptlang.org/ year/month/day/hour/minute/second/day of the week to a certain value.\n",
      "=================================================================\n",
      "It allows commands such as \"last Thursday\", which can be translated to \"DateTime.Current.offset(week:-1).set (weekOfTheDay:4)\".\n",
      "=================================================================\n",
      "Other HelperClass instances can be defined by the developer to support more complex operations such as length unit conversion and so on.\n",
      "=================================================================\n",
      "The HelperClass is only required to have a constructor method that takes the data as input and initializes the instance.\n",
      "=================================================================\n",
      "ReactGenie will also generate a Current property for the HelperClass that returns the instance that is being referred to by the user.\n",
      "=================================================================\n",
      "ReactGenie does not keep track of the instances of HelperClass separately in memory, but instead, they will be part of the DataClass that uses them.\n",
      "=================================================================\n",
      "For both DataClass and HelperClass, the developer can define a description method to customize the string representation of the instance for response generation.\n",
      "=================================================================\n",
      "By default, ReactGenie will generate a JSON-like representation using all the properties of the instance.\n",
      "=================================================================\n",
      "Natural language annotations.\n",
      "=================================================================\n",
      "ReactGenie calls an LLM with the functions and properties labeled GenieFunction and GenieProperty in a format resembling a class definition to teach the LLM the app's functionality.\n",
      "=================================================================\n",
      "If the design of the classes is unusual, the developer needs to define a few example commands and the corresponding ReactGenieDSL to teach the LLM the syntax (see Section 3.3.2).\n",
      "=================================================================\n",
      "For example, when building an example food ordering app, we represent both the current shopping cart and previously placed orders using the same Order object and distinguishing them with the orderPlaced property.\n",
      "=================================================================\n",
      "We found that the LLM cannot parse some of the commands correctly, and we need to provide two examples to fix the majority of parsing errors.\n",
      "=================================================================\n",
      "Graphical user interface declarations.\n",
      "=================================================================\n",
      "The ReactGenie developer needs to define the GUI (as shown in the bottom left of Figure 4 ) as a set of functional components [3] similar to React.\n",
      "=================================================================\n",
      "These components can refer to each other to facilitate reuse.\n",
      "=================================================================\n",
      "It is common for every single instance of a DataClass or HelperClass to be represented by a component.\n",
      "=================================================================\n",
      "Therefore, ReactGenie introduces a special component called GenieComponent for that purpose.\n",
      "=================================================================\n",
      "Instead of the arbitrary parameters of a normal functional component, GenieComponent takes a DataClass or HelperClass instance as input.\n",
      "=================================================================\n",
      "GenieComponent allows the ReactGenie runtime to understand which component is mapped to which instance in memory.\n",
      "=================================================================\n",
      "It also allows ReactGenie to render the result of the user's request using the developer-defined component.\n",
      "=================================================================\n",
      "While defining GenieComponent, the developer can also specify an optional title and priority (both can be a method of the state instance) for the interface, which is relevant for choosing the interface to render multimodal command responses.\n",
      "=================================================================\n",
      "Therefore, because of L1, ReactGenieDSL has to be in a form that is similar to existing programming languages that LLMs are trained on.\n",
      "=================================================================\n",
      "To help with L2, we also want ReactGenieDSL to be strongly typed.\n",
      "=================================================================\n",
      "We tried a syntax similar to TypeScript, but we noticed the LLM-based language parser tended to generate the correct parameters but in the wrong order from time to time.\n",
      "=================================================================\n",
      "Therefore, we decided to use a syntax similar to Swift, which is also a strongly typed language but requires parameter names to be specified in the function call.\n",
      "=================================================================\n",
      "To reduce complexity (also helps with L2), we decided not to use any lambda functions in ReactGenieDSL.\n",
      "=================================================================\n",
      "To maintain expressiveness L3, we added a few array functions to aid basic filtering (matching, between), sorting (sort), and summarization (sum, average, count).\n",
      "=================================================================\n",
      "The full grammar of ReactGenieDSL is listed in Appendix A.\n",
      "=================================================================\n",
      "We implemented the DSL interpreter module using peggy [4] .\n",
      "=================================================================\n",
      "The ReactGenie framework ships with one Helperclass called DateTime and a set of GenieComponent that can be used to represent the DateTime instance, like one that shows the data in a convenient \" seconds/minutes/hours/days ago\" format.\n",
      "=================================================================\n",
      "In the future, ReactGenie can provide more HelperClass and GenieComponent to support common operations.\n",
      "=================================================================\n",
      "Libraries can also be built on top of ReactGenie to provide more HelperClass and GenieComponent for specific domains.\n",
      "=================================================================\n",
      "3.4 System Workflow Typically, a TypeScript app is transpiled to JavaScript so that it can be run in a mobile app or in a browser.\n",
      "=================================================================\n",
      "Transpilation is a source-tosource translation process from the TypeScript that the developer writes to Javascript that the machine executes.\n",
      "=================================================================\n",
      "However, during the transpilation process, the metadata like typing and function parameter names are removed.\n",
      "=================================================================\n",
      "The metadata lost in transpilation is required by ReactGenie to understand the developer's code.\n",
      "=================================================================\n",
      "Therefore, ReactGenie works by generating modules during transpilation and at initialization time when information about the developer's code is still available.\n",
      "=================================================================\n",
      "ReactGenie calls the generated modules during runtime (as shown in the top right of Figure 4 ).\n",
      "=================================================================\n",
      "3.4.1 Transpilation and Initialization for States.\n",
      "=================================================================\n",
      "We built a custom transpiler plugin that generates extra metadata for @GenieProperty, and @GenieFunction of DataClass and HelperClass.\n",
      "=================================================================\n",
      "During initialization of the app, ReactGenie will load injected metadata from state classes to generate a prompt for the LLM.\n",
      "=================================================================\n",
      "LLMs work by generating text continuations given a paragraph of previous text.\n",
      "=================================================================\n",
      "The provided previous text is often referred to as the prompt.\n",
      "=================================================================\n",
      "By controlling the prompt, we change the information that the LLM has access to and guide the LLM to do what we want (generate the corresponding ReactGenieDSL of the user's command).\n",
      "=================================================================\n",
      "ReactGenie's generated prompt contains two parts: 1) The class definitions contain all the DataClass and HelperClass methods and properties definitions with the implementation stripped out.\n",
      "=================================================================\n",
      "It is rendered in a format similar to Swift syntax.\n",
      "=================================================================\n",
      "2) The example parses provided by the developer are also included as few-shot examples.\n",
      "=================================================================\n",
      "The language parser adds the user input to the generated prompt and presents that to the LLM to translate into ReactGenieDSL.\n",
      "=================================================================\n",
      "The response generator prompts the LLM with the generated prompt, the user input, the parsed ReactGenieDSL, and the description of the return value from the execution of ReactGenieDSL to produce a short text response.\n",
      "=================================================================\n",
      "We built the language parser using the OpenAI Codex model code-davanci-2 and the response generator using the OpenAI GPT 3.5 model text-davanci-3.\n",
      "=================================================================\n",
      "3.4.2 Transpilation and Initialization for Components.\n",
      "=================================================================\n",
      "During initialization time, we also process GenieComponent functions to save a mapping between GenieComponent and the GenieClass that they are representing.\n",
      "=================================================================\n",
      "From this information, we generate input and output UI mapping modules.\n",
      "=================================================================\n",
      "For input mapping, we monitor the bounding box of all GenieComponents.\n",
      "=================================================================\n",
      "When the user touches the screen while expressing a multimodal command, ReactGenie will use the bounding box information to figure out which component the user is pointing to.\n",
      "=================================================================\n",
      "It is common for multiple UI components to cover the area where the user taps on the screen.\n",
      "=================================================================\n",
      "For example, in Figure 1 , all the FoodThumbnail components overlap with the OrderItemView components.\n",
      "=================================================================\n",
      "ReactGenie allows the user to use their voice to disambiguate the reference: If the user mentions food, such as \"this food\" (FoodItem.Current()), or actions that can only be done with food, like \"what is the price for this\" (FoodItem.Current().price), Re-actGenie will use the FoodItem object and vice versa.\n",
      "=================================================================\n",
      "In the special case where the tapped area is covered by multiple components of the same type, ReactGenie uses the one with the smallest bounding box.\n",
      "=================================================================\n",
      "Another common scenario is that if one object is clearly in the \"foreground\" of the graphical UI, the user may naturally refer to it as \"this\" without explicitly specifying the component via touch.\n",
      "=================================================================\n",
      "So, when the user refers to a state class and either there is no touch point or the touch point does not match any component representing that class, ReactGenie will use the largest component on the screen representing that class as the reference.\n",
      "=================================================================\n",
      "We also use GenieComponent to generate output UI mapping modules.\n",
      "=================================================================\n",
      "We gather all the GenieComponents with supplied priority and title and group them by the state class they are representing.\n",
      "=================================================================\n",
      "When the result of the executed ReactGenieDSL is a state class instance, ReactGenie enumerates through all the GenieComponents representing that class and renders the one with the highest priority.\n",
      "=================================================================\n",
      "There are two types of execution results.\n",
      "=================================================================\n",
      "The first type is that the translated ReactGenieDSL returns an instance that can be rendered by a GenieComponent.\n",
      "=================================================================\n",
      "This is common when the user asks to either retrieve some data \"what are my most recent orders from this restaurant\" or to perform some action with a clear result \"create an empty cart\".\n",
      "=================================================================\n",
      "In that case, it would be intuitive to render the result on the screen directly.\n",
      "=================================================================\n",
      "So, when the return value can be represented by a GenieComponent, ReactGenie will always find the highest priority GenieComponent and render it.\n",
      "=================================================================\n",
      "<b>ReactGenieFoodOrdering ReactGenieSocial ReactGenieSign</b> ReactGenieSign -NDA Management The second type is that the translated ReactGenieDSL returns a value that cannot be rendered by a GenieComponent.\n",
      "=================================================================\n",
      "For example, if the user asks to \"add a hamburger to the cart\" (Re-actGenieDSL: Order.GetActiveCart().addItem([Food.Named (\"hamburger\")]), it would return void which cannot be rendered.\n",
      "=================================================================\n",
      "For these actions, the return value is less important and the user is more interested in the side effect of the action, so we can provide confirmation via text feedback.\n",
      "=================================================================\n",
      "In the case when the user is already on a restaurant page where they can see an indicator of the number of items in the cart (the counter as a component would also represent the cart instance), it would be redundant to show the cart again.\n",
      "=================================================================\n",
      "However, if the user is on the past order page where they cannot see any representation of the cart, it would be useful to show the cart to make sure the user understands the action being performed.\n",
      "=================================================================\n",
      "In that case, ReactGenie will automatically check the execution history and find the last readable result is Order.GetActiveCart ().\n",
      "=================================================================\n",
      "ReactGenie will also check all the currently shown components (in a similar way as how the input UI mapping works) to see if there is any component representing the same instance.\n",
      "=================================================================\n",
      "ReactGenie would only render this result if the current page does not have any component representing the same instance.\n",
      "=================================================================\n",
      "3.4.3 Runtime.\n",
      "=================================================================\n",
      "Similar to normal React or React-Native apps, when users interact with buttons and visual controls in the app, the app calls the corresponding methods to update data in the state instances.\n",
      "=================================================================\n",
      "In turn, the state instances trigger the GenieComponents to update their UI.\n",
      "=================================================================\n",
      "As shown in the bottom right of Figure 4 , the multimodal interactions are handled through developer modules (Section 3.3.1), the ReactGenie modules (Section 3.3.2), and the generated modules (Section 3.4.1) collectively.\n",
      "=================================================================\n",
      "When the user touches the microphone button on the UI, ReactGenie starts listening to the user's voice command and intercepts all touch events on the screen.\n",
      "=================================================================\n",
      "From this, we gather two inputs: the user's voice command and the touch point(s).\n",
      "=================================================================\n",
      "We use speech recognition from Azure to transcribe the user's speech to text.\n",
      "=================================================================\n",
      "The voice command transcript then is passed to the language parser module to generate the ReactGenieDSL code.\n",
      "=================================================================\n",
      "The touch point(s) are passed to the input UI mapping module to figure out which component and state instance the user can be referring to.\n",
      "=================================================================\n",
      "Both pieces of information are then passed to the React-GenieDSL interpreter to execute the ReactGenieDSL code with the corresponding relevant state instance.\n",
      "=================================================================\n",
      "ReactGenie uses the methods and properties of the developer-provided state classes to execute the ReactGenieDSL code.\n",
      "=================================================================\n",
      "After the execution, we record both the final return value and the intermediate values during execution.\n",
      "=================================================================\n",
      "ReactGenie uses the return value and the parsed DSL to generate a text response using the response generator.\n",
      "=================================================================\n",
      "ReactGenie also passes execution steps to the output UI mapping module to figure out whether and how to render the result on the screen.\n",
      "=================================================================\n",
      "Finally, the text response and the rendered UI are used to generate Feedback in Text and Content in UI.\n",
      "=================================================================\n",
      "Table 1 : Implementation statistics of demo apps.\n",
      "=================================================================\n",
      "We listed all the DataClass, HelperClass, and the number of GenieComponent and GenieFunction used in the apps.\n",
      "=================================================================\n",
      "We also listed the number of lines of code for the state and component code and the number of example parses provided for the voice parser.\n",
      "=================================================================\n",
      "<b>App</b> 4 EVALUATION We first followed the guidelines of Ledo et al.\n",
      "=================================================================\n",
      "[34] to evaluate ReactGenie as a toolkit through demonstration and technical performance.\n",
      "=================================================================\n",
      "We demonstrate the expressiveness and usefulness of the ReactGenie framework by building three demo apps.\n",
      "=================================================================\n",
      "We analyze the number of annotations and examples needed when building the demo apps to show that the cost of building multimodal support is low.\n",
      "=================================================================\n",
      "We also analyze the technical performance of the ReactGenie system by measuring the accuracy of the language parser through an elicitation study.\n",
      "=================================================================\n",
      "We then verified our demo apps are effective through a lab-based usability study with the demo food ordering app.\n",
      "=================================================================\n",
      "Our results showed the multimodal version of the app significantly improved the user experience over the GUI-only version in various aspects.\n",
      "=================================================================\n",
      "4.1 Example Apps We built three example apps across three major categories of apps: food & drink, social networking, and business, as shown in Figure [5] .\n",
      "=================================================================\n",
      "The implementation statistics are shown in Table 1 .\n",
      "=================================================================\n",
      "4.1.1 ReactGenieFoodOrdering.\n",
      "=================================================================\n",
      "ReactGenieFoodOrdering is a food ordering app that allows users to order food from a restaurant.\n",
      "=================================================================\n",
      "It has the basic functions of browsing menus, shopping cart management, and checking order history.\n",
      "=================================================================\n",
      "In total, the app is composed of around 2689 lines of code, only 88 (3%) of which are related to building multimodal UIs.\n",
      "=================================================================\n",
      "Note that every example parse provided by the developer takes four lines of code and every GenieClass, GenieFunction, and GenieProperty annotation takes just one line of code.\n",
      "=================================================================\n",
      "4.1.2 ReactGenieSocial.\n",
      "=================================================================\n",
      "ReactGenieSocial is a social networking app that allows users to post pictures, comment on pictures, and share pictures with friends.\n",
      "=================================================================\n",
      "It has the basic functionalities of browsing posts, interacting with posts, and sharing posts.\n",
      "=================================================================\n",
      "In total, the app is composed of around 1034 lines of code, only 49 (5%) of which are related to building multimodal UIs.\n",
      "=================================================================\n",
      "4.1.3 ReactGenieSign.\n",
      "=================================================================\n",
      "ReactGenieSign is a business app that manages NDAs and contracts.\n",
      "=================================================================\n",
      "It has the basic functionalities of creating documents, sharing documents for signing, and user management.\n",
      "=================================================================\n",
      "In total, the app is composed of around 867 lines of code, only 51 (6%) of which are related to building multimodal UIs.\n",
      "=================================================================\n",
      "4.1.4 Summary.\n",
      "=================================================================\n",
      "These demo apps showed that when building a typical app with ReactGenie, only a small fraction (5% on average) of the code has to be written to handle multimodal interactions.\n",
      "=================================================================\n",
      "This is particularly impressive since defining multimodal interaction can be intricate and typically requires a substantial amount of code to support.\n",
      "=================================================================\n",
      "While building these demo apps, we also noticed that most UIs are naturally decomposed into components that represent different ReactGenie state instances, which made it easy to decompose the UI into GenieComponents.\n",
      "=================================================================\n",
      "4.2 Elicitation Study To understand how well the ReactGenie parser works with information extracted from the developer's code, we elicited commands from crowd workers for the ReactGenieFoodOrdering app and tested our parser.\n",
      "=================================================================\n",
      "Specifically, we would like to know: (1) RQ1: What percentage of the commands 1) are achievable with a single UI interaction on screen, 2) fall into the three targeted interactions mentioned in Section 3.1, or 3) are out of scope of ReactGenie.\n",
      "=================================================================\n",
      "(2) RQ2: How accurate the parser is when parsing commands in the targeted interactions.\n",
      "=================================================================\n",
      "4.2.1 Elicitation.\n",
      "=================================================================\n",
      "We would like to get multimodal commands that users may use in a real-world scenario.\n",
      "=================================================================\n",
      "We adopted a similar method as described as Cloudlicit [9] .\n",
      "=================================================================\n",
      "We provided the user with three screenshots (restaurant listing page, restaurant menu page, and past orders page) of the two most popular food ordering apps in the US: DoorDash and UberEats.\n",
      "=================================================================\n",
      "In our pilot study, we found that many participants thoughts on what they can do were limited to what's on-screen and what they think the current generation of voice assistants can do.\n",
      "=================================================================\n",
      "Therefore, we showed the final study participants 12 videos in a random order, containing 4 videos for each of the three categories of interactions.\n",
      "=================================================================\n",
      "Among these 12 videos, we also made sure half of them involve only voice and the other half contained voice and touch.\n",
      "=================================================================\n",
      "We recruited 50 participants from Prolific, a crowdsourcing platform.\n",
      "=================================================================\n",
      "We used the balanced sample options when finding participants, so we had 25 female and 25 male participants.\n",
      "=================================================================\n",
      "The age range of the participants was 20 to 79, with a median age of 29.\n",
      "=================================================================\n",
      "The survey took approximately five minutes to complete and we paid $2 for each participant.\n",
      "=================================================================\n",
      "From these 50 participants, we got 300 commands.\n",
      "=================================================================\n",
      "We filtered out 12 responses that were unclear or not related to the survey.\n",
      "=================================================================\n",
      "For example, one participant wrote \"various good foods to order or view that can be good\" as a command.\n",
      "=================================================================\n",
      "After filtering, we have 288 commands in the dataset.\n",
      "=================================================================\n",
      "4.2.2 RQ1: Percentages of categories of commands.\n",
      "=================================================================\n",
      "We classify the commands into three categories: (1) Simple UI interaction: The command can be achieved with a single UI interaction on screen.\n",
      "=================================================================\n",
      "For example, \"Look at Curry Up Now Menu\" when the restaurant is visible on screen.\n",
      "=================================================================\n",
      "(2) Within the three targeted interaction categories: The command falls into the three targeted interactions mentioned in Section 3.1.\n",
      "=================================================================\n",
      "For example, one participant filled \"Order me two big macs and large fries from Mcdonald's for pickup.\n",
      "=================================================================\n",
      "\" With a GUI, this command would typically be achieved via multiple taps to add the foods and configure the delivery options.\n",
      "=================================================================\n",
      "(3) Out of scope of ReactGenie: The command is out of scope of ReactGenie.\n",
      "=================================================================\n",
      "For example, \"How do I repeat past orders?\".\n",
      "=================================================================\n",
      "ReactGenie tries to help people complete complex tasks, but it does not have built-in knowledge about how to use the UI of the app.\n",
      "=================================================================\n",
      "Two researchers collaboratively labeled 30 commands to get a rubric for the rest of the commands.\n",
      "=================================================================\n",
      "We then labeled the rest of the commands (258 commands) using the rubric separately.\n",
      "=================================================================\n",
      "Both labelers labeled the same label for 224 commands and different labels for 34 commands.\n",
      "=================================================================\n",
      "Because the labels have a skewed distribution, we used Gwet's AC1 [26] to measure the inter-rater reliability.\n",
      "=================================================================\n",
      "The AC1 score is 0.83, which means the labels are highly consistent.\n",
      "=================================================================\n",
      "We resolved the disagreement and got a final label for each command.\n",
      "=================================================================\n",
      "From this analysis, we found that 100 of the elicited commands were simple UI interactions, 172 commands fall into the three targeted interactions, and 16 commands were out of the scope of ReactGenie.\n",
      "=================================================================\n",
      "This shows that users can come up with tasks that are beyond just simple UI interactions even when the type of multimodal interfaces that ReactGenie supports are not available in commercial apps.\n",
      "=================================================================\n",
      "It may also hint at user interest in the types of interactions that we propose here.\n",
      "=================================================================\n",
      "4.2.3 RQ2: Accuracy of the parser.\n",
      "=================================================================\n",
      "We tested the parser on the 172 commands that fall into the three targeted interactions.\n",
      "=================================================================\n",
      "We ran the parser based on the ReactGenieFoodOrdering app and read the generated ReactGenieDSL to see if the parses are correct.\n",
      "=================================================================\n",
      "While working on labeling the correctness, we also noticed that many of the commands are not supported by our simple demo app, e.g., ReactGenieFoodOrdering only knows delivery fees for different restaurants, but not estimated delivery times.\n",
      "=================================================================\n",
      "So we also labeled whether the feature that the command is trying to use is supported by ReactGenieFoodOrdering.\n",
      "=================================================================\n",
      "Our analysis showed that 101 commands are supported by Re-actGenieFoodOrdering and 71 commands are not supported.\n",
      "=================================================================\n",
      "Some features that are missing from ReactGenieFoodOrdering are 1) toppings/customization of a food item; 2) reviews of a restaurant or a food item; 3) delivery time estimates for restaurants.\n",
      "=================================================================\n",
      "From the 101 commands that are supported by ReactGe-nieFoodOrdering, we found that 91 commands are parsed correctly by the parser, and 10 commands are not parsed correctly.\n",
      "=================================================================\n",
      "Therefore, on this dataset, the parser has an accuracy of 90%.\n",
      "=================================================================\n",
      "We also looked at the 71 commands that are not supported by ReactGenieFoodOrdering.\n",
      "=================================================================\n",
      "These commands mention features that are not in the ReactGenieFoodOrdering app.\n",
      "=================================================================\n",
      "To our surprise, the parser also generated sensible ReactGenieDSL for the majority (38) of these commands.\n",
      "=================================================================\n",
      "ReactGenie parser approximates the request command with available features in the app for 24 of these commands.\n",
      "=================================================================\n",
      "For example, React-Genie parser generates Restaurant.GetRestaurant(name :\"pizzahut\").getFoodItems().between(field:.price,from :0,to:5) for the command \"What deals does pizza hut have?\".\n",
      "=================================================================\n",
      "In this case, the parser approximates deals with food items that are less than 5 USD.\n",
      "=================================================================\n",
      "For 14 commands, the parser would generate function calls and property accesses that are not supported by the app.\n",
      "=================================================================\n",
      "For example, the parser parsers \"What time does Chipotle open?\" to Restaurant.GetRestaurant(name:\"Chipotle\").openingTime.\n",
      "=================================================================\n",
      "In this case, ReactGenieFoodOrdering does not have the property openingTime for restaurants, but the parser is still capable enough to generate a sensible ReactGenieDSL.\n",
      "=================================================================\n",
      "In the future, the ReactGenie runtime can leverage this information to inform the user of the missing property and potentially even suggest the developer add common missing features to the app.\n",
      "=================================================================\n",
      "There are 33 unsupported commands that are not parsed correctly by the parser.\n",
      "=================================================================\n",
      "Some of them are due to the parser generating ungrammatical ReactGenieDSL and others use incorrect properties and methods.\n",
      "=================================================================\n",
      "For example, the parser parses \"Find restaurants that deliver in less than 25 minutes.\" to Restaurant.All ().matching(field:.deliveryFee,value:<25).\n",
      "=================================================================\n",
      "In this case, Re-actGenieFoodOrdering does not know the estimated delivery time of restaurants, but the correct parsing should be Restaurant.All ().between(field:.deliveryTime,from:0,to:25).\n",
      "=================================================================\n",
      "The results show that ReactGenie parser is a reasonably good implementation for parsing natural language commands to React-GenieDSL using only information extracted from the shared logic code and the few-shot examples provided by the developer.\n",
      "=================================================================\n",
      "Another interesting metric is that 104 of the 172 commands contain at least one touch point, but there are only 18 cases where these touch points are required to execute the command.\n",
      "=================================================================\n",
      "In many of these commands, the user taps relevant objects, hoping that it would help the system understand.\n",
      "=================================================================\n",
      "For example, they would tap on the \"Restaurant\" menu bar while saying \"Show me a pizza restaurant near me.\n",
      "=================================================================\n",
      "\" Another interesting observation is that when they referred to objects on screen, they often would not use a reference term like \"this\" or \"that.\n",
      "=================================================================\n",
      "\" Instead of saying \"Reorder this order\", the participant would say \"Reorder my Mendocino Farms order from Thursday.\n",
      "=================================================================\n",
      "\" This shows a potential opportunity to improve the language parser by always adding the touch context even when it seems unnecessary.\n",
      "=================================================================\n",
      "4.3 Usability Study with Prototype Applications We conducted a usability study with the ReactGenieFoodOrdering app to understand if the generated multimodal UIs are useful for end users.\n",
      "=================================================================\n",
      "We measured the performance of the multimodal UIs in terms of the time it takes to complete a task, the cognitive load, and the usability of the experience when using the app compared to the same app limited to using only the GUI.\n",
      "=================================================================\n",
      "4.3.1 Study Design.\n",
      "=================================================================\n",
      "In the study, we asked participants to complete a set of tasks using two variants of the ReactGenieFoodOrdering app, one generated by ReactGenie and one limited to only the GUI.\n",
      "=================================================================\n",
      "We used a within-subject design, where each participant completed the same tasks using both variants of the app.\n",
      "=================================================================\n",
      "For each variant of the app, we first teach the participant how to use the app using one example task, then we ask them to complete two test tasks with the variant.\n",
      "=================================================================\n",
      "After completing the two tasks, we asked them to complete a survey about their cognitive load using the system (using NASA-TLX [27] ) and the usability of the experience (using SUS [7] ).\n",
      "=================================================================\n",
      "At the end of the study, we asked the participants about their subjective preferences between the two variants of the app and their reasons for their preferences.\n",
      "=================================================================\n",
      "We designed one training task and two test tasks for each variant of the app.\n",
      "=================================================================\n",
      "The training tasks are to order the cheapest food item from the menu of two different restaurants.\n",
      "=================================================================\n",
      "The test tasks are reordering an order from two different days (today or yesterday), and finding the most recent order containing two different items.\n",
      "=================================================================\n",
      "When presenting these tasks, we described a scenario, what we want them to do, and what's the expected outcome (order placed screen or a certain screen showing a past history order).\n",
      "=================================================================\n",
      "We counterbalanced the order of the two apps and the order of the three pairs of tasks.\n",
      "=================================================================\n",
      "uses food ordering daily, two use it weekly, five use it monthly, seven use it a few times per year, and one rarely to never uses it.\n",
      "=================================================================\n",
      "All of our participants use graphical mobile interfaces daily.\n",
      "=================================================================\n",
      "Two of our participants use voice interfaces daily, four use them weekly, two use them monthly, four use them a few times per year, and four rarely to never use them.\n",
      "=================================================================\n",
      "The study took about 30 minutes to complete, and we compensated each participant with a 15 USD Amazon gift card for their time.\n",
      "=================================================================\n",
      "4.3.3 Result.\n",
      "=================================================================\n",
      "We computed the time it takes to complete each task using the graphical UI and the multimodal UI (see Figure [7] ).\n",
      "=================================================================\n",
      "The average time it takes to complete both tasks using the graphical UI is 63.6 seconds, while the average time it takes to complete a task using the multimodal UI is 33.6 seconds.\n",
      "=================================================================\n",
      "We used a paired t-test and found that the difference is statistically significant ( = 0.0004, = 3.955).\n",
      "=================================================================\n",
      "We compared NASA-TLX average scores between the two conditions (see Figure [6] left).\n",
      "=================================================================\n",
      "The average NASA-TLX score for the graphical UI is 34.5, while the average NASA-TLX score for the multimodal UI is 24.6 (note: lower is better).\n",
      "=================================================================\n",
      "We used a Wilcoxon test and found that the difference is statistically significant ( = 0.013, = 21).\n",
      "=================================================================\n",
      "We compared the average SUS scores between the two conditions (see Figure [6] right).\n",
      "=================================================================\n",
      "The average SUS score for the graphical UI is 63.3, while the average SUS score for the multimodal UI is 73.0 (note: higher is better).\n",
      "=================================================================\n",
      "We used a Wilcoxon test and found that the difference is statistically significant ( = 0.031, = 22).\n",
      "=================================================================\n",
      "11 out of 16 of our participants preferred the ReactGenie generated multimodal UI over the graphical UI.\n",
      "=================================================================\n",
      "For people who preferred the multimodal UI, the most common reason was that it was easier to use (P4, P8, P13, P16).\n",
      "=================================================================\n",
      "P2 mentioned that they would prefer to use a mix of both in the real world, which is well supported by ReactGenie.\n",
      "=================================================================\n",
      "P6 mentioned that the multimodal UI could be especially useful when they are unfamiliar with the app.\n",
      "=================================================================\n",
      "P12 mentioned that the multimodal commands allowed them to do more complex tasks with a clear path rather than searching and finding out how in the graphical UI.\n",
      "=================================================================\n",
      "For people who preferred the graphical UI, the most common reason was that the speech recognition was not accurate (P5, P7, P14).\n",
      "=================================================================\n",
      "P9 and P11 mentioned that they generally do not use voice interfaces.\n",
      "=================================================================\n",
      "4.3.4 Discussion.\n",
      "=================================================================\n",
      "The results of our usability study show that the multimodal UIs generated by ReactGenie are more efficient, have a lower cognitive load, and have higher usability compared to the graphical UIs.\n",
      "=================================================================\n",
      "These findings suggest that the ReactGenie system is successful in generating multimodal UIs that enhance the user experience, making it easier and more efficient for users to complete tasks.\n",
      "=================================================================\n",
      "The combination of graphical and voice interfaces allows users to take advantage of the strengths of each modality, resulting in a more streamlined and enjoyable experience.\n",
      "=================================================================\n",
      "5 DISCUSSION In this section, we will discuss the limitations, future work, safety, and implications of ReactGenie.\n",
      "=================================================================\n",
      "5.1 Limitations and Future Work ReactGenie is the first attempt at integrating multimodal development into the declarative GUI development process.\n",
      "=================================================================\n",
      "It provides a familiar workflow, allows reuse of state code and UI, and can understand complex multimodal commands.\n",
      "=================================================================\n",
      "However, it is far from perfect.\n",
      "=================================================================\n",
      "There are two directions that future work can improve on: 1) better voice interfaces and 2) better developer support.\n",
      "=================================================================\n",
      "5.1.1 Better Voice Interfaces.\n",
      "=================================================================\n",
      "ReactGenie accepts the user's voice input and generates text and GUI output based on the result.\n",
      "=================================================================\n",
      "We currently provide text but not voice feedback, which is easy to change by using a commercial text-to-speech module.\n",
      "=================================================================\n",
      "However, what can be an area of improvement is maintaining natural language context.\n",
      "=================================================================\n",
      "For example, if the user says, \"What is the best pizza restaurant?\" and then asks, \"What about Chinese food?\" the system should be able to understand that the user is asking about the best Chinese food instead of any Chinese food restaurant.\n",
      "=================================================================\n",
      "Note that ReactGenie can actually handle some conversations gracefully by using the current UI context as the context for the next command.\n",
      "=================================================================\n",
      "An example would be the user saying, \"Find me the cheapest hamburger at McDonald's\" and then asks, \"Order one of that\" (ReactGenieDSLOrder.GetActiveCart().addItems ([FoodItem.Current()])).\n",
      "=================================================================\n",
      "ReactGenie would present the food item after the first command and when the user says the second command, ReactGenie would know that the user is referring to the food item presented in the UI.\n",
      "=================================================================\n",
      "Another way to improve the reliability of the generated interfaces is to better leverage multimodal commands for disambiguation.\n",
      "=================================================================\n",
      "As shown in Section 4.2.3, many of our elicited commands include redundant information from voice and the GUI.\n",
      "=================================================================\n",
      "Future work can leverage this redundancy and provide extra GUI context to the language parser to further push the parser's accuracy closer to 100%.\n",
      "=================================================================\n",
      "5.1.2 Better Developer Support.\n",
      "=================================================================\n",
      "Although ReactGenie provides a customizable and easy way of programming multimodal apps, it can still be improved.\n",
      "=================================================================\n",
      "One area that we see as a potential improvement is to reduce the number of examples necessary and to also increase the effectiveness of the examples.\n",
      "=================================================================\n",
      "The majority of these examples are there for teaching the parser how to generate syntactically correct ReactGenieDSL code.\n",
      "=================================================================\n",
      "However, given we have the interpreter, we can potentially use it as an example generator to teach the parser how to generate syntactically correct ReactGenieDSL code, similar to the method used in SEMPRE [11] or Genie [16] .\n",
      "=================================================================\n",
      "Another route is to fine-tune the Codex model with the ReactGenieDSL code generated by the interpreter so that the interpreter can generate syntactically correct ReactGenieDSL code with fewer examples.\n",
      "=================================================================\n",
      "Future extensions to the ReactGenie framework can also help developers to identify potential voice commands that the user may want to say.\n",
      "=================================================================\n",
      "Using ReactGenieFoodOrdering as an example, its API only supports 59% of the commands that we elicited from crowd workers.\n",
      "=================================================================\n",
      "Some top categories of unimplemented commands are about delivery time (mentioned in 8 commands), food customization options [(8)] , discount/deal information [(7)] , pickup/delivery support of restaurants (6), food types (e.g., vegetarian or vegan) (6), and calorie/health/allergy information (6).\n",
      "=================================================================\n",
      "If we can implement these commands, we can potentially reduce the number of unsupported commands by more than 50%.\n",
      "=================================================================\n",
      "Future work can consider embedding elicitation studies directly into the app development cycle, or the framework could record unsupported commands from actual users and use this data as feedback to the development team to help improve the system.\n",
      "=================================================================\n",
      "5.2 Safety and Implications ReactGenie uses a machine learning model to understand the users' commands.\n",
      "=================================================================\n",
      "This may bring safety issues when the wrong command is interpreted and executed.\n",
      "=================================================================\n",
      "This risk can be reduced by having a more accurate language parser, and ReactGenie already has relatively high accuracy.\n",
      "=================================================================\n",
      "Also, compared with an end-to-end natural language assistant like ChatGPT [5] , ReactGenie allows more control over the presented information and performed actions.\n",
      "=================================================================\n",
      "ReactGenie will only present information that exists in the app and will not hallucinate information.\n",
      "=================================================================\n",
      "One particular case of error is when the user asks for delivery time but because the app does not support delivery time estimation, ReactGenie returns the delivery fee instead.\n",
      "=================================================================\n",
      "In this case, the text feedback mechanism can be used to inform the user of the information that is actually returned.\n",
      "=================================================================\n",
      "In the future, an error correction mechanism would be useful for the user to report the error and the developer to fix it.\n",
      "=================================================================\n",
      "For performed actions, ReactGenie gives text feedback and renders the related UI elements to ensure the user is aware of the command that is being executed, so when there is an error, the user can easily identify and recover from it.\n",
      "=================================================================\n",
      "A design decision we made while creating the three demo apps is not to expose non-recoverable actions to voice.\n",
      "=================================================================\n",
      "For example, in the ReactGenieFoodOrdering app, the user can browse items, add items to the cart, and go to the checkout page via voice, but placing the order will only present the checkout page and require the user to click the \"Place Order\" button to place the order.\n",
      "=================================================================\n",
      "This way, the irreversible action is only triggered through GUI where there is little room for error.\n",
      "=================================================================\n",
      "Another implication of ReactGenie is the possible social implications of multimodal interaction.\n",
      "=================================================================\n",
      "ReactGenie encourages users to use voice and touch to quickly achieve their goals without the need to go through multiple UI actions and exploration steps.\n",
      "=================================================================\n",
      "The benefit of ReactGenie comes from the expressiveness of voice and touch, but voice interfaces may not always be appropriate.\n",
      "=================================================================\n",
      "One possibility is to explore silent voice interfaces like those presented by Denby et al.\n",
      "=================================================================\n",
      "[22] that can be used in public spaces.\n",
      "=================================================================\n",
      "6 CONCLUSION Commercial user interfaces have stagnated with the same mobile GUI for the past decade.\n",
      "=================================================================\n",
      "Although these GUIs work well for communicating exact information (e.g., from a menu) and binary actions (e.g., using a button), they are not expressive enough to communicate and collect complex information such as the way a waiter or waitress can obtain a person's order from a restaurant menu.\n",
      "=================================================================\n",
      "ReactGenie attempts to break that UI stagnation by allowing developers to create multimodal UIs that allow for more expressiveness than traditional GUIs, with little additional programming effort.\n",
      "=================================================================\n",
      "ReactGenie accomplishes this by introducing a new object-oriented state programming framework coupled with a powerful natural language understanding module that leverages the capabilities of LLMs.\n",
      "=================================================================\n",
      "In this paper, we demonstrated the expressiveness, usefulness, and accuracy of the ReactGenie framework.\n",
      "=================================================================\n",
      "In the future, through the introduction of developer tools based on frameworks like ReactGenie, and the research on multimodal interaction these tools enable, we hope to see humans communicating with computers more expressively and more easily.\n",
      "=================================================================\n"
     ]
    }
   ],
   "source": [
    "for i in xml_sentence_lst:\n",
    "    print(i)\n",
    "    print(\"=================================================================\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from thefuzz import fuzz\n",
    "from thefuzz import process\n",
    "\n",
    "matched_pairs = []\n",
    "choices = equ_sentence_lst\n",
    "for xml_sentence in xml_sentence_lst:\n",
    "    matched = process.extractOne(xml_sentence, choices, scorer=fuzz.partial_ratio)\n",
    "    equ_text = matched[0]\n",
    "    score = matched[1]\n",
    "    if score >= 90 and len(xml_sentence)/len(equ_text) >= 0.85 and len(xml_sentence)/len(equ_text) <= 1.18:\n",
    "        matched_pairs.append((xml_sentence, equ_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-We found in a user study ( =16) that the app generated with ReactGenie significantly reduced cognitive load and task completion time compared to a GUI-only app.\n",
      "We found in a user study ($N$=16) that the app generated with \\mr{} significantly reduced cognitive load and task completion time compared to a GUI-only app.\n",
      "==================================\n",
      "We used a paired t-test and found that the difference is statistically significant ( = 0.0004, = 3.955).\n",
      "We used a paired t-test and found that the difference is statistically significant ($p=0.0004, t=3.955$).\n",
      "==================================\n",
      "We used a Wilcoxon test and found that the difference is statistically significant ( = 0.013, = 21).\n",
      "We used a Wilcoxon test and found that the difference is statistically significant ($p=0.013, z=21$).\n",
      "==================================\n",
      "We used a Wilcoxon test and found that the difference is statistically significant ( = 0.031, = 22).\n",
      "We used a Wilcoxon test and found that the difference is statistically significant ($p=0.031, z=22$).\n",
      "==================================\n"
     ]
    }
   ],
   "source": [
    "for i in matched_pairs:\n",
    "    print(i[0])\n",
    "    print(i[1])\n",
    "    print(\"==================================\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.30268199233716475"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len()/len(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "83"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = r\"Using the fact that Z in Eq.( [1] ) is close to 1, it is possible to approximate the occupation probability of the binding site by [16] : P ij = 1 1 + 1/(n j W ij ) .In the following, we denote by n the maximum number of TF molecules that can arise when a gene is fully on, taking for simplicity \"\n",
    "b = r\"Using the fact that $Z$ in Eq.~() is close to 1, it is possible to approximate the occupation probability of the binding site by~: \\begin{equation} P_{ij} = \\frac{1}{1 + 1/(n_j W_{ij})} \\; . \\end{equation} In the following, we denote by $n$ the maximum number of TF molecules that can arise when a gene is fully ``on'', taking for simplicity this maximum to be independent of $j$.\"\n",
    "fuzz.ratio(a,b)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
