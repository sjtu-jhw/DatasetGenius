<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Adapt Your Teacher: Improving Knowledge Distillation for Exemplar-free Continual Learning</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName><forename type="first">Filip</forename><surname>Szatkowski</surname></persName>
							<email>filip.szatkowski.dokt@pw.edu.pl</email>
							<affiliation key="aff0">
								<note type="raw_affiliation"><label>1</label> Warsaw University of Technology,</note>
								<orgName type="institution">Warsaw University of Technology</orgName>
							</affiliation>
							<affiliation key="aff1">
								<note type="raw_affiliation"><label>2</label> IDEAS NCBR,</note>
								<orgName type="department">IDEAS NCBR</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Mateusz</forename><surname>Pyla</surname></persName>
							<affiliation key="aff1">
								<note type="raw_affiliation"><label>2</label> IDEAS NCBR,</note>
								<orgName type="department">IDEAS NCBR</orgName>
							</affiliation>
							<affiliation key="aff2">
								<note type="raw_affiliation"><label>3</label> Jagiellonian University, Faculty of Mathematics and Computer Science,</note>
								<orgName type="department">Faculty of Mathematics and Computer Science</orgName>
								<orgName type="institution">Jagiellonian University</orgName>
							</affiliation>
							<affiliation key="aff3">
								<note type="raw_affiliation"><label>4</label> Jagiellonian University, Doctoral School of Exact and Natural Sciences,</note>
								<orgName type="department">Doctoral School of Exact and Natural Sciences</orgName>
								<orgName type="institution">Jagiellonian University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Marcin</forename><surname>Przewięźlikowski</surname></persName>
							<affiliation key="aff1">
								<note type="raw_affiliation"><label>2</label> IDEAS NCBR,</note>
								<orgName type="department">IDEAS NCBR</orgName>
							</affiliation>
							<affiliation key="aff2">
								<note type="raw_affiliation"><label>3</label> Jagiellonian University, Faculty of Mathematics and Computer Science,</note>
								<orgName type="department">Faculty of Mathematics and Computer Science</orgName>
								<orgName type="institution">Jagiellonian University</orgName>
							</affiliation>
							<affiliation key="aff3">
								<note type="raw_affiliation"><label>4</label> Jagiellonian University, Doctoral School of Exact and Natural Sciences,</note>
								<orgName type="department">Doctoral School of Exact and Natural Sciences</orgName>
								<orgName type="institution">Jagiellonian University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Sebastian</forename><surname>Cygert</surname></persName>
							<affiliation key="aff1">
								<note type="raw_affiliation"><label>2</label> IDEAS NCBR,</note>
								<orgName type="department">IDEAS NCBR</orgName>
							</affiliation>
							<affiliation key="aff4">
								<note type="raw_affiliation"><label>5</label> Gdańsk University of Technology,</note>
								<orgName type="institution">Gdańsk University of Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Bartłomiej</forename><surname>Twardowski</surname></persName>
							<affiliation key="aff1">
								<note type="raw_affiliation"><label>2</label> IDEAS NCBR,</note>
								<orgName type="department">IDEAS NCBR</orgName>
							</affiliation>
							<affiliation key="aff5">
								<note type="raw_affiliation"><label>6</label> Autonomous University of Barcelona,</note>
								<orgName type="institution">Autonomous University of Barcelona</orgName>
							</affiliation>
							<affiliation key="aff6">
								<note type="raw_affiliation"><label>7</label> Computer Vision Center,</note>
								<orgName type="department">Computer Vision Center</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Tomasz</forename><surname>Trzciński</surname></persName>
							<affiliation key="aff0">
								<note type="raw_affiliation"><label>1</label> Warsaw University of Technology,</note>
								<orgName type="institution">Warsaw University of Technology</orgName>
							</affiliation>
							<affiliation key="aff1">
								<note type="raw_affiliation"><label>2</label> IDEAS NCBR,</note>
								<orgName type="department">IDEAS NCBR</orgName>
							</affiliation>
							<affiliation key="aff7">
								<note type="raw_affiliation"><label>8</label> Tooploox</note>
								<address>
									<settlement>Tooploox</settlement>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Adapt Your Teacher: Improving Knowledge Distillation for Exemplar-free Continual Learning</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">EF6AA617943F48FAA9AADE4888C63016</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2023-10-13T09:56+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p><s coords="1,62.07,301.74,225.95,8.59;1,50.11,313.69,237.90,8.59;1,50.11,325.65,193.27,8.59">In this work, we investigate exemplar-free class incremental learning (CIL) with knowledge distillation (KD) as a regularization strategy, aiming to prevent forgetting.</s><s coords="1,246.49,325.65,39.87,8.59;1,50.11,337.60,236.25,8.59;1,50.11,349.56,236.25,8.59;1,50.11,361.51,133.63,8.59">KD-based methods are successfully used in CIL, but they often struggle to regularize the model without access to exemplars of the training data from previous tasks.</s><s coords="1,186.86,361.51,99.51,8.59;1,50.11,373.47,236.25,8.59;1,50.11,385.42,236.25,8.59;1,50.11,397.38,20.61,8.59">Our analysis reveals that this issue originates from substantial representation shifts in the teacher network when dealing with out-of-distribution data.</s><s coords="1,75.60,397.38,212.50,8.59;1,50.11,409.33,176.43,8.59">This causes large errors in the KD loss component, leading to performance degradation in CIL.</s><s coords="1,229.03,409.33,58.99,8.59;1,50.11,421.29,236.25,8.59;1,49.50,433.24,236.85,8.59;1,50.11,445.20,237.99,8.59">Inspired by recent test-time adaptation methods, we introduce Teacher Adaptation (TA), a method that concurrently updates the teacher and the main model during incremental training.</s><s coords="1,49.75,457.15,238.26,8.59;1,50.11,469.11,236.25,8.59;1,50.11,481.06,237.99,8.59">Our method seamlessly integrates with KD-based CIL approaches and allows for consistent enhancement of their performance across multiple exemplar-free CIL benchmarks.</s></p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1." coords="1,50.11,530.02,76.84,10.75">Introduction</head><p><s coords="1,62.07,551.63,225.95,8.64;1,50.11,563.40,236.25,8.82;1,50.11,575.54,236.25,8.64;1,50.11,587.49,236.25,8.64;1,50.11,599.45,20.79,8.64">One of the most challenging continual learning scenarios is class incremental learning (CIL) <ref type="bibr" coords="1,209.83,563.58,15.89,8.64" target="#b32">[33,</ref><ref type="bibr" coords="1,228.23,563.58,11.91,8.64" target="#b22">23]</ref>, where the model is trained to classify objects incrementally from the sequence of tasks, without forgetting the previously learned ones.</s><s coords="1,74.00,599.45,212.36,8.64;1,50.11,611.22,237.90,8.82;1,50.11,623.18,236.25,8.82;1,50.11,635.31,97.37,8.64">A simple and effective method of reducing forgetting is by leveraging exemplars <ref type="bibr" coords="1,160.27,611.40,15.85,8.64" target="#b27">[28,</ref><ref type="bibr" coords="1,178.61,611.40,12.48,8.64" target="#b15">16,</ref><ref type="bibr" coords="1,193.58,611.40,7.50,8.64" target="#b4">5,</ref><ref type="bibr" coords="1,203.57,611.40,13.32,8.64" target="#b25">26]</ref> of previously encountered training examples, e.g. by replaying them or using them for regularization.</s><s coords="1,152.67,635.31,133.70,8.64;1,50.11,647.27,236.25,8.64;1,50.11,659.22,88.03,8.64">However, this approach presents challenges, particularly in terms of additional storage needs and privacy concerns.</s><s coords="1,141.23,659.22,145.13,8.64;1,50.11,671.18,236.25,8.64;1,50.11,683.13,76.40,8.64">Therefore, recently there has been a notable surge of interest in methods for more challenging exemplar-free CIL.</s><s coords="1,320.82,620.83,225.95,8.64;1,308.86,632.78,236.91,8.64;1,308.86,644.74,236.25,8.64;1,308.86,656.69,236.25,8.64;1,308.53,668.65,154.57,8.64">A common approach for exemplar-free CIL is knowledge distillation (KD), where the current model (student) is trained on the new data with a regularization term that minimizes the output difference with the previous model (teacher), which is kept frozen <ref type="bibr" coords="1,443.84,668.65,15.41,8.64" target="#b20">[21]</ref>.</s><s coords="1,471.47,668.65,73.99,8.64;1,308.86,680.60,237.91,8.64;1,308.86,692.56,236.25,8.64;1,308.86,704.51,178.29,8.64">Since then, many methods such as iCaRL <ref type="bibr" coords="1,408.42,680.60,15.41,8.64" target="#b27">[28]</ref>, EEIL <ref type="bibr" coords="1,455.21,680.60,10.72,8.64" target="#b5">[6]</ref>, LUCIR <ref type="bibr" coords="1,505.51,680.60,15.42,8.64" target="#b13">[14]</ref>, Pod-NET <ref type="bibr" coords="1,331.71,692.56,15.42,8.64" target="#b10">[11]</ref>, SSIL <ref type="bibr" coords="1,378.20,692.56,10.72,8.64" target="#b0">[1]</ref>, or DMC <ref type="bibr" coords="1,433.53,692.56,15.89,8.64" target="#b40">[41,</ref><ref type="bibr" coords="1,452.51,692.56,13.35,8.64" target="#b19">20]</ref> employed KD, but most of them use exemplars or external data.</s><s coords="2,529.66,223.88,15.62,8.64;2,50.11,235.83,495.00,8.64;2,50.11,247.79,330.11,8.64">Our method leads to more consistent representations, as visualized by the CKA <ref type="bibr" coords="2,351.47,235.83,16.61,8.64" target="#b17">[18]</ref> between the representations of the new data obtained in the teacher and student models while learning the second task (middle).</s><s coords="2,383.31,247.79,161.80,8.64;2,50.11,259.74,65.68,8.64">KD with TA leads to better task-agnostic accuracy (right).</s><s coords="2,118.88,259.74,246.27,8.64">We conduct the experiments on CIFAR100 split into 10 tasks.</s></p><p><s coords="2,62.07,293.22,224.29,8.64;2,50.11,305.17,236.25,8.64;2,50.11,317.13,236.25,8.64;2,50.11,329.08,94.33,8.64">Exemplar-free CIL still remains challenging <ref type="bibr" coords="2,239.10,293.22,16.53,8.64" target="#b31">[32]</ref> for KD methods due to the possibility of significant distribution drift in subsequent tasks, which leads to large errors during training with KD loss.</s><s coords="2,151.69,329.08,134.68,8.64;2,50.11,341.04,236.25,8.64;2,50.11,352.99,236.25,8.64;2,50.11,364.95,236.25,8.64;2,50.11,376.90,236.25,8.64;2,50.11,388.86,237.99,8.64">Motivated by the recent domain adaptation methods <ref type="bibr" coords="2,131.97,341.04,15.89,8.64" target="#b33">[34,</ref><ref type="bibr" coords="2,150.39,341.04,11.92,8.64" target="#b30">31]</ref>, we examine the role of batch normalization (BN) statistics in CIL training with KD loss and conjecture that in standard KD methods, the KD loss between models with different BN statistics may introduce unwanted model updates due to the data distribution shifts.</s><s coords="2,49.80,400.81,236.56,8.64;2,50.11,412.77,229.40,8.64">To avoid this, we propose to continuously adapt them to the new data for the teacher model while training the student.</s></p><p><s coords="2,62.07,426.36,224.65,8.64;2,50.11,438.31,236.25,8.64;2,50.11,450.27,237.90,8.64;2,50.11,462.22,25.16,8.64">We show that adapting the teacher BN statistics to the new task can significantly lower KD loss without affecting the CE loss, which leads to reduced changes in representations (Figure <ref type="figure" coords="2,64.70,462.22,3.52,8.64" target="#fig_1">2</ref>).</s><s coords="2,78.23,462.22,208.30,8.64;2,50.11,474.18,236.25,8.64;2,50.11,486.13,236.25,8.64;2,50.11,498.09,236.25,8.64;2,50.11,510.04,19.09,8.64">We note that TA has been used in standard KD <ref type="bibr" coords="2,259.82,462.22,16.46,8.64" target="#b42">[43]</ref> or in the online continual learning with exemplars <ref type="bibr" coords="2,238.52,474.18,15.19,8.64" target="#b11">[12]</ref>, but we are the first to apply it to exemplar-free CIL scenario, where the teacher and the model are trained on non-overlapping data.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2." coords="2,50.11,537.09,85.22,10.75">Related works</head><p><s coords="2,50.11,559.03,236.25,9.03;2,50.11,571.37,236.25,8.64;2,50.11,583.33,100.90,8.64">Class Incremental Learning (CIL) <ref type="bibr" coords="2,200.06,559.42,15.85,8.64" target="#b32">[33,</ref><ref type="bibr" coords="2,218.41,559.42,13.33,8.64" target="#b22">23]</ref> aims to learn incrementally from a stream of tasks, without the knowledge about the task identifier.</s><s coords="2,157.47,583.33,129.05,8.64;2,50.11,595.28,236.60,8.64;2,50.11,607.24,237.49,8.64;2,50.11,619.19,181.75,8.64">Most CIL methods store either exemplars or features from the previous tasks in the replay buffer <ref type="bibr" coords="2,76.18,607.24,15.69,8.64" target="#b27">[28,</ref><ref type="bibr" coords="2,94.36,607.24,12.42,8.64" target="#b15">16,</ref><ref type="bibr" coords="2,109.27,607.24,7.44,8.64" target="#b4">5,</ref><ref type="bibr" coords="2,119.19,607.24,11.77,8.64" target="#b25">26]</ref>, modify the structure of the model <ref type="bibr" coords="2,271.91,607.24,15.70,8.64" target="#b35">[36,</ref><ref type="bibr" coords="2,50.11,619.19,13.35,8.64" target="#b34">35]</ref> or regularize changes in model <ref type="bibr" coords="2,197.38,619.19,15.89,8.64" target="#b16">[17,</ref><ref type="bibr" coords="2,215.98,619.19,11.91,8.64" target="#b20">21]</ref>.</s><s coords="2,235.60,619.19,51.25,8.64;2,50.11,631.15,237.50,8.64;2,50.11,643.10,236.25,8.64;2,50.11,655.06,144.80,8.64">Modern CIL methods usually combine those approaches <ref type="bibr" coords="2,230.52,631.15,10.91,8.64" target="#b5">[6,</ref><ref type="bibr" coords="2,244.32,631.15,12.50,8.64" target="#b36">37,</ref><ref type="bibr" coords="2,259.71,631.15,12.50,8.64" target="#b28">29,</ref><ref type="bibr" coords="2,275.10,631.15,12.50,8.64" target="#b27">28,</ref><ref type="bibr" coords="2,50.11,643.10,12.50,8.64" target="#b20">21,</ref><ref type="bibr" coords="2,66.16,643.10,8.36,8.64" target="#b0">1]</ref> and often rely heavily on exemplars, which raises issues with data storage and privacy.</s></p><p><s coords="2,62.07,668.65,224.65,8.64;2,50.11,680.60,237.90,8.64;2,50.11,692.56,236.25,8.64;2,50.11,704.51,46.07,8.64">Regularization methods for continual learning employ either parameter regularization <ref type="bibr" coords="2,174.79,680.60,15.75,8.64" target="#b16">[17,</ref><ref type="bibr" coords="2,193.02,680.60,12.44,8.64" target="#b38">39,</ref><ref type="bibr" coords="2,207.95,680.60,8.29,8.64" target="#b1">2]</ref> or functional regularization through knowledge distillation (KD) on model activations.</s><s coords="2,99.82,704.51,187.79,8.64;2,308.86,293.22,237.50,8.64;2,308.51,305.17,15.89,8.64">In CL, KD was first applied in LwF <ref type="bibr" coords="2,248.40,704.51,15.42,8.64" target="#b20">[21]</ref>, and, since then, has been widely used <ref type="bibr" coords="2,444.63,293.22,15.89,8.64" target="#b26">[27,</ref><ref type="bibr" coords="2,463.16,293.22,12.50,8.64" target="#b13">14,</ref><ref type="bibr" coords="2,478.29,293.22,12.50,8.64" target="#b27">28,</ref><ref type="bibr" coords="2,493.43,293.22,12.50,8.64" target="#b25">26,</ref><ref type="bibr" coords="2,508.56,293.22,7.52,8.64" target="#b0">1,</ref><ref type="bibr" coords="2,518.72,293.22,12.50,8.64" target="#b10">11,</ref><ref type="bibr" coords="2,533.85,293.22,12.50,8.64" target="#b9">10,</ref><ref type="bibr" coords="2,308.51,305.17,11.92,8.64" target="#b39">40]</ref>.</s><s coords="2,329.69,305.17,215.58,8.64;2,308.86,317.13,236.25,8.64;2,308.86,329.08,77.55,8.64">However, most of those methods are impractical for exemplar-free settings, as their performance heavily relies on exemplar buffer.</s><s coords="2,308.86,340.65,236.25,9.03;2,308.86,352.99,237.90,8.64;2,308.86,364.95,237.90,8.64;2,308.86,376.90,139.52,8.64">Modifying the teacher model in KD was recently explored in a setting where both models operate on the same domain <ref type="bibr" coords="2,333.49,364.95,15.89,8.64" target="#b42">[43,</ref><ref type="bibr" coords="2,353.69,364.95,13.35,8.64" target="#b21">22]</ref> and the teacher is adapted through metalearning to better guide the student.</s><s coords="2,451.47,376.90,93.65,8.64;2,308.86,388.86,236.25,8.64;2,308.86,400.81,124.80,8.64">La-MAML <ref type="bibr" coords="2,498.23,376.90,16.52,8.64" target="#b11">[12]</ref> applies a similar idea in online continual learning, using exemplars for the outer loop optimization.</s><s coords="2,308.86,412.38,236.25,9.03;2,308.86,424.72,236.25,8.64;2,308.86,436.68,200.57,8.64">Batch Normalization (BN) <ref type="bibr" coords="2,429.21,412.77,16.73,8.64" target="#b14">[15]</ref> is widely used in deep learning, but it was shown to be problematic in CL <ref type="bibr" coords="2,517.38,424.72,16.73,8.64" target="#b29">[30]</ref> as its statistics change drastically between the tasks.</s><s coords="2,512.90,436.68,33.86,8.64;2,308.86,448.63,236.42,8.64;2,308.86,460.59,236.24,8.64;2,308.86,472.54,85.31,8.64">Alternative normalization approaches such as LayerNorm <ref type="bibr" coords="2,521.54,448.63,11.75,8.64" target="#b3">[4]</ref> or GroupNorm <ref type="bibr" coords="2,362.08,460.59,16.73,8.64" target="#b37">[38]</ref> often lead to decreased performance in standard CL models.</s><s coords="2,398.78,472.54,146.33,8.64;2,308.86,484.50,185.60,8.64">Several domain adaptation methods use BN statistics for domain transfer <ref type="bibr" coords="2,460.14,484.50,15.89,8.64" target="#b33">[34,</ref><ref type="bibr" coords="2,478.57,484.50,11.91,8.64" target="#b30">31]</ref>.</s><s coords="2,497.70,484.50,47.42,8.64;2,308.86,496.45,236.25,8.64;2,308.86,508.41,176.70,8.64">CL-specific normalization methods also have been proposed <ref type="bibr" coords="2,501.25,496.45,15.72,8.64" target="#b24">[25,</ref><ref type="bibr" coords="2,519.46,496.45,7.16,8.64" target="#b6">7]</ref>, but they are not suited for exemplar-free setting.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3." coords="2,308.86,530.51,51.80,10.75">Method</head><p><s coords="2,320.82,551.02,224.30,8.82;2,308.86,563.16,186.07,8.64">We propose Teacher Adaptation -a simple, yet effective method for CIL with KD presented in Figure <ref type="figure" coords="2,487.59,563.16,3.67,8.64" target="#fig_0">1</ref>.</s><s coords="2,498.02,563.16,47.10,8.64;2,308.86,575.11,236.25,8.64;2,308.86,587.07,236.25,8.64;2,308.86,599.02,236.25,8.64;2,308.86,610.98,236.25,8.64;2,308.86,622.93,123.95,8.64">Our method allows the teacher model to continuously update BN statistics alongside the student when training on the new data, which addresses the problem of diverging BN statistics between the teacher and student model caused by the shifts in training data between subsequent tasks.</s><s coords="2,308.86,634.50,198.24,8.96">Knowledge Distillation in Continual Learning.</s><s coords="2,516.37,634.89,30.40,8.64;2,308.86,646.84,236.25,8.64;2,308.86,658.48,236.25,9.65;2,308.86,670.44,237.49,9.65;2,308.50,682.71,126.05,8.64;3,49.75,236.75,236.61,9.65;3,50.11,248.71,236.25,8.96;3,50.11,260.98,89.85,8.64">Knowledge distillation (KD) methods for continual learning save the (teacher) model Θ t trained after each task t and use it during learning the (student) model Θ t+1 on new task t + 1, with general learning objective: where L CE is the cross-entropy loss, L KD is the KD loss and λ is the coefficient that controls the trade-off between stability and plasticity.</s></p><formula xml:id="formula_0" coords="2,386.13,704.20,159.65,9.65">L = L CE + λL KD ,<label>(1)</label></formula><formula xml:id="formula_1" coords="3,87.24,75.99,416.49,43.25">T10S10 T20S5 T11S50 T26S50 Acc Inc ↑ F org Inc ↓ Acc Inc ↑ F org Inc ↓ Acc Inc ↑ F org Inc ↓ Acc Inc ↑ F org Inc ↓ a)</formula><p><s coords="3,62.07,272.94,224.30,8.64;3,50.11,284.89,29.98,8.64">The most popular formulation of KD loss was proposed in <ref type="bibr" coords="3,60.82,284.89,15.42,8.64" target="#b20">[21]</ref>.</s><s coords="3,84.13,284.89,202.90,8.64;3,50.11,296.85,62.81,8.64">Following <ref type="bibr" coords="3,128.32,284.89,10.71,8.64" target="#b0">[1]</ref>, we refer to it as global KD (GKD) and define it as:</s></p><formula xml:id="formula_2" coords="3,94.52,314.93,192.51,31.18">L GKD (y o , ŷo ) = − |Ct| i=1 p (i) o log p(i) o ,<label>(2)</label></formula><p><s coords="3,49.75,356.46,236.60,9.65;3,50.11,370.35,61.42,9.65">where |C t | is the number of classes learned by previous model Θ t and p</s></p><formula xml:id="formula_3" coords="3,111.54,367.22,28.31,12.46">(i) o , p<label>(i)</label></formula><p><s coords="3,130.80,370.67,157.21,9.00;3,50.11,382.63,34.32,8.64">o are temperature-scaled softmax probabilities:</s></p><formula xml:id="formula_4" coords="3,89.19,405.96,25.07,12.69">p (i) o =</formula><p><s coords="3,126.41,401.29,4.64,8.74;3,131.05,399.72,16.83,6.12;3,128.74,420.07,3.30,6.12;3,134.60,414.20,21.47,9.57;3,158.83,408.03,23.34,8.74">e yo/T j e yo/T , po</s></p><formula xml:id="formula_5" coords="3,182.66,399.72,104.37,26.47">(i) = e ŷo/T j e ŷo/T<label>(3)</label></formula><p><s coords="3,49.64,436.23,238.38,8.96;3,50.11,449.79,98.54,8.96">We denote temperature parameter with T and use o to emphasise that the logits y</s></p><formula xml:id="formula_6" coords="3,148.66,446.65,30.32,12.46">(i) o , ŷ(i)</formula><p><s coords="3,169.58,449.93,116.79,9.18;3,50.11,462.06,80.83,8.64">o only relate to old classes from previous tasks.</s></p><p><s coords="3,62.07,474.02,224.29,8.64;3,50.11,485.97,237.91,8.64;3,50.11,497.93,237.91,8.64;3,50.11,509.88,208.63,8.64">Ahn et al. <ref type="bibr" coords="3,103.75,474.02,11.65,8.64" target="#b0">[1]</ref> noticed that GKD formulation encourages forgetting of previous tasks and proposed task-wise knowledge distillation (TKD), which computes softmax probabilities separately across the model classification heads:</s></p><formula xml:id="formula_7" coords="3,85.15,529.08,201.88,30.32">L T KD (y o , ŷo ) = t i=1 D KL (p (i) o log p(i) o ),<label>(4)</label></formula><p><s coords="3,49.75,572.69,206.64,9.65">where D KL is Kullback-Leibler divergence and p</s></p><formula xml:id="formula_8" coords="3,256.39,569.55,29.47,12.46">(i) o , p<label>(i)</label></formula><p><s coords="3,276.82,575.89,3.93,6.12;3,50.11,584.64,236.24,8.96;3,50.11,596.92,55.89,8.64">o are computed task-wise across the outputs for task i as in Equation ( <ref type="formula" coords="3,92.06,596.92,3.48,8.64" target="#formula_5">3</ref>)).</s></p><p><s coords="3,49.78,608.48,88.78,8.96">Teacher Adaptation.</s><s coords="3,141.65,608.87,144.71,8.64;3,50.11,620.83,237.49,8.64;3,49.75,632.78,236.61,8.64;3,50.11,644.42,236.25,9.65;3,50.11,656.37,58.35,9.65">Most models used in CIL for vision tasks are convolutional neural networks such as ResNet <ref type="bibr" coords="3,268.70,620.83,15.12,8.64" target="#b12">[13]</ref>, which typically use BN layers and keep the parameters and statistics of those layers in the teacher model Θ t fixed during learning Θ t+1 .</s><s coords="3,111.58,656.69,176.43,8.64;3,50.11,668.65,236.42,8.64;3,50.11,680.60,33.78,8.64">However, when changing the task, BN statistics in both models quickly diverge, which leads to higher KD loss.</s><s coords="3,86.83,680.60,199.53,8.64;3,50.11,692.56,236.41,8.64;3,50.11,704.51,232.98,8.64">Gradient updates in this case not only regularize the model towards the previous tasks, but also compensate for the changes in BN statistics, harming the learning process.</s></p><p><s coords="3,320.82,237.07,224.30,8.64;3,308.86,249.03,236.25,8.64;3,308.50,260.80,137.03,8.82">Inspired by test-time adaptation methods <ref type="bibr" coords="3,480.00,237.07,15.12,8.64" target="#b33">[34]</ref>, we propose to reduce this negative interference with a simple method that we label Teacher Adaptation (TA).</s><s coords="3,448.01,260.98,97.10,8.64;3,308.86,272.94,236.25,8.64;3,308.86,284.89,89.62,8.64">Our method updates BN statistics of both models simultaneously on new data while learning the new task.</s><s coords="3,402.41,284.89,142.86,8.64;3,308.86,296.85,236.25,8.64;3,308.86,308.80,225.67,8.64">As shown in Figure <ref type="figure" coords="3,485.50,284.89,3.81,8.64" target="#fig_1">2</ref>, it allows for significantly reduced KD loss over learning from sequential tasks in CIL, which improves the overall model stability.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4." coords="3,308.86,334.04,77.04,10.75">Experiments</head><p><s coords="3,308.53,355.37,148.70,8.96">TA on standard CIL benchmarks.</s><s coords="3,465.78,355.76,80.98,8.64;3,308.86,367.71,236.25,8.64;3,308.86,379.67,236.25,8.64;3,308.86,391.62,236.25,8.64;3,308.86,403.58,30.76,8.64">We evaluate knowledge distillation approaches described in Section 3 on the standard continual learning benchmarks CIFAR100 <ref type="bibr" coords="3,528.38,379.67,16.73,8.64" target="#b18">[19]</ref> and ImageNet-Subset <ref type="bibr" coords="3,396.74,391.62,10.52,8.64" target="#b8">[9]</ref>, each containing images from 100 classes.</s><s coords="3,343.08,403.58,202.04,8.64;3,308.86,415.53,215.28,8.64">For experiments on CIFAR100, we keep the class order from iCaRL <ref type="bibr" coords="3,388.72,415.53,16.73,8.64" target="#b27">[28]</ref> and we use ResNet32 <ref type="bibr" coords="3,504.87,415.53,15.42,8.64" target="#b12">[13]</ref>.</s><s coords="3,531.31,415.53,13.97,8.64;3,308.86,427.49,161.26,8.64">For ImageNet Subset, we use ResNet18 <ref type="bibr" coords="3,451.21,427.49,15.13,8.64" target="#b12">[13]</ref>.</s><s coords="3,473.22,427.49,73.55,8.64;3,308.86,439.44,236.25,8.64;3,308.86,451.01,237.49,9.03;3,308.86,462.96,236.74,9.03;3,308.86,475.31,236.25,8.64;3,308.86,487.26,236.25,8.64;3,308.86,499.22,56.89,8.64">We investigate different class splits, which we denote using the total number of tasks T (which includes the first pretraining task if present), and the number of classes in the first task S. We use FACIL framework provided by Masana et al. <ref type="bibr" coords="3,462.40,475.31,15.39,8.64" target="#b22">[23]</ref>, and always use the same hyperparameters for each KD method within a single setting.</s><s coords="3,370.28,499.22,175.00,8.64;3,308.86,511.17,236.25,8.64;3,308.86,523.13,112.40,8.64">We train the network on each new task for 200 epochs in all experiments, using SGD optimizer without momentum or weight decay.</s><s coords="3,424.37,523.13,120.75,8.64;3,308.86,535.08,236.25,8.64;3,308.86,547.04,237.99,8.64">Following Zhou et al. <ref type="bibr" coords="3,512.10,523.13,15.21,8.64" target="#b41">[42]</ref>, we use a learning rate scheduler with the initial learning rate of 0.1 and 10x decay on the 60th, 120th, and 160th epoch.</s><s coords="3,308.39,559.00,220.70,8.64">We report the results averaged over three random seeds.</s><s coords="3,532.19,559.00,12.92,8.64;3,308.86,570.95,224.53,8.64">We provide the description of reported metrics in Appendix.</s></p><p><s coords="3,320.82,583.93,224.29,8.64;3,308.86,595.89,44.29,8.64">We present the results obtained on standard CIL baselines in Table <ref type="table" coords="3,345.53,595.89,3.81,8.64" target="#tab_0">1</ref>.</s><s coords="3,358.39,595.89,186.72,8.64;3,308.86,607.84,127.02,8.64">On CIFAR100, TA consistently improves the accuracy across all the settings.</s><s coords="3,438.97,607.84,106.14,8.64;3,308.86,619.80,236.25,8.64;3,308.86,631.75,187.22,8.64">On ImageNet, our method improves upon the baseline for most settings, or at worst stays within the margin of error of the baseline.</s><s coords="3,499.16,631.75,45.95,8.64;3,308.86,643.71,236.25,8.64;3,308.86,655.49,180.44,8.82">We observe that applying our method generally leads to a more stable network and therefore reduces forgetting, i.e.</s><s coords="3,492.42,655.67,52.87,8.64;3,308.86,667.62,165.48,8.64">TKD+TA for equally split ImageNet (T10S10, T20S5).</s><s coords="3,308.53,680.22,238.23,8.96;3,308.86,692.17,39.61,8.96">Teacher Adaptation under varying degrees of distribution shift.</s><s coords="3,354.62,692.56,190.49,8.64;3,308.50,704.51,236.61,8.64;4,50.11,206.36,495.00,8.64;4,50.11,218.31,58.25,8.64">We also introduce a corrupted CIFAR100 setting where data in every other task contains a noise of varying Figure <ref type="figure" coords="4,78.57,206.36,3.87,8.64">3</ref>: Average incremental accuracy for standard KD and our method of TA under varying strength of data shift on splits of CIFAR100.</s><s coords="4,112.40,218.31,377.88,8.64">We vary the shift strength by adding Gaussian noise of different severity to every other task.</s><s coords="4,494.33,218.31,50.78,8.64;4,50.11,230.27,233.09,8.64">As the noise strengthens, the gap between TA and standard KD widens.</s><s coords="4,286.30,230.27,252.70,8.64">Our method leads to more robust learning in case of data shifts.</s><s coords="4,50.11,263.74,236.42,8.64;4,49.86,275.70,187.29,8.64">severity, which allows us to measure the impact of TA under varying and controllable degrees of data shift.</s><s coords="4,240.98,275.70,45.38,8.64;4,50.11,287.65,236.25,8.64;4,50.11,299.61,236.25,8.64;4,50.11,311.56,78.29,8.64">We corrupt every other task in this setting with Gaussian noise, so that in subsequent tasks the data distribution changes from clean to noisy or vice versa.</s><s coords="4,131.48,311.56,156.53,8.64;4,50.11,323.52,236.25,8.64;4,50.11,335.47,111.58,8.64">We obtain varying strength of distribution shift by using different levels of noise severity, following the methodology from <ref type="bibr" coords="4,142.51,335.47,15.35,8.64" target="#b23">[24]</ref>.</s><s coords="4,164.79,335.47,123.23,8.64;4,50.11,347.43,87.24,8.64">We show the results of this experiment in Figure <ref type="figure" coords="4,129.73,347.43,3.81,8.64">3</ref>.</s><s coords="4,142.02,347.43,144.35,8.64;4,50.11,359.38,236.25,8.64;4,50.11,371.34,236.25,8.64;4,50.11,383.29,186.15,8.64">As the noise severity increases, the gap between standard KD and TA widens, indicating that our method is better suited to more challenging scenarios of learning under extreme data distribution shifts.</s><s coords="4,49.75,395.54,238.26,8.96;4,50.11,407.50,19.48,8.96">Alternative solutions to problems with batch normalization.</s><s coords="4,76.15,407.89,210.21,8.64;4,49.75,419.84,238.35,8.64">To justify the validity of our method, we compare it with other potential solutions for the problem with BN layers.</s><s coords="4,49.64,431.80,236.72,8.64;4,50.11,443.75,237.90,8.64;4,50.11,455.71,236.25,8.64;4,50.11,467.66,236.25,8.64;4,50.11,479.62,237.49,8.64;4,49.76,491.57,237.26,8.64;4,50.11,503.53,177.08,8.64">We use GKD on CIFAR100 split into 10 tasks and compare the following solutions: 1) standard training with BN statistics from the previous task fixed in the teacher model, but updated in the student model, 2) BN layers removed, 3) BN statistics fixed in both models after learning the first task, 4) BN layers replaced with LayerNorm <ref type="bibr" coords="4,215.60,491.57,11.75,8.64" target="#b3">[4]</ref> layers, and 5) finally our solution of Teacher Adaptation.</s><s coords="4,232.78,503.53,53.58,8.64;4,50.11,515.48,236.25,8.64;4,50.11,527.44,236.59,8.64;4,50.11,539.39,236.25,8.64;4,50.11,551.35,236.25,8.64;4,50.11,563.30,35.74,8.64">We show the results of those experiments in Table <ref type="table" coords="4,198.50,515.48,3.73,8.64" target="#tab_1">2</ref>. Fixing or removing BN leads to unstable training, which can be partially fixed by setting a high gradient clipping value or lowering the lambda parameter, but at the cost of the worse performance of the network.</s><s coords="4,89.28,563.30,198.33,8.64;4,50.11,575.26,237.90,8.64;4,50.11,587.21,137.51,8.64">Training the networks with LayerNorm is stable, but ultimately those networks converge to much worse solutions than the variants with BN.</s><s coords="4,190.27,587.21,96.44,8.64;4,50.11,598.85,236.25,8.96;4,50.11,611.12,144.74,8.64">Our solution is the only one that improves over different values of λ and without the need of clipping the gradient values.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5." coords="4,50.11,635.32,73.74,10.75">Conclusions</head><p><s coords="4,62.07,656.69,225.95,8.64;4,50.11,668.65,236.25,8.64;4,50.11,680.60,123.94,8.64">We propose Teacher Adaptation (TA), a simple, yet effective method to improve the performance of KD-based methods in exemplar-free CIL.</s><s coords="4,176.55,680.60,111.06,8.64;4,49.80,692.56,238.21,8.64;4,50.11,704.51,74.61,8.64">During learning a new task, TA updates the teacher network by adjusting its BN statistics with new data.</s><s coords="4,127.82,704.51,158.54,8.64;4,308.86,499.17,236.60,8.64;4,308.86,511.13,237.90,8.64;4,308.86,523.09,173.51,8.64">This mitigates the changes in the model caused by KD loss that arise as the current learner constantly tries to compensate for the diverging normalization statistics between itself and the teacher model.</s><s coords="4,488.41,523.09,56.70,8.64;4,308.55,535.04,236.56,8.64;4,308.86,547.00,236.25,8.64;4,308.86,558.95,29.64,8.64">We show that TA consistently improves the results for different KD-based methods on several CIL benchmarks in an exemplar-free setting.</s><s coords="4,343.26,558.95,202.02,8.64;4,308.86,570.91,236.25,8.64;4,308.86,582.86,108.09,8.64">Moreover, we demonstrate that benefits from our method increase as we increase the degree of shift in data between subsequent tasks.</s><s coords="4,421.88,582.86,123.23,8.64;4,308.86,594.82,237.90,8.64;4,308.86,606.77,236.25,8.64;4,308.86,618.73,155.54,8.64">TA can be easily added to the existing CIL methods and induces only a slight computational overhead, making it a valuable addition to existing exemplar-free KD-based CIL methods.</s></p><p><s coords="7,157.60,130.07,4.24,6.12;7,175.37,121.63,4.24,6.12;7,175.37,129.62,13.79,6.12;7,191.32,124.59,95.05,9.65;7,50.11,136.55,236.25,8.96;7,50.11,148.50,145.44,8.96">k k j=1 a k,j , where a k,j ∈ [0, 1] be the accuracy of the j-th task (j ≤ k) after training the network sequentially for k tasks <ref type="bibr" coords="7,181.35,148.82,10.65,8.64" target="#b2">[3]</ref>.</s><s coords="7,198.63,148.82,89.38,8.64;7,50.11,160.46,237.99,9.65">Overall average incremental accuracy Acc Inc is the mean value from all tasks.</s><s coords="7,49.64,172.55,236.72,8.82;7,50.11,184.37,220.00,9.65">We also report average forgetting as defined in <ref type="bibr" coords="7,246.30,172.73,10.72,8.64" target="#b7">[8]</ref>, while the F org Inc is similarly the mean value from all tasks.</s><s coords="7,273.22,184.69,13.15,8.64;7,50.11,196.64,236.60,8.64;7,50.11,208.28,231.09,9.65">We provide results with additional metrics such as final accuracy Acc F inal and final forgetting F org F inal in the Appendix.</s></p><p><s coords="7,50.11,234.45,190.20,8.96">B. Alternative methods of teacher adaptation.</s><s coords="7,250.27,234.84,36.44,8.64;7,50.11,246.79,236.60,8.64;7,50.11,258.75,236.42,8.64;7,50.11,270.70,27.95,8.64">We study alternative methods of adapting the teacher model and try pertaining (P ) and continuously training (CT ) the teacher model.</s><s coords="7,81.43,270.70,204.93,8.64;7,50.11,282.66,236.25,8.64;7,49.75,294.61,236.61,8.64;7,50.11,306.57,107.27,8.64">For pertaining, we train the teacher on new data in isolation for a few epochs, while during continuous training we update the teacher alongside the main model using the same batches of new data.</s><s coords="7,161.47,306.57,125.06,8.64;7,50.11,318.21,237.99,8.96">We train either the full teacher model (F M ) or only its batch normalization layers (BN ).</s><s coords="7,50.11,330.48,237.91,8.64;7,50.11,342.12,122.11,8.96">Finally, we repeat all the experiments with fixed batch normalization statistics (f ix BN ).</s><s coords="7,175.16,342.43,112.94,8.64;7,49.75,354.39,236.61,8.64;7,50.11,366.35,237.90,8.64;7,50.11,377.98,123.00,8.96;7,173.11,376.41,10.20,6.12;7,183.80,377.98,103.80,8.96;7,50.11,390.26,236.25,8.64;7,50.11,402.21,118.71,8.64">We present results in Table <ref type="table" coords="7,280.78,342.43,3.66,8.64" target="#tab_2">3</ref>. Alternative solutions perform within the standard deviation of TA, but the values of the hyperparmeters for those models are small (learning rate 10 −7 , 5 epochs of pertaining), indicating that the teacher the crucial change in the model is batch normalization statistics.</s><s coords="7,50.11,656.30,237.99,8.96">C. Task-recency bias reduction with Teacher Adaptation.</s></p><p><s coords="7,49.64,668.65,236.89,8.64;7,49.75,680.60,236.61,8.64;7,50.11,692.56,220.55,8.64">We also conduct additional analysis of our method of Teacher Adaptation (TA) to understand the mechanism with which it improves upon the standard knowledge distillation.</s><s coords="7,276.20,692.56,10.16,8.64;7,50.11,704.51,236.25,8.64;7,308.86,450.82,236.25,8.64;7,308.86,462.77,237.99,8.64">At Figure <ref type="figure" coords="7,79.80,704.51,3.81,8.64">4</ref>, we analyze task confusion matrices of standard Figure <ref type="figure" coords="7,336.28,450.82,3.80,8.64">4</ref>: Task confusion matrix after learning all ten tasks on CIFAR100/10 for (upper) base GKD and (lower) GKD+TA.</s></p><p><s coords="7,308.39,474.73,238.38,8.64;7,308.86,486.68,194.13,8.64">We see that TA leads to a model that is better at distinguishing between tasks and shows lower recency bias.</s><s coords="7,308.86,520.16,149.39,8.64">KD (GKD) and its extension with TA.</s><s coords="7,460.76,520.16,84.35,8.64;7,308.55,532.11,236.56,8.64;7,308.86,544.07,206.13,8.64">We find that applying TA results in a model that is better at distinguishing between the tasks, and generally exhibits lower recency bias.</s><s coords="7,518.09,544.07,28.68,8.64;7,308.86,556.02,236.25,8.64;7,308.55,567.98,236.55,8.64;7,308.86,579.93,236.25,8.64;7,308.86,591.89,40.38,8.64">We hypothesize that the lower KD loss that we observe when using TA results in smaller updates to the model, so the difference between the magnitudes of logits learned for different tasks is smaller.</s><s coords="7,352.35,591.89,192.76,8.64;7,308.86,603.84,28.78,8.64">Therefore, TA helps to alleviate the recency bias in CIL.</s></p><p><s coords="7,308.86,629.98,204.10,8.96">D. Additional results for standard benchmarks.</s><s coords="7,522.93,630.37,23.84,8.64;7,308.86,642.32,236.25,8.64;7,308.86,654.28,236.24,8.64;7,308.86,666.24,110.46,8.64">In addition to results in Section 4, we conduct more experiments on CIFAR100 and ImageNet100, adding two settings with a smaller number of tasks.</s><s coords="7,425.18,666.24,119.93,8.64;7,308.86,678.19,237.90,8.64;7,308.86,690.15,17.74,8.64">We report final accuracy and forgetting in addition to incremental accuracy and forgetting.</s><s coords="7,329.73,690.15,215.56,8.64;7,308.86,702.10,98.81,8.64">We report the results for CIFAR100 in Table <ref type="table" coords="7,507.27,690.15,3.68,8.64" target="#tab_3">4</ref>, and for ImageNet100 in Table <ref type="table" coords="7,400.20,702.10,3.74,8.64" target="#tab_4">5</ref>.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head coords="8,174.23,80.33,41.93,8.44">Equal split</head><p><s coords="8,380.54,80.33,97.38,8.44;8,181.81,96.91,26.78,8.44;8,415.84,96.91,26.78,8.44">First task with 50 classes 5 tasks 6 tasks</s></p><formula xml:id="formula_9" coords="8,84.12,113.17,419.44,9.43">Acc F inal ↑ Acc Inc ↑ F org F inal ↓ F org Inc ↓ Acc F inal ↑ Acc Inc ↑ F org F inal ↓ F</formula></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="1,308.86,538.64,237.91,8.64;1,308.86,550.59,236.61,8.64;1,308.86,562.55,237.90,8.64;1,308.86,574.50,236.25,8.64;1,308.86,586.46,231.04,8.64"><head>Figure 1 :</head><label>1</label><figDesc><div><p><s coords="1,308.86,538.64,237.91,8.64;1,308.86,550.59,193.02,8.64">Figure 1: Comparison of vanilla Knowledge Distillation approach and our method of Teacher Adaptation.</s><s coords="1,506.75,550.59,38.72,8.64;1,308.86,562.55,237.90,8.64;1,308.86,574.50,236.25,8.64;1,308.86,586.46,231.04,8.64">We allow the teacher model to continuously update its batch normalization statistics on the new data, which reduces knowledge distillation loss and leads to an overall more stable model.</s></p></div></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="2,50.11,211.92,495.17,8.64;2,50.11,223.88,495.17,8.64;2,50.11,235.83,495.00,8.64;2,50.11,247.79,495.00,8.64;2,50.11,259.74,315.04,8.64"><head>Figure 2 :</head><label>2</label><figDesc><div><p><s coords="2,50.11,211.92,495.17,8.64;2,50.11,223.88,129.63,8.64">Figure2: Applying our teacher adaptation (TA) method reduces knowledge distillation (KD) loss and improves stability over the course of continual learning.</s><s coords="2,182.83,223.88,344.34,8.64">(left) KD loss and cross-entropy (CE) loss of training the model with and without TA.</s><s coords="2,529.66,223.88,15.62,8.64;2,50.11,235.83,495.00,8.64;2,50.11,247.79,330.11,8.64">Our method leads to more consistent representations, as visualized by the CKA<ref type="bibr" coords="2,351.47,235.83,16.61,8.64" target="#b17">[18]</ref> between the representations of the new data obtained in the teacher and student models while learning the second task (middle).</s><s coords="2,383.31,247.79,161.80,8.64;2,50.11,259.74,65.68,8.64">KD with TA leads to better task-agnostic accuracy (right).</s><s coords="2,118.88,259.74,246.27,8.64">We conduct the experiments on CIFAR100 split into 10 tasks.</s></p></div></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="3,49.80,100.30,495.31,111.94"><head>Table 1 :</head><label>1</label><figDesc><div><p><s coords="3,94.53,113.08,31.03,6.15;3,152.18,100.30,355.80,6.15;3,152.18,108.69,12.72,6.12">CIFAR100 GKD 42.52±0.7622.26±0.3131.89±0.4534.68±1.8741.69±1.1818.09±0.8817.64±0.939.67±0.26+TA</s><s coords="3,176.08,108.54,331.91,6.38;3,152.18,120.90,14.59,6.15">44.09±0.9719.41±0.60 35.99±0.7923.32±1.7944.05±1.1212.97±0.4319.37±1.738.31±0.68TKD</s><s coords="3,176.08,120.63,331.91,6.43;3,152.18,129.30,12.72,6.12">43.74±0.8423.65±0.7934.58±0.3421.13±1.1740.44±1.4012.20±0.4614.64±0.336.02±0.54+TA</s><s coords="3,176.08,129.15,179.92,6.38;3,85.01,191.64,460.10,8.64;3,50.11,203.60,174.97,8.64">45.29±1.0219.42±0.8534.62±0.9214.72±1.2841.Comparision of standard Knowledge Distillation (KD) techniques with added Teacher Adaptation (TA) on different splits of a) CIFAR100 and b) ImageNet100.</s><s coords="3,228.17,203.60,291.90,8.64">Adapting the teacher is beneficial to the learning process for all the tasks.</s></p></div></figDesc><table coords="3,355.99,129.15,151.99,6.43"><row><cell>68±1.03</cell><cell>9.29±0.75 16.66±1.66</cell><cell>6.88±0.36</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="4,308.55,258.92,236.56,215.42"><head>Table 2 :</head><label>2</label><figDesc><div><p><s coords="4,346.61,429.83,198.50,8.64;4,308.86,441.79,236.25,8.64;4,308.86,453.75,138.05,8.64">Results for different solutions to the problem of diverging BN layers when using KD in CL. "-" indicates that training crashes due to instability.</s><s coords="4,451.08,453.75,94.04,8.64;4,308.86,465.70,128.93,8.64">TA is the only solution that improves upon the baseline.</s></p></div></figDesc><table coords="4,308.86,258.92,236.25,155.77"><row><cell>clip = 100</cell><cell cols="2">λ = 5</cell><cell cols="2">λ = 10</cell></row><row><cell></cell><cell>Acc F inal</cell><cell>Acc Inc</cell><cell>Acc F inal</cell><cell>Acc Inc</cell></row><row><cell>1) GKD</cell><cell cols="4">25.47±0.57 41.59±0.32 27.96±0.34 42.28±0.67</cell></row><row><cell>2) -BN</cell><cell>0.33±1.15</cell><cell>2.01±2.67</cell><cell>0.33±1.15</cell><cell>2.85±3.81</cell></row><row><cell>3) fix BN</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell cols="2">4) -BN +LN 21.94±0.95</cell><cell cols="3">34.7±0.48 22.76±1.05 34.48±0.15</cell></row><row><cell>5) +TA</cell><cell cols="4">31.39±0.17 44.98±0.38 31.85±0.10 44.06±0.69</cell></row><row><cell>clip = 1</cell><cell cols="2">λ = 5</cell><cell cols="2">λ = 10</cell></row><row><cell></cell><cell>Acc F inal</cell><cell>Acc Inc</cell><cell>Acc F inal</cell><cell>Acc Inc</cell></row><row><cell>1) GKD</cell><cell cols="4">20.80±0.56 34.28±0.24 27.96±0.34 42.28±0.67</cell></row><row><cell>2) -BN</cell><cell cols="2">19.47±0.18 29.83±0.53</cell><cell>0.33±1.15</cell><cell>2.85±3.81</cell></row><row><cell>3) fix BN</cell><cell cols="2">20.21±0.31 32.07±0.20</cell><cell>-</cell><cell>-</cell></row><row><cell cols="5">4) -BN +LN 18.49±1.41 30.39±0.72 22.76±1.05 34.48±0.15</cell></row><row><cell>5) +TA</cell><cell cols="4">24.19±0.90 36.13±0.24 31.85±0.10 44.06±0.69</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="7,49.80,424.73,238.21,199.56"><head>Table 3 :</head><label>3</label><figDesc><div><p><s coords="7,100.99,455.02,81.22,7.55">54±0.67 43.46±0.72</s><s coords="7,192.66,455.02,41.63,7.55">24.18±1.17</s><s coords="7,244.74,455.02,41.62,7.55">20.80±1.51</s><s coords="7,50.11,465.32,80.02,7.71">+fix BN 28.02±0.60</s><s coords="7,140.59,465.47,41.62,7.55">42.33±0.53</s><s coords="7,192.66,465.47,41.63,7.55">29.91±1.27</s><s coords="7,244.74,465.47,41.62,7.55">22.66±0.95</s><s coords="7,50.11,480.48,19.85,7.55;7,88.51,480.48,41.62,7.55">P-BN 31.16±0.54</s><s coords="7,140.59,480.48,41.62,7.55">43.64±0.77</s><s coords="7,192.66,480.48,41.63,7.55">24.44±0.96</s><s coords="7,244.74,480.48,41.62,7.55;7,50.11,490.78,80.02,7.71">20.13±0.75 +fix BN 27.62±0.48</s><s coords="7,140.59,490.94,41.62,7.55">42.12±0.38</s><s coords="7,192.66,490.94,41.63,7.55">29.95±1.64</s><s coords="7,244.74,490.94,41.62,7.55">22.50±0.95</s><s coords="7,50.11,505.94,20.01,7.55;7,88.51,505.94,41.62,7.55">T-FM 31.37±0.94</s><s coords="7,140.59,505.94,41.62,7.55">43.38±0.77</s><s coords="7,192.66,505.94,41.63,7.55">24.34±1.37</s><s coords="7,244.74,505.94,41.62,7.55">20.93±1.58</s><s coords="7,50.11,516.24,80.02,7.71">+fix BN 28.17±0.49</s><s coords="7,140.59,516.40,41.62,7.55">42.29±0.42</s><s coords="7,192.66,516.40,41.63,7.55">29.79±1.02</s><s coords="7,244.74,516.40,41.62,7.55">22.55±0.67</s><s coords="7,50.11,531.40,19.52,7.55;7,88.51,531.40,41.62,7.55">T-BN 31.35±0.63</s><s coords="7,140.59,531.40,41.62,7.55">43.69±0.76</s><s coords="7,192.66,531.40,41.63,7.55">24.29±0.61</s><s coords="7,244.74,531.40,41.62,7.55">20.23±0.59</s><s coords="7,50.11,541.70,80.02,7.71">+fix BN 27.33±0.50</s><s coords="7,140.59,541.86,41.62,7.55">42.09±0.45</s><s coords="7,192.66,541.86,93.70,7.55">30.20±1.73 22.50±0.85</s><s coords="7,83.87,579.78,202.67,8.64;7,50.11,591.74,26.90,8.64">Ablation study of different ways to adapt the teacher model.</s><s coords="7,80.14,591.74,206.22,8.64;7,50.11,603.69,126.26,8.64">Our method achieves the best results while requiring no additional hyperparameters.</s><s coords="7,179.85,603.69,108.16,8.64;7,50.11,615.65,160.06,8.64">All experiments were conducted on CIFAR100 split into 10 tasks.</s></p></div></figDesc><table coords="7,50.11,424.73,236.26,139.69"><row><cell>Method</cell><cell>Acc F inal</cell><cell>Acc Inc</cell><cell>F org F inal</cell><cell>F org Inc</cell></row><row><cell>Base</cell><cell cols="4">27.53±0.15 42.22±0.38 31.28±1.64 23.11±1.58</cell></row><row><cell cols="5">P-FM 31.TA 32.15±0.12 44.31±0.26 23.55±0.51 19.85±0.93</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" coords="8,50.11,113.17,495.00,569.11"><head>Table 4 :</head><label>4</label><figDesc><div><p><s coords="8,504.91,113.17,34.36,9.43;8,248.19,374.58,133.76,8.64;8,146.33,436.66,36.07,9.43;8,199.31,436.66,105.92,9.43;8,320.42,436.66,44.07,9.43;8,382.64,436.66,36.07,9.43;8,435.62,436.66,103.65,9.43;8,146.33,526.73,36.07,9.43;8,199.31,526.73,105.92,9.43;8,320.42,526.73,44.07,9.43;8,382.64,526.73,36.07,9.43;8,435.62,526.73,103.65,9.43;8,50.11,543.62,79.30,8.44">org Inc ↓ Additional results for CIFAR100.Acc Inc ↑ F org F inal ↓ F org Inc ↓ Acc F inal ↑ Acc Inc ↑ F org F inal ↓ F org Inc ↓ Acc Inc ↑ F org F inal ↓ F org Inc ↓ Acc F inal ↑ Acc Inc ↑ F org F inal ↓ F org Inc ↓ GKD 40.33±0.37</s><s coords="8,141.10,543.62,46.53,8.44;8,202.77,543.62,46.53,8.44">54.62±0.52 32.72±0.09</s><s coords="8,260.98,543.62,46.53,8.44">25.95±0.11</s><s coords="8,319.20,543.24,104.74,8.82">41.56±1.44 52.67±0.93</s><s coords="8,439.08,543.62,46.53,8.44">18.94±1.21</s><s coords="8,503.46,543.62,41.66,8.44">9.92±0.83</s><s coords="8,50.11,555.13,17.45,8.39;8,82.89,554.93,46.53,8.75">+TA 43.17±1.06</s><s coords="8,141.10,554.93,46.53,8.75">55.82±0.61</s><s coords="8,202.77,554.93,46.53,8.75">25.10±1.03</s><s coords="8,260.98,554.93,46.53,8.75">20.52±0.24</s><s coords="8,319.20,554.93,46.53,8.75">44.60±0.24</s><s coords="8,377.41,555.31,46.53,8.44">51.44±0.51</s><s coords="8,439.08,554.93,46.53,8.75">13.80±1.09</s><s coords="8,498.59,554.93,46.53,8.75">14.55±0.76</s><s coords="8,50.11,572.08,79.30,8.44">TKD 43.19±0.16</s><s coords="8,141.10,572.08,46.53,8.44">55.70±0.49</s><s coords="8,202.77,572.08,46.53,8.44">24.84±0.35</s><s coords="8,260.98,572.08,46.53,8.44">23.55±0.35</s><s coords="8,319.20,572.08,46.53,8.44">40.56±1.30</s><s coords="8,377.41,571.70,46.53,8.75">54.72±0.86</s><s coords="8,439.08,572.08,46.53,8.44">15.39±1.01</s><s coords="8,498.59,571.70,46.53,8.75">10.16±0.34</s><s coords="8,50.11,583.59,17.45,8.39;8,82.89,583.39,46.53,8.75">+TA 43.93±0.72</s><s coords="8,141.10,583.39,46.53,8.75;8,202.77,583.39,46.53,8.75">56.23±0.70 17.89±0.14</s><s coords="8,260.98,583.39,46.53,8.75">18.09±0.26</s><s coords="8,319.20,583.39,46.53,8.75">42.83±0.61</s><s coords="8,377.41,583.77,46.53,8.44">53.85±0.39</s><s coords="8,439.08,583.39,106.03,8.82">10.13±0.20 13.15±0.16</s><s coords="8,146.33,616.81,36.07,9.44;8,199.31,616.81,105.92,9.44;8,320.42,616.81,44.07,9.44;8,382.64,616.81,36.07,9.44;8,435.62,616.81,103.65,9.44">Acc Inc ↑ F org F inal ↓ F org Inc ↓ Acc F inal ↑ Acc Inc ↑ F org F inal ↓ F org Inc ↓</s></p></div></figDesc><table coords="8,50.11,130.06,495.00,552.22"><row><cell cols="3">GKD 37.63±0.52 48.80±0.36</cell><cell cols="3">23.13±2.28 19.30±2.37 40.74±0.72 51.93±1.24</cell><cell>25.87±0.75 18.90±0.29</cell></row><row><cell>+TA</cell><cell cols="2">40.84±0.23 50.08±0.26</cell><cell cols="3">16.76±1.68 16.66±1.25 43.18±1.66 52.22±1.28</cell><cell>18.73±1.61 14.09±0.66</cell></row><row><cell cols="3">TKD 38.33±0.70 49.56±0.48</cell><cell cols="3">24.78±2.58 25.04±2.55 41.19±0.42 52.07±1.35</cell><cell>17.31±1.20 15.02±0.39</cell></row><row><cell>+TA</cell><cell cols="2">41.12±0.35 50.87±0.15</cell><cell cols="3">18.12±1.55 21.09±1.39 41.36±0.89 51.88±0.80</cell><cell>12.84±1.25 11.42±0.64</cell></row><row><cell></cell><cell></cell><cell cols="2">10 tasks</cell><cell></cell><cell cols="2">11 tasks</cell></row><row><cell></cell><cell>Acc F inal ↑</cell><cell>Acc Inc ↑</cell><cell>F org F inal ↓ F org Inc ↓</cell><cell>Acc F inal ↑</cell><cell>Acc Inc ↑</cell><cell>F org F inal ↓ F org Inc ↓</cell></row><row><cell cols="3">GKD 28.27±0.44 42.52±0.76</cell><cell cols="3">29.59±0.92 22.26±0.31 30.79±1.62 41.69±1.18</cell><cell>26.84±2.12 18.09±0.88</cell></row><row><cell>+TA</cell><cell cols="2">31.92±0.86 44.09±0.97</cell><cell cols="3">22.65±1.32 19.41±0.60 33.20±0.76 44.05±1.12</cell><cell>18.90±0.19 12.97±0.43</cell></row><row><cell cols="3">TKD 30.05±0.81 43.74±0.84</cell><cell cols="3">24.53±0.23 23.65±0.79 28.38±1.46 40.44±1.40</cell><cell>15.68±0.84 12.20±0.46</cell></row><row><cell>+TA</cell><cell cols="2">31.80±0.67 45.29±1.02</cell><cell cols="3">18.59±0.90 19.42±0.85 28.50±0.39 41.68±1.03</cell><cell>11.58±0.38</cell><cell>9.29±0.75</cell></row><row><cell></cell><cell></cell><cell cols="2">20 tasks</cell><cell></cell><cell cols="2">26 tasks</cell></row><row><cell></cell><cell>Acc F inal ↑</cell><cell>Acc Inc ↑</cell><cell>F org F inal ↓ F org Inc ↓</cell><cell>Acc F inal ↑</cell><cell>Acc Inc ↑</cell><cell>F org F inal ↓ F org Inc ↓</cell></row><row><cell cols="3">GKD 15.59±0.32 31.89±0.45</cell><cell cols="3">43.28±0.56 34.68±1.87 10.10±0.71 17.64±0.93</cell><cell>15.29±0.27</cell><cell>9.67±0.26</cell></row><row><cell>+TA</cell><cell cols="2">19.55±0.24 35.99±0.79</cell><cell cols="3">30.38±2.08 23.32±1.79 11.99±0.66 19.37±1.73</cell><cell>9.05±0.63</cell><cell>8.31±0.68</cell></row><row><cell cols="3">TKD 19.39±0.41 34.58±0.34</cell><cell>22.06±0.46 21.13±1.17</cell><cell cols="2">7.88±0.08 14.64±0.33</cell><cell>7.96±0.47</cell><cell>6.02±0.54</cell></row><row><cell>+TA</cell><cell cols="2">18.30±0.50 34.62±0.92</cell><cell>15.22±1.25 14.72±1.28</cell><cell cols="2">9.05±0.64 16.66±1.66</cell><cell>7.17±0.53</cell><cell>6.88±0.36</cell></row><row><cell></cell><cell></cell><cell cols="2">Equal split</cell><cell></cell><cell cols="2">First task with 50 classes</cell></row><row><cell></cell><cell></cell><cell cols="2">5 tasks</cell><cell></cell><cell cols="2">6 tasks</cell></row><row><cell></cell><cell>Acc F inal ↑</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="3">GKD 51.06±0.59 61.95±0.50</cell><cell cols="3">27.57±0.79 23.16±0.88 55.71±0.82 63.40±0.41</cell><cell>13.26±0.87</cell><cell>7.83±0.40</cell></row><row><cell>+TA</cell><cell cols="2">52.29±0.28 62.89±0.32</cell><cell cols="3">23.40±0.31 18.94±0.43 55.18±0.84 62.94±0.17</cell><cell>13.67±0.81 10.25±0.38</cell></row><row><cell cols="3">TKD 53.73±0.25 62.91±0.44</cell><cell cols="3">20.77±0.58 20.30±0.80 57.17±0.45 66.17±0.24</cell><cell>11.38±0.39</cell><cell>8.92±0.20</cell></row><row><cell>+TA</cell><cell cols="2">53.29±0.04 63.04±0.30</cell><cell cols="3">18.28±0.72 17.26±0.89 56.58±0.88 65.53±0.23</cell><cell>11.66±0.87 10.75±0.42</cell></row><row><cell></cell><cell></cell><cell cols="2">10 tasks</cell><cell></cell><cell cols="2">11 tasks</cell></row><row><cell></cell><cell>Acc F inal ↑</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell cols="2">20 tasks</cell><cell></cell><cell cols="2">26 tasks</cell></row><row><cell></cell><cell>Acc F inal ↑</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="3">GKD 24.14±0.91 42.82±0.58</cell><cell cols="3">45.53±0.88 35.39±0.88 14.82±0.85 21.91±0.06</cell><cell>17.99±0.53</cell><cell>9.29±0.69</cell></row><row><cell>+TA</cell><cell cols="2">31.89±1.63 45.88±0.79</cell><cell cols="3">27.74±1.03 23.25±0.62 17.16±0.84 22.31±0.64</cell><cell>12.71±1.11 11.28±0.98</cell></row><row><cell cols="3">TKD 28.90±0.42 45.71±0.37</cell><cell cols="3">29.87±0.97 25.85±0.26 10.99±0.22 19.32±0.23</cell><cell>13.55±0.88</cell><cell>9.67±0.61</cell></row><row><cell>+TA</cell><cell cols="2">30.13±0.34 45.14±0.78</cell><cell cols="3">15.42±0.89 15.62±0.51 13.90±0.52 22.55±0.83</cell><cell>7.24±0.71</cell><cell>9.96±0.28</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" coords="8,207.63,698.07,179.66,8.64"><head>Table 5 :</head><label>5</label><figDesc><div><p><s coords="8,242.85,698.07,144.44,8.64">Additional results for ImageNet100.</s></p></div></figDesc><table /></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head coords="4,308.86,640.87,98.83,10.75">Acknowledgements</head><p><s coords="4,320.82,661.57,224.29,8.64;4,308.86,673.52,237.99,8.64;4,308.86,685.48,92.26,8.64">Filip Szatkowski and Tomasz Trzcinski are supported by National Centre of Science (NCP, Poland) Grant No. 2022/45/B/ST6/02817.</s><s coords="4,404.21,685.48,140.90,8.64;4,308.86,697.43,168.87,8.64">Tomasz Trzciński is also supported by NCP Grant No. 2020/39/B/ST6/01511.</s></p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Ss-il: Separated softmax for incremental learning</title>
		<author>
			<persName><forename type="first">Jihwan</forename><surname>Hongjoon Ahn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Subin</forename><surname>Kwak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hyeonsu</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hyojun</forename><surname>Bang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Taesup</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><surname>Moon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International conference on computer vision</title>
				<meeting>the IEEE/CVF International conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="844" to="853" />
		</imprint>
	</monogr>
	<note type="raw_reference">Hongjoon Ahn, Jihwan Kwak, Subin Lim, Hyeonsu Bang, Hyojun Kim, and Taesup Moon. Ss-il: Separated softmax for incremental learning. In Proceedings of the IEEE/CVF International conference on computer vision, pages 844-853, 2021.</note>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Memory aware synapses: Learning what (not) to forget</title>
		<author>
			<persName><forename type="first">Rahaf</forename><surname>Aljundi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Francesca</forename><surname>Babiloni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohamed</forename><surname>Elhoseiny</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marcus</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tinne</forename><surname>Tuytelaars</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Rahaf Aljundi, Francesca Babiloni, Mohamed Elhoseiny, Marcus Rohrbach, and Tinne Tuytelaars. Memory aware synapses: Learning what (not) to forget, 2018.</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Expert gate: Lifelong learning with a network of experts</title>
		<author>
			<persName><forename type="first">Rahaf</forename><surname>Aljundi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Punarjay</forename><surname>Chakravarty</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tinne</forename><surname>Tuytelaars</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
				<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="3366" to="3375" />
		</imprint>
	</monogr>
	<note type="raw_reference">Rahaf Aljundi, Punarjay Chakravarty, and Tinne Tuytelaars. Expert gate: Lifelong learning with a network of experts. In CVPR, pages 3366-3375, 2017.</note>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Jimmy</forename><surname>Lei Ba</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jamie</forename><surname>Ryan Kiros</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1607.06450</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">Layer normalization. arXiv preprint</note>
	<note type="raw_reference">Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton. Layer normalization. arXiv preprint arXiv:1607.06450, 2016.</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Rainbow memory: Continual learning with a memory of diverse samples</title>
		<author>
			<persName><forename type="first">Jihwan</forename><surname>Bang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Heesu</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Youngjoon</forename><surname>Yoo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jung-Woo</forename><surname>Ha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonghyun</forename><surname>Choi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
				<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="8218" to="8227" />
		</imprint>
	</monogr>
	<note type="raw_reference">Jihwan Bang, Heesu Kim, YoungJoon Yoo, Jung-Woo Ha, and Jonghyun Choi. Rainbow memory: Continual learn- ing with a memory of diverse samples. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 8218-8227, 2021.</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">End-to-end incremental learning</title>
		<author>
			<persName><forename type="first">Manuel</forename><forename type="middle">J</forename><surname>Francisco M Castro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nicolás</forename><surname>Marín-Jiménez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cordelia</forename><surname>Guil</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Karteek</forename><surname>Schmid</surname></persName>
		</author>
		<author>
			<persName><surname>Alahari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European conference on computer vision (ECCV)</title>
				<meeting>the European conference on computer vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="233" to="248" />
		</imprint>
	</monogr>
	<note type="raw_reference">Francisco M Castro, Manuel J Marín-Jiménez, Nicolás Guil, Cordelia Schmid, and Karteek Alahari. End-to-end incremen- tal learning. In Proceedings of the European conference on computer vision (ECCV), pages 233-248, 2018.</note>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Rebalancing batch normalization for exemplar-based class-incremental learning</title>
		<author>
			<persName><forename type="first">Sungmin</forename><surname>Cha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sungjun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dasol</forename><surname>Hwang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sunwon</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Moontae</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Taesup</forename><surname>Moon</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Sungmin Cha, Sungjun Cho, Dasol Hwang, Sunwon Hong, Moontae Lee, and Taesup Moon. Rebalancing batch normal- ization for exemplar-based class-incremental learning, 2023.</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Riemannian walk for incremental learning: Understanding forgetting and intransigence</title>
		<author>
			<persName><forename type="first">Arslan</forename><surname>Chaudhry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thalaiyasingam</forename><surname>Puneet K Dokania</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Philip</forename><surname>Ajanthan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Torr</forename><surname>Hs</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
				<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="532" to="547" />
		</imprint>
	</monogr>
	<note type="raw_reference">Arslan Chaudhry, Puneet K Dokania, Thalaiyasingam Ajan- than, and Philip HS Torr. Riemannian walk for incremental learning: Understanding forgetting and intransigence. In ECCV, pages 532-547, 2018.</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li-Jia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kai</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fei-Fei</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2009 IEEE Computer Society Conference on Computer Vision and Pattern Recognition</title>
				<meeting><address><addrLine>Miami, Florida, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2009-06">2009. June 2009. 2009</date>
			<biblScope unit="page" from="248" to="255" />
		</imprint>
	</monogr>
	<note type="raw_reference">Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Fei-Fei Li. Imagenet: A large-scale hierarchical im- age database. In 2009 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR 2009), 20-25 June 2009, Miami, Florida, USA, pages 248-255, 2009.</note>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Multi-level knowledge distillation via knowledge alignment and correlation</title>
		<author>
			<persName><forename type="first">Fei</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongxin</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Venkat</forename><surname>Krovi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Feng</forename><surname>Luo</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Fei Ding, Yin Yang, Hongxin Hu, Venkat Krovi, and Feng Luo. Multi-level knowledge distillation via knowledge align- ment and correlation, 2021.</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Podnet: Pooled outputs distillation for small-tasks incremental learning</title>
		<author>
			<persName><forename type="first">Arthur</forename><surname>Douillard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthieu</forename><surname>Cord</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Charles</forename><surname>Ollion</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Robert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eduardo</forename><surname>Valle</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision-ECCV 2020: 16th European Conference</title>
				<meeting><address><addrLine>Glasgow, UK</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020">August 23-28, 2020. 2020</date>
			<biblScope unit="page" from="86" to="102" />
		</imprint>
	</monogr>
	<note>Proceedings, Part XX 16</note>
	<note type="raw_reference">Arthur Douillard, Matthieu Cord, Charles Ollion, Thomas Robert, and Eduardo Valle. Podnet: Pooled outputs distil- lation for small-tasks incremental learning. In Computer Vision-ECCV 2020: 16th European Conference, Glasgow, UK, August 23-28, 2020, Proceedings, Part XX 16, pages 86-102. Springer, 2020.</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Look-ahead meta learning for continual learning</title>
		<author>
			<persName><forename type="first">Gunshi</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Karmesh</forename><surname>Yadav</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liam</forename><surname>Paull</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="11588" to="11598" />
		</imprint>
	</monogr>
	<note type="raw_reference">Gunshi Gupta, Karmesh Yadav, and Liam Paull. Look-ahead meta learning for continual learning. Advances in Neural Information Processing Systems, 33:11588-11598, 2020.</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
				<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
	<note type="raw_reference">Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In Proceed- ings of the IEEE conference on computer vision and pattern recognition, pages 770-778, 2016.</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Learning a unified classifier incrementally via rebalancing</title>
		<author>
			<persName><forename type="first">Saihui</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xinyu</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chen</forename><forename type="middle">Change</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zilei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dahua</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF conference on Computer Vision and Pattern Recognition</title>
				<meeting>the IEEE/CVF conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="831" to="839" />
		</imprint>
	</monogr>
	<note type="raw_reference">Saihui Hou, Xinyu Pan, Chen Change Loy, Zilei Wang, and Dahua Lin. Learning a unified classifier incrementally via rebalancing. In Proceedings of the IEEE/CVF conference on Computer Vision and Pattern Recognition, pages 831-839, 2019.</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Batch normalization: Accelerating deep network training by reducing internal covariate shift</title>
		<author>
			<persName><forename type="first">Sergey</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning</title>
				<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="448" to="456" />
		</imprint>
	</monogr>
	<note type="raw_reference">Sergey Ioffe and Christian Szegedy. Batch normalization: Accelerating deep network training by reducing internal co- variate shift. In International conference on machine learning, pages 448-456. pmlr, 2015.</note>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">A memory transformer network for incremental learning</title>
		<author>
			<persName><forename type="first">Ahmet</forename><surname>Iscen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Bird</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mathilde</forename><surname>Caron</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alireza</forename><surname>Fathi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cordelia</forename><surname>Schmid</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Ahmet Iscen, Thomas Bird, Mathilde Caron, Alireza Fathi, and Cordelia Schmid. A memory transformer network for incremental learning, 2022.</note>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Overcoming catastrophic forgetting in neural networks</title>
		<author>
			<persName><forename type="first">James</forename><surname>Kirkpatrick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Razvan</forename><surname>Pascanu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Neil</forename><surname>Rabinowitz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joel</forename><surname>Veness</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guillaume</forename><surname>Desjardins</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrei</forename><forename type="middle">A</forename><surname>Rusu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kieran</forename><surname>Milan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><surname>Quan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tiago</forename><surname>Ramalho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Agnieszka</forename><surname>Grabska-Barwinska</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017">2017</date>
			<publisher>Proceedings of the national academy of sciences</publisher>
			<biblScope unit="volume">114</biblScope>
			<biblScope unit="page" from="3521" to="3526" />
		</imprint>
	</monogr>
	<note type="raw_reference">James Kirkpatrick, Razvan Pascanu, Neil Rabinowitz, Joel Veness, Guillaume Desjardins, Andrei A Rusu, Kieran Milan, John Quan, Tiago Ramalho, Agnieszka Grabska-Barwinska, et al. Overcoming catastrophic forgetting in neural net- works. Proceedings of the national academy of sciences, 114(13):3521-3526, 2017.</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Similarity of neural network representations revisited</title>
		<author>
			<persName><forename type="first">Simon</forename><surname>Kornblith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohammad</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Honglak</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
				<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="3519" to="3529" />
		</imprint>
	</monogr>
	<note type="raw_reference">Simon Kornblith, Mohammad Norouzi, Honglak Lee, and Geoffrey Hinton. Similarity of neural network representations revisited. In International Conference on Machine Learning, pages 3519-3529. PMLR, 2019.</note>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Learning multiple layers of features from tiny images</title>
		<author>
			<persName><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Alex Krizhevsky. Learning multiple layers of features from tiny images. 2009.</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Overcoming catastrophic forgetting with unlabeled data in the wild</title>
		<author>
			<persName><forename type="first">Kibok</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kimin</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jinwoo</forename><surname>Shin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Honglak</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
				<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="312" to="321" />
		</imprint>
	</monogr>
	<note type="raw_reference">Kibok Lee, Kimin Lee, Jinwoo Shin, and Honglak Lee. Over- coming catastrophic forgetting with unlabeled data in the wild. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 312-321, 2019.</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Learning without forgetting</title>
		<author>
			<persName><forename type="first">Zhizhong</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Derek</forename><surname>Hoiem</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
				<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="page" from="2935" to="2947" />
		</imprint>
	</monogr>
	<note type="raw_reference">Zhizhong Li and Derek Hoiem. Learning without forget- ting. IEEE transactions on pattern analysis and machine intelligence, 40(12):2935-2947, 2017.</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Knowledge distillation with reptile meta-learning for pretrained language model compression</title>
		<author>
			<persName><forename type="first">Xinge</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liang-Chih</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xuejie</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 29th International Conference on Computational Linguistics</title>
				<meeting>the 29th International Conference on Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="4907" to="4917" />
		</imprint>
	</monogr>
	<note type="raw_reference">Xinge Ma, Jin Wang, Liang-Chih Yu, and Xuejie Zhang. Knowledge distillation with reptile meta-learning for pre- trained language model compression. In Proceedings of the 29th International Conference on Computational Linguistics, pages 4907-4917, 2022.</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Classincremental learning: survey and performance evaluation on image classification</title>
		<author>
			<persName><forename type="first">Marc</forename><surname>Masana</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xialei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bartłomiej</forename><surname>Twardowski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mikel</forename><surname>Menta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><forename type="middle">D</forename><surname>Bagdanov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joost</forename><surname>Van De Weijer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Marc Masana, Xialei Liu, Bartłomiej Twardowski, Mikel Menta, Andrew D Bagdanov, and Joost van de Weijer. Class- incremental learning: survey and performance evaluation on image classification. IEEE Transactions on Pattern Analysis and Machine Intelligence, 2022.</note>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Benchmarking robustness in object detection: Autonomous driving when winter is coming</title>
		<author>
			<persName><forename type="first">Claudio</forename><surname>Michaelis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Benjamin</forename><surname>Mitzkus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Robert</forename><surname>Geirhos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Evgenia</forename><surname>Rusak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oliver</forename><surname>Bringmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><forename type="middle">S</forename><surname>Ecker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthias</forename><surname>Bethge</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wieland</forename><surname>Brendel</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1907.07484</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note type="raw_reference">Claudio Michaelis, Benjamin Mitzkus, Robert Geirhos, Evge- nia Rusak, Oliver Bringmann, Alexander S. Ecker, Matthias Bethge, and Wieland Brendel. Benchmarking robustness in object detection: Autonomous driving when winter is coming. arXiv preprint arXiv:1907.07484, 2019.</note>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Continual normalization: Rethinking batch normalization for online continual learning</title>
		<author>
			<persName><forename type="first">Quang</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chenghao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Steven</forename><surname>Hoi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2203.16102</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note type="raw_reference">Quang Pham, Chenghao Liu, and Steven Hoi. Continual normalization: Rethinking batch normalization for online continual learning. arXiv preprint arXiv:2203.16102, 2022.</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Gdumb: A simple approach that questions our progress in continual learning</title>
		<author>
			<persName><forename type="first">Ameya</forename><surname>Prabhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Puneet</forename><forename type="middle">K</forename><surname>Philip Hs Torr</surname></persName>
		</author>
		<author>
			<persName><surname>Dokania</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision-ECCV 2020: 16th European Conference</title>
				<meeting><address><addrLine>Glasgow, UK</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020">August 23-28, 2020. 2020</date>
			<biblScope unit="page" from="524" to="540" />
		</imprint>
	</monogr>
	<note>Proceedings, Part II 16</note>
	<note type="raw_reference">Ameya Prabhu, Philip HS Torr, and Puneet K Dokania. Gdumb: A simple approach that questions our progress in continual learning. In Computer Vision-ECCV 2020: 16th European Conference, Glasgow, UK, August 23-28, 2020, Proceedings, Part II 16, pages 524-540. Springer, 2020.</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Efficient parametrization of multi-domain deep neural networks</title>
		<author>
			<persName><forename type="first">Hakan</forename><surname>Sylvestre-Alvise Rebuffi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrea</forename><surname>Bilen</surname></persName>
		</author>
		<author>
			<persName><surname>Vedaldi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
				<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="8119" to="8127" />
		</imprint>
	</monogr>
	<note type="raw_reference">Sylvestre-Alvise Rebuffi, Hakan Bilen, and Andrea Vedaldi. Efficient parametrization of multi-domain deep neural net- works. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 8119-8127, 2018.</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">icarl: Incremental classifier and representation learning</title>
		<author>
			<persName><surname>Sylvestre-Alvise</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Rebuffi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Georg</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christoph</forename><forename type="middle">H</forename><surname>Sperl</surname></persName>
		</author>
		<author>
			<persName><surname>Lampert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on Computer Vision and Pattern Recognition</title>
				<meeting>the IEEE conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2001">2001-2010, 2017</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Sylvestre-Alvise Rebuffi, Alexander Kolesnikov, Georg Sperl, and Christoph H Lampert. icarl: Incremental classifier and representation learning. In Proceedings of the IEEE con- ference on Computer Vision and Pattern Recognition, pages 2001-2010, 2017.</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Experience replay for continual learning</title>
		<author>
			<persName><forename type="first">David</forename><surname>Rolnick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arun</forename><surname>Ahuja</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Schwarz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Timothy</forename><surname>Lillicrap</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gregory</forename><surname>Wayne</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="raw_reference">David Rolnick, Arun Ahuja, Jonathan Schwarz, Timothy Lill- icrap, and Gregory Wayne. Experience replay for continual learning. Advances in Neural Information Processing Systems, 32, 2019.</note>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">How does batch normalization help optimization?</title>
		<author>
			<persName><forename type="first">Shibani</forename><surname>Santurkar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dimitris</forename><surname>Tsipras</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Ilyas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aleksander</forename><surname>Madry</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Shibani Santurkar, Dimitris Tsipras, Andrew Ilyas, and Alek- sander Madry. How does batch normalization help optimiza- tion?, 2019.</note>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Improving robustness against common corruptions by covariate shift adaptation</title>
		<author>
			<persName><forename type="first">Steffen</forename><surname>Schneider</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Evgenia</forename><surname>Rusak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luisa</forename><surname>Eck</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oliver</forename><surname>Bringmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wieland</forename><surname>Brendel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthias</forename><surname>Bethge</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<editor>
			<persName><forename type="first">H</forename><surname>Larochelle</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Ranzato</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">R</forename><surname>Hadsell</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><forename type="middle">F</forename><surname>Balcan</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">H</forename><surname>Lin</surname></persName>
		</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="11539" to="11551" />
		</imprint>
	</monogr>
	<note type="raw_reference">Steffen Schneider, Evgenia Rusak, Luisa Eck, Oliver Bring- mann, Wieland Brendel, and Matthias Bethge. Improving robustness against common corruptions by covariate shift adaptation. In H. Larochelle, M. Ranzato, R. Hadsell, M.F. Balcan, and H. Lin, editors, Advances in Neural Information Processing Systems, volume 33, pages 11539-11551. Curran Associates, Inc., 2020.</note>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">A closer look at rehearsal-free continual learning</title>
		<author>
			<persName><forename type="first">James</forename><surname>Seale</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Smith</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Junjiao</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shaunak</forename><surname>Halbe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yen-Chang</forename><surname>Hsu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zsolt</forename><surname>Kira</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
				<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2023">2023</date>
			<biblScope unit="page" from="2409" to="2419" />
		</imprint>
	</monogr>
	<note type="raw_reference">James Seale Smith, Junjiao Tian, Shaunak Halbe, Yen-Chang Hsu, and Zsolt Kira. A closer look at rehearsal-free continual learning. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2409-2419, 2023.</note>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Three scenarios for continual learning</title>
		<author>
			<persName><forename type="first">M</forename><surname>Gido</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andreas</forename><forename type="middle">S</forename><surname>Van De Ven</surname></persName>
		</author>
		<author>
			<persName><surname>Tolias</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.07734</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note type="raw_reference">Gido M Van de Ven and Andreas S Tolias. Three scenarios for continual learning. arXiv preprint arXiv:1904.07734, 2019.</note>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Tent: Fully test-time adaptation by entropy minimization</title>
		<author>
			<persName><forename type="first">Dequan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Evan</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shaoteng</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bruno</forename><forename type="middle">A</forename><surname>Olshausen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">9th International Conference on Learning Representations, ICLR 2021, Virtual Event</title>
				<meeting><address><addrLine>Austria</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2021">May 3-7, 2021. OpenReview.net, 2021</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Dequan Wang, Evan Shelhamer, Shaoteng Liu, Bruno A. Ol- shausen, and Trevor Darrell. Tent: Fully test-time adaptation by entropy minimization. In 9th International Conference on Learning Representations, ICLR 2021, Virtual Event, Austria, May 3-7, 2021. OpenReview.net, 2021.</note>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Dualprompt: Complementary prompting for rehearsal-free continual learning</title>
		<author>
			<persName><forename type="first">Zifeng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zizhao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sayna</forename><surname>Ebrahimi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruoxi</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Han</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chen-Yu</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaoqi</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guolong</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vincent</forename><surname>Perot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jennifer</forename><surname>Dy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision-ECCV 2022: 17th European Conference</title>
				<meeting><address><addrLine>Tel Aviv</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2022">October 23-27, 2022. 2022</date>
			<biblScope unit="page" from="631" to="648" />
		</imprint>
	</monogr>
	<note>Proceedings, Part XXVI</note>
	<note type="raw_reference">Zifeng Wang, Zizhao Zhang, Sayna Ebrahimi, Ruoxi Sun, Han Zhang, Chen-Yu Lee, Xiaoqi Ren, Guolong Su, Vin- cent Perot, Jennifer Dy, et al. Dualprompt: Complementary prompting for rehearsal-free continual learning. In Computer Vision-ECCV 2022: 17th European Conference, Tel Aviv, Is- rael, October 23-27, 2022, Proceedings, Part XXVI, pages 631-648. Springer, 2022.</note>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Learning to prompt for continual learning</title>
		<author>
			<persName><forename type="first">Zifeng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zizhao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chen-Yu</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Han</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruoxi</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaoqi</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guolong</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vincent</forename><surname>Perot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jennifer</forename><surname>Dy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tomas</forename><surname>Pfister</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
				<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="139" to="149" />
		</imprint>
	</monogr>
	<note type="raw_reference">Zifeng Wang, Zizhao Zhang, Chen-Yu Lee, Han Zhang, Ruoxi Sun, Xiaoqi Ren, Guolong Su, Vincent Perot, Jen- nifer Dy, and Tomas Pfister. Learning to prompt for continual learning. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 139-149, 2022.</note>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Large scale incremental learning</title>
		<author>
			<persName><forename type="first">Yue</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yinpeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lijuan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuancheng</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zicheng</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yandong</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yun</forename><surname>Fu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
				<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2019-06">June 2019</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Yue Wu, Yinpeng Chen, Lijuan Wang, Yuancheng Ye, Zicheng Liu, Yandong Guo, and Yun Fu. Large scale incre- mental learning. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), June 2019.</note>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<author>
			<persName><forename type="first">Yuxin</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<title level="m">Group normalization</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Yuxin Wu and Kaiming He. Group normalization, 2018.</note>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Continual learning through synaptic intelligence</title>
		<author>
			<persName><forename type="first">Friedemann</forename><surname>Zenke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ben</forename><surname>Poole</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Surya</forename><surname>Ganguli</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Friedemann Zenke, Ben Poole, and Surya Ganguli. Continual learning through synaptic intelligence, 2017.</note>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Classincremental learning via deep model consolidation</title>
		<author>
			<persName><forename type="first">Junting</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jie</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shalini</forename><surname>Ghosh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dawei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Serafettin</forename><surname>Tasci</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Larry</forename><surname>Heck</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Heming</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">C</forename><surname>Jay Kuo</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Junting Zhang, Jie Zhang, Shalini Ghosh, Dawei Li, Serafettin Tasci, Larry Heck, Heming Zhang, and C. C. Jay Kuo. Class- incremental learning via deep model consolidation, 2020.</note>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Class-incremental learning via deep model consolidation</title>
		<author>
			<persName><forename type="first">Junting</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jie</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shalini</forename><surname>Ghosh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dawei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Serafettin</forename><surname>Tasci</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Larry</forename><forename type="middle">P</forename><surname>Heck</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Heming</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C.-C. Jay</forename><surname>Kuo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Winter Conference on Applications of Computer Vision, WACV 2020</title>
				<meeting><address><addrLine>Snowmass Village, CO, USA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2020">March 1-5, 2020. 2020</date>
			<biblScope unit="page" from="1120" to="1129" />
		</imprint>
	</monogr>
	<note type="raw_reference">Junting Zhang, Jie Zhang, Shalini Ghosh, Dawei Li, Serafettin Tasci, Larry P. Heck, Heming Zhang, and C.-C. Jay Kuo. Class-incremental learning via deep model consolidation. In IEEE Winter Conference on Applications of Computer Vision, WACV 2020, Snowmass Village, CO, USA, March 1-5, 2020, pages 1120-1129. IEEE, 2020.</note>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">Pycil: A python toolbox for class-incremental learning</title>
		<author>
			<persName><forename type="first">Da-Wei</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fu-Yun</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Han-Jia</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">De-Chuan</forename><surname>Zhan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Da-Wei Zhou, Fu-Yun Wang, Han-Jia Ye, and De-Chuan Zhan. Pycil: A python toolbox for class-incremental learning, 2023.</note>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">BERT learns to teach: Knowledge distillation with meta learning</title>
		<author>
			<persName><forename type="first">Wangchunshu</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Canwen</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Julian</forename><forename type="middle">J</forename><surname>Mcauley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics</title>
		<title level="s">Long Papers</title>
		<editor>
			<persName><forename type="first">Preslav</forename><surname>Smaranda Muresan</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Aline</forename><surname>Nakov</surname></persName>
		</editor>
		<editor>
			<persName><surname>Villavicencio</surname></persName>
		</editor>
		<meeting>the 60th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Dublin, Ireland</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2022">May 22-27, 2022. 2022</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="7037" to="7049" />
		</imprint>
	</monogr>
	<note>ACL 2022</note>
	<note type="raw_reference">Wangchunshu Zhou, Canwen Xu, and Julian J. McAuley. BERT learns to teach: Knowledge distillation with meta learn- ing. In Smaranda Muresan, Preslav Nakov, and Aline Villavi- cencio, editors, Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), ACL 2022, Dublin, Ireland, May 22-27, 2022, pages 7037-7049. Association for Computational Linguistics, 2022.</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
