<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Conditional Automated Channel Pruning for Deep Neural Networks</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Yixin</forename><surname>Liu</surname></persName>
							<affiliation key="aff0">
								<note type="raw_affiliation"><label>†</label> South China University of Technology, ‡ Monash University</note>
								<orgName type="institution" key="instit1">South China University of Technology</orgName>
								<orgName type="institution" key="instit2">‡ Monash University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Yong</forename><surname>Guo</surname></persName>
							<affiliation key="aff0">
								<note type="raw_affiliation"><label>†</label> South China University of Technology, ‡ Monash University</note>
								<orgName type="institution" key="instit1">South China University of Technology</orgName>
								<orgName type="institution" key="instit2">‡ Monash University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Zichang</forename><surname>Liu</surname></persName>
							<affiliation key="aff0">
								<note type="raw_affiliation"><label>†</label> South China University of Technology, ‡ Monash University</note>
								<orgName type="institution" key="instit1">South China University of Technology</orgName>
								<orgName type="institution" key="instit2">‡ Monash University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Haohua</forename><surname>Liu</surname></persName>
							<affiliation key="aff0">
								<note type="raw_affiliation"><label>†</label> South China University of Technology, ‡ Monash University</note>
								<orgName type="institution" key="instit1">South China University of Technology</orgName>
								<orgName type="institution" key="instit2">‡ Monash University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Jingjie</forename><surname>Zhang</surname></persName>
							<affiliation key="aff0">
								<note type="raw_affiliation"><label>†</label> South China University of Technology, ‡ Monash University</note>
								<orgName type="institution" key="instit1">South China University of Technology</orgName>
								<orgName type="institution" key="instit2">‡ Monash University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Zejun</forename><surname>Chen</surname></persName>
							<affiliation key="aff0">
								<note type="raw_affiliation"><label>†</label> South China University of Technology, ‡ Monash University</note>
								<orgName type="institution" key="instit1">South China University of Technology</orgName>
								<orgName type="institution" key="instit2">‡ Monash University</orgName>
							</affiliation>
						</author>
						<author role="corresp">
							<persName><forename type="first">Jing</forename><surname>Liu</surname></persName>
							<email>liujing95@outlook.com</email>
							<affiliation key="aff0">
								<note type="raw_affiliation"><label>†</label> South China University of Technology, ‡ Monash University</note>
								<orgName type="institution" key="instit1">South China University of Technology</orgName>
								<orgName type="institution" key="instit2">‡ Monash University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Jian</forename><surname>Chen</surname></persName>
							<affiliation key="aff0">
								<note type="raw_affiliation"><label>†</label> South China University of Technology, ‡ Monash University</note>
								<orgName type="institution" key="instit1">South China University of Technology</orgName>
								<orgName type="institution" key="instit2">‡ Monash University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Conditional Automated Channel Pruning for Deep Neural Networks</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">FB9E5C39158F819272F73EFF0B148491</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2023-10-13T09:56+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p><s coords="1,63.96,238.39,218.57,7.77;1,63.96,248.35,130.43,7.77">Model compression aims to reduce the redundancy of deep networks to obtain compact models.</s><s coords="1,196.83,248.35,85.71,7.77;1,63.96,258.32,218.57,7.77;1,63.96,268.28,198.22,7.77">Recently, channel pruning has become one of the predominant compression methods to deploy deep models on resource-constrained devices.</s><s coords="1,264.10,268.28,18.43,7.77;1,63.96,278.24,218.57,7.77;1,63.96,288.21,218.57,7.77;1,63.96,298.17,29.64,7.77">Most channel pruning methods often use a fixed compression rate for all the layers of the model, which, however, may not be optimal.</s><s coords="1,95.51,298.17,187.03,7.77;1,63.96,308.13,218.57,7.77;1,63.96,318.09,83.82,7.77">To address this issue, given a target compression rate for the whole model, one can search for the optimal compression rate for each layer.</s><s coords="1,150.00,318.09,132.54,7.77;1,63.96,328.06,194.88,7.77">Nevertheless, these methods perform channel pruning for a specific target compression rate.</s><s coords="1,261.13,328.06,21.41,7.77;1,63.96,338.02,218.57,7.77;1,63.96,347.98,218.57,7.77;1,63.96,357.94,91.81,7.77">When we consider multiple compression rates, they have to repeat the channel pruning process multiple times, which is very inefficient yet unnecessary.</s><s coords="1,158.89,357.94,123.65,7.77;1,63.96,367.91,218.57,7.77;1,63.96,377.87,218.57,7.77;1,63.96,387.83,162.03,7.77">To address this issue, we propose a Conditional Automated Channel Pruning (CACP) method to obtain the compressed models with different compression rates through single channel pruning process.</s><s coords="1,228.16,387.83,54.37,7.77;1,63.96,397.79,218.57,7.77;1,63.96,407.76,218.57,7.77;1,63.96,417.72,24.66,7.77">To this end, we develop a conditional model that takes an arbitrary compression rate as input and outputs the corresponding compressed model.</s><s coords="1,90.52,417.72,192.01,7.77;1,63.96,427.68,218.57,7.77;1,63.96,437.64,218.57,7.77;1,63.96,447.61,117.11,7.77">In the experiments, the resultant models with different compression rates consistently outperform the models compressed by existing methods with a channel pruning process for each target compression rate.</s></p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head coords="1,140.81,475.14,64.88,10.75">Introduction</head><p><s coords="1,54.00,492.15,238.50,8.64;1,54.00,503.11,238.50,8.64;1,54.00,514.07,78.02,8.64">Deep Neural Networks (DNNs) has achieved great success in many tasks, e.g., image classification, face recognition, and video analysis.</s><s coords="1,135.47,514.07,157.03,8.64;1,54.00,525.03,238.50,8.64;1,54.00,535.99,238.50,8.64;1,54.00,546.95,145.20,8.64">However, deep models often contain a large number of parameters and require high computational resources.As a result, it is hard to apply deep learning methods to resource-constrained devices.</s><s coords="1,202.04,546.95,90.46,8.64;1,54.00,557.91,238.50,8.64;1,54.00,568.87,221.53,8.64">To address this, model compression has been an effective way to reduce redundancy of deep networks <ref type="bibr" coords="1,125.00,568.87,80.80,8.64" target="#b5">(Zhuang et al. 2018;</ref><ref type="bibr" coords="1,208.29,568.87,62.95,8.64" target="#b0">Guo et al. 2019)</ref>.</s></p><p><s coords="1,63.96,580.26,228.54,8.64;1,54.00,591.22,186.37,8.64">Recently, channel pruning has been one of the predominant approaches for deep model compression.</s><s coords="1,243.62,591.22,48.89,8.64;1,54.00,602.18,238.50,8.64;1,54.00,613.13,115.84,8.64">Specifically, channel pruning aims to remove the redundant channels of the layers in a deep network.</s><s coords="1,172.61,613.13,119.89,8.64;1,54.00,624.09,238.50,8.64;1,54.00,635.05,238.50,8.64;1,54.00,646.01,86.39,8.64">To obtain the models with the desired compactness, most methods apply a fixed compression rate to all the layers of the deep model <ref type="bibr" coords="1,235.46,635.05,57.05,8.64;1,54.00,646.01,22.69,8.64" target="#b5">(Zhuang et al. 2018;</ref><ref type="bibr" coords="1,79.07,646.01,52.74,8.64" target="#b4">He et al. 2019</ref>).</s><s coords="1,142.76,646.01,149.75,8.64;1,54.00,656.97,93.22,8.64">However, not all layers have the same amount of redundancy.</s><s coords="1,150.66,656.97,141.85,8.64">Thus, pruning the same proportion</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head coords="1,491.04,220.82,59.43,6.30">Compressed Model</head><formula xml:id="formula_0" coords="1,319.50,245.91,234.44,115.51">M ) * M ) + M ) , M Figure 1:</formula><p><s coords="1,359.40,352.78,151.35,8.64">The overview of the proposed CACP.</s><s coords="1,513.82,352.78,44.18,8.64;1,319.50,363.42,238.50,8.96;1,319.50,374.70,238.50,8.64;1,319.50,385.66,140.92,8.64">Our CACP takes a pretrained model M and a target compression rate as inputs and outputs the compressed model that satisfies the considered target compression rate.</s></p><p><s coords="1,319.50,417.97,192.71,8.64">of channels for all the layer may be suboptimal.</s><s coords="1,515.04,417.97,42.97,8.64;1,319.50,428.93,238.50,8.64;1,319.50,439.89,238.50,8.64;1,319.50,450.84,57.38,8.64">To address this issue, <ref type="bibr" coords="1,363.08,428.93,70.17,8.64" target="#b3">He et al. (2018b)</ref> propose an automatic pruning method AMC that searches for the optimal compression rate for each layer.</s><s coords="1,379.72,450.84,178.28,8.64;1,319.50,461.80,197.95,8.64">However, these methods only perform channel pruning for a specific target compression rate.</s><s coords="1,520.02,461.80,37.99,8.64;1,319.50,472.76,238.50,8.64;1,319.50,483.72,238.50,8.64;1,319.50,494.68,145.78,8.64">When we consider different compression rates, they have to repeat the channel pruning for each compression rate, which is very time-consuming and labor-intensive.</s><s coords="1,329.46,505.64,228.54,8.64;1,319.50,516.60,238.50,8.64;1,319.50,527.56,177.12,8.64">To address the above issue, we seek to train a single model to obtain the compressed models with different target compression rates simultaneously (See Figure <ref type="figure" coords="1,485.84,527.56,3.60,8.64">1</ref>).</s><s coords="1,498.68,527.56,59.33,8.64;1,319.50,538.52,238.50,8.64;1,319.50,549.47,238.50,8.64;1,319.50,560.43,238.50,8.64;1,319.50,571.39,25.74,8.64">To this end, we propose a Conditional Automated Channel Pruning (CACP) method that takes the target compression rate as the condition to obtain the compressed model satisfying this condition.</s><s coords="1,348.40,571.39,209.60,8.64;1,319.50,582.35,238.50,8.64;1,319.50,593.31,207.80,8.64">Extensive experiments show that the resultant models obtained by our CACP significantly outperform the compressed models by the considered baseline methods.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head coords="1,333.27,614.27,210.97,10.75">Conditional Automated Channel Pruning</head><p><s coords="1,319.50,629.76,238.50,8.64;1,319.50,640.72,238.50,8.64;1,319.50,651.68,238.50,8.64;1,319.50,662.64,17.42,8.64">Existing channel pruning methods may either apply a fixed compression rate to all the layers or search for the optimal compression rate for each layer to satisfy the overall target rate.</s><s coords="1,340.15,662.64,217.85,8.64;1,319.50,673.60,238.50,8.64;1,319.50,684.56,238.50,8.64;1,319.50,695.51,107.83,8.64">However, when we consider different target compression rates, we have to repeat the channel pruning process to obtain the models satisfying these target rates, which is very inefficient yet unnecessary.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head coords="1,18.34,393.90,18.00,166.10;1,18.34,321.14,18.00,62.76;1,18.34,291.14,18.00,20.00;1,18.34,256.14,18.00,30.00;1,18.34,211.14,18.00,40.00">arXiv:2009.09724v2 [cs.CV] 27 Sep 2020</head><p><s coords="2,63.96,57.48,228.53,8.64;2,54.00,68.44,238.50,8.64;2,54.00,79.39,238.50,8.64">To address this issue, we propose a Conditional Automated Channel Pruning (CACP) method that automatically compresses models with different target compression rates.</s><s coords="2,54.00,90.35,238.50,8.64;2,54.00,101.31,238.50,8.64;2,54.00,112.27,62.18,8.64">In this sense, we only train the CACP model once to obtain the models with different computational cost (e.g., FLOPs) simultaneously.</s><s coords="2,118.44,112.27,174.07,8.64;2,54.00,123.23,238.50,8.64;2,54.00,134.19,238.50,8.64;2,54.00,145.15,21.87,8.64">To achieve this, we treat the target compression rate as a condition and train a conditional model to obtain the compressed models satisfying the desired conditions.</s><s coords="2,78.85,144.83,213.65,8.96;2,54.00,155.79,238.50,8.96;2,54.00,166.75,238.50,9.65;2,54.00,178.03,131.06,8.64">Given a pretrained model M and any arbitrary target compression rate β, we seek to obtain the compressed model by M β = CACP(M, β; θ), where θ denotes the learnable parameters of the CACP model.</s><s coords="2,188.53,178.03,103.98,8.64;2,54.00,188.98,238.50,8.64;2,54.00,199.94,238.50,8.64;2,54.00,210.58,182.84,8.96">To enable CACP to compress models under different target compression rates, we train the model by maximizing the expected reward over a distribution of compression rate, i.e., β∼p(•).</s><s coords="2,239.76,210.90,52.74,8.64;2,54.00,221.54,217.36,8.96">In this paper, we assume p(•) to be a uniformly discrete distribution.</s><s coords="2,273.89,221.86,18.61,8.64;2,54.00,232.82,238.50,8.64;2,54.00,243.46,20.65,8.74">During training, we use the validation accuracy as the reward R(•).</s><s coords="2,77.14,243.78,160.91,8.64">Thus, the objective can be formulated as</s></p><formula xml:id="formula_1" coords="2,124.58,265.38,97.35,14.66">max θ E β∼p(•) [R (M β )] .</formula><p><s coords="2,280.88,265.70,11.62,8.64">(1)</s></p><p><s coords="2,63.96,282.69,228.54,8.64;2,54.00,293.65,44.30,8.64">However, directly obtaining the compressed models is non-trivial.</s><s coords="2,101.66,293.65,190.84,8.64;2,54.00,304.61,238.50,8.64;2,54.00,315.57,94.93,8.64">Instead, we seek to determine the compression rate for each layer and perform channel selection to obtain the compressed models.</s><s coords="2,151.16,315.57,141.35,8.64;2,54.00,326.53,238.50,8.64;2,54.00,337.49,133.55,8.64">Following <ref type="bibr" coords="2,193.95,315.57,64.81,8.64" target="#b3">(He et al. 2018b)</ref>, we use reinforcement learning to search for the optimal compression rate in a layer-wise manner.</s><s coords="2,190.90,337.49,101.61,8.64;2,54.00,348.45,238.50,8.64;2,54.00,359.41,238.50,8.64;2,54.00,370.37,74.62,8.64">As for channel selection, we use L1 norm to measure the importance of channels <ref type="bibr" coords="2,272.59,348.45,19.92,8.64;2,54.00,359.41,44.27,8.64" target="#b1">(Han et al. 2015)</ref> and prune the unimportant channels based on the compression rates.</s><s coords="2,131.58,370.37,160.93,8.64;2,54.00,381.32,238.50,8.64;2,54.00,392.28,238.50,8.64;2,54.00,403.24,135.98,8.64">Note that we have to limit the compression rate when we find that the resultant model cannot reach the target compression rate even though we remove all the channels of the following layers.</s><s coords="2,193.96,402.92,98.54,9.65;2,54.00,416.48,121.65,8.96;2,175.93,413.35,11.10,6.12;2,175.65,421.53,2.52,6.12;2,190.13,416.80,102.38,8.64;2,54.00,427.44,238.50,8.96;2,54.00,438.40,135.11,8.96">Let C l be the computational cost of this layer, and D (β) l be the lower bound of the computational cost that should be reduced for the l-th layer to achieve the compression rate β.</s><s coords="2,191.35,438.72,101.15,8.64;2,54.00,449.68,238.50,8.64;2,54.00,460.32,173.04,8.96">To ensure that we can obtain the model satisfying the overall compression rate, the compression rate for the l-th layer becomes</s></p><formula xml:id="formula_2" coords="2,97.26,474.97,195.24,14.30">α (β) l = max f (S l , β; θ), D (β) l C l ,<label>(2)</label></formula><p><s coords="2,63.96,496.23,228.54,9.65;2,54.00,507.51,238.50,8.64;2,54.00,518.15,199.52,8.96">where S l denotes the state/features of this layer, f (•) denotes the function that determines the optimal compression rate for layer l under the target compression rate β.</s><s coords="2,255.58,518.15,36.42,9.65;2,54.00,529.42,238.50,8.64;2,54.00,540.06,238.00,9.65;2,54.00,551.34,238.50,8.64;2,54.00,561.98,238.50,9.65;2,54.00,573.26,41.35,8.64">Let α max be the maximum possible compression rate for the following layers, C all be the overall cost of the whole model, C reduced be the total amount of reduced cost in the previous layers, and C rest be the amount of the remaining cost in the following layers.</s><s coords="2,97.71,573.26,194.80,8.64;2,54.00,583.90,122.95,8.96">Thus, the minimum computation cost that should be reduced for layer l becomes</s></p><formula xml:id="formula_3" coords="2,86.32,602.68,173.86,14.30">D (β) l = β • C all − α max • C rest − C reduced .</formula><p><s coords="2,280.88,606.14,11.62,8.64;2,54.00,618.80,113.42,8.64">(3) In this way, based on Eqns.</s><s coords="2,170.89,618.80,121.61,8.64;2,54.00,629.76,238.50,8.64;2,54.00,640.72,37.06,8.64">( <ref type="formula" coords="2,174.76,618.80,3.87,8.64" target="#formula_2">2</ref>) and (3), we can guarantee that the compressed model would satisfy the target compression rate.</s><s coords="2,94.09,640.72,198.41,8.64;2,54.00,651.68,238.50,8.64;2,54.00,662.64,238.50,8.64;2,54.00,673.60,159.46,8.64">During inference, based on a well-learned CACP model, we only need to feed in a specific target compression rate as a condition to obtain the optimal compression rates for all the layers under this condition.</s><s coords="2,217.60,673.60,74.91,8.64;2,54.00,684.56,238.50,8.64;2,54.00,695.51,220.89,8.64">Then, we perform channel selection to obtain the desired compressed model with the desired target compression rate (See Figure <ref type="figure" coords="2,264.10,695.51,3.60,8.64">1</ref>).</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head coords="2,406.21,55.71,65.08,10.75">Experiments</head><p><s coords="2,319.50,70.59,238.50,8.64;2,319.50,81.55,90.78,8.64">In this section, we empirically evaluate the proposed CACP method on CIFAR-10.</s><s coords="2,413.57,81.55,144.44,8.64;2,319.50,92.51,238.50,8.64;2,319.50,103.47,214.94,8.64">Several state-of-the-art methods are adopted as the baselines, including SFP <ref type="bibr" coords="2,485.62,92.51,68.08,8.64" target="#b2">(He et al. 2018a)</ref>, DCP <ref type="bibr" coords="2,340.84,103.47,77.95,8.64" target="#b5">(Zhuang et al. 2018)</ref>, and AMC <ref type="bibr" coords="2,466.04,103.47,64.02,8.64" target="#b3">(He et al. 2018b)</ref>.</s><s coords="2,536.42,103.47,21.59,8.64;2,319.50,114.43,238.50,8.64;2,319.50,125.39,238.50,8.64;2,319.50,136.35,21.30,8.64">From Table <ref type="table" coords="2,343.95,114.43,3.74,8.64" target="#tab_1">1</ref>, the models obtained by CACP significantly outperform the considered baseline methods with all compression rates.</s><s coords="2,343.55,136.35,214.46,8.64;2,319.50,147.31,238.50,8.64;2,319.50,158.26,173.10,8.64">It is worth noting that all the resultant models are obtained through a single channel pruning process, which is essentially different from existing methods.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head coords="2,410.18,326.15,57.13,10.75">Conclusion</head><p><s coords="2,319.50,341.03,238.50,8.64;2,319.50,351.99,238.50,8.64;2,319.50,362.95,238.50,8.64;2,319.50,373.90,171.01,8.64">In this paper, we have proposed a Conditional Automated Channel Pruning method (CACP) that obtains the compressed models with different target compression rates through a single channel pruning process.</s><s coords="2,494.00,373.90,64.00,8.64;2,319.50,384.86,238.50,8.64;2,319.50,395.82,221.22,8.64">Specifically, we treat the target compression rate as a condition and train a conditional pruning model to compress deep networks.</s><s coords="2,543.62,395.82,14.39,8.64;2,319.50,406.78,238.50,8.64;2,319.50,417.74,238.50,8.64;2,319.50,428.70,102.65,8.64">Extensive experiments show that our compressed models with different compression rates consistently outperform the considered baseline methods.</s></p></div><figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="2,319.50,178.39,238.50,123.22"><head>Table 1 :</head><label>1</label><figDesc><div><p><s coords="2,354.10,178.39,203.90,8.64;2,319.50,189.34,56.83,8.64">Comparisons of the compressed ResNet-56 models on CIFAR-10.</s><s coords="2,378.82,189.34,173.75,8.64">"-" denotes the results that are not reported.</s></p></div></figDesc><table coords="2,325.28,212.12,226.94,89.48"><row><cell>Compression Rate</cell><cell>Method</cell><cell cols="3">Acc. (%) #FLOPs ↓ (%) #Params. ↓ (%)</cell></row><row><cell>0</cell><cell>Baseline</cell><cell>93.80</cell><cell>0</cell><cell>0</cell></row><row><cell></cell><cell>SFP</cell><cell>93.59</cell><cell>28.4</cell><cell>-</cell></row><row><cell>0.3</cell><cell>AMC</cell><cell>93.75</cell><cell>31.1</cell><cell>19.5</cell></row><row><cell></cell><cell>CACP (Ours)</cell><cell>93.98</cell><cell>30.2</cell><cell>28.2</cell></row><row><cell></cell><cell>SFP</cell><cell>92.57</cell><cell>52.6</cell><cell>-</cell></row><row><cell>0.5</cell><cell>DCP AMC</cell><cell>93.77 93.57</cell><cell>50.6 49.9</cell><cell>49.7 44.6</cell></row><row><cell></cell><cell>CACP (Ours)</cell><cell>93.84</cell><cell>50.3</cell><cell>45.9</cell></row><row><cell></cell><cell>DCP</cell><cell>92.98</cell><cell>68.4</cell><cell>68.3</cell></row><row><cell>0.7</cell><cell>AMC</cell><cell>92.61</cell><cell>69.9</cell><cell>69.8</cell></row><row><cell></cell><cell>CACP (Ours)</cell><cell>93.13</cell><cell>69.9</cell><cell>71.2</cell></row></table></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Nat: Neural architecture transformer for accurate and compact architectures</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note>In NeurIPS</note>
	<note type="raw_reference">Guo, Y.; Zheng, Y.; Tan, M.; Chen, Q.; Chen, J.; Zhao, P.; and Huang, J. 2019. Nat: Neural architecture transformer for accurate and compact architectures. In NeurIPS.</note>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Learning both weights and connections for efficient neural network</title>
		<author>
			<persName><forename type="first">S</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Pool</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Dally</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
	<note>In NeurIPS</note>
	<note type="raw_reference">Han, S.; Pool, J.; Tran, J.; and Dally, W. 2015. Learning both weights and connections for efficient neural network. In NeurIPS.</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Soft Filter Pruning for Accelerating Deep Convolutional Neural Networks</title>
		<author>
			<persName><forename type="first">Y</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCAI</title>
				<imprint>
			<date type="published" when="2018">2018a</date>
		</imprint>
	</monogr>
	<note type="raw_reference">He, Y.; Kang, G.; Dong, X.; Fu, Y.; and Yang, Y. 2018a. Soft Filter Pruning for Accelerating Deep Convolutional Neural Networks. In IJCAI.</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Amc: Automl for model compression and acceleration on mobile devices</title>
		<author>
			<persName><forename type="first">Y</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L.-J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
				<imprint>
			<date type="published" when="2018">2018b</date>
		</imprint>
	</monogr>
	<note type="raw_reference">He, Y.; Lin, J.; Liu, Z.; Wang, H.; Li, L.-J.; and Han, S. 2018b. Amc: Automl for model compression and acceler- ation on mobile devices. In ECCV.</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Filter pruning via geometric median for deep convolutional neural networks acceleration</title>
		<author>
			<persName><forename type="first">Y</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
				<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="4340" to="4349" />
		</imprint>
	</monogr>
	<note type="raw_reference">He, Y.; Liu, P.; Wang, Z.; Hu, Z.; and Yang, Y. 2019. Filter pruning via geometric median for deep convolutional neural networks acceleration. In CVPR, 4340-4349.</note>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Discrimination-aware channel pruning for deep neural networks</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhuang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Zhuang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note>In NeurIPS</note>
	<note type="raw_reference">Zhuang, Z.; Tan, M.; Zhuang, B.; Liu, J.; Guo, Y.; Wu, Q.; Huang, J.; and Zhu, J. 2018. Discrimination-aware channel pruning for deep neural networks. In NeurIPS.</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
