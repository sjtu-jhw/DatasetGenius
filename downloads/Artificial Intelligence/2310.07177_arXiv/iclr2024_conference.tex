
\documentclass{article} %
\usepackage{iclr2024_conference,times}

\input{math_commands.tex}

\usepackage{hyperref}
\usepackage{url}
\usepackage{enumitem}
\usepackage{amssymb}
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage[capitalize,noabbrev]{cleveref}

\usepackage{graphicx}
\usepackage{booktabs}
\usepackage{multirow}
\usepackage{listings}
\usepackage{xspace}
\usepackage{inconsolata}
\lstset{
        language=Python,
        captionpos=b,
        xleftmargin=.01in,
        keywordstyle=\color{blue},
        showstringspaces=false,
		breaklines=true,
		numbersep=3pt, 
        numbers=left,
		tabsize=2,  
		numberstyle=\tiny\color{gray},
        commentstyle=\color{brown},
        basicstyle=\scriptsize\ttfamily,
        escapeinside={@}{@}
}


\newcommand{\tool}{OSD\xspace} %
\newcommand{\zhijie}[1]{\textcolor{green}{zhijie:#1}}
\newcommand{\lily}[1]{\textcolor{orange}{lily:#1}}
\newcommand{\pb}[1]{\textcolor{blue}{pb:#1}}
\newcommand{\ion}[1]{\textcolor{red}{ion:#1}}
\newcommand{\hao}[1]{\textcolor{orange}{hao:#1}}
\newcommand{\alvin}[1]{\textcolor{brown}{alvin: #1}}
\newcommand{\woosuk}[1]{\textcolor{brown}{woosuk: #1}}
\newcommand{\zhuohan}[1]{\textcolor{brown}{zhuohan: #1}}


\title{Online Speculative Decoding}



\author{
    \centerline{\textbf{Xiaoxuan Liu$\,\:$$^{1}$ \hspace{.03mm}
    $\qquad$
    Lanxiang Hu$^{2}$$\qquad$
    Peter Bailis$^{3}$$\qquad$
    Ion Stoica$^{1}$
    }}\\\vspace{1mm}
    \centerline{\textbf{Zhijie Deng$^{4}$$\thanks{Corresponding author}$ $\qquad$
    Alvin Cheung$^{1}$$\qquad$ Hao Zhang$^{2*}$}}\\
    \centerline{$^{1}$~UC Berkeley\qquad $^{2}$~UCSD\qquad $^{3}$~Sisu Data\qquad $^{4}$~SJTU}\\
    \centerline{\texttt{\footnotesize{\{xiaoxuanliu, istoica, akcheung\}@cs.berkeley.edu}}} \\
    \centerline{\texttt{\footnotesize{\{lah003, haozhang\}@ucsd.edu, peter@sisudata.com, zhijied@sjtu.edu.cn}}}
}


\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}

\newcommand\todo[1]{\ifthenelse{\equal{\showcomments}{yes}}{{\color{red} TODO: #1}}{\ignorespaces}}

\iclrfinalcopy %
\begin{document}


\maketitle

\begin{abstract}
Speculative decoding is a pivotal technique to accelerate the inference of large language models (LLMs) by employing a smaller draft model to predict the target model's outputs. However, its efficacy can be limited due to the low predictive accuracy of the draft model, particularly when faced with diverse text inputs and a significant capability gap between the draft and target models. 
We introduce online speculative decoding to address this challenge. 
The main idea is to continually update (multiple) draft model(s) on observed user query data using the abundant excess computational power in an LLM serving cluster.
Given that LLM inference is memory-bounded, the surplus computational power in a typical LLM serving cluster can be repurposed for online retraining of draft models, thereby making the training cost-neutral.
Since the query distribution of an LLM service is relatively simple, retraining on query distribution enables the draft model to more accurately predict the target model's outputs, particularly on data originating from query distributions.
As the draft model evolves online, it aligns with the query distribution in real time, mitigating distribution shifts. 
We develop a prototype of online speculative decoding based on online knowledge distillation and evaluate it using both synthetic and real query data on several popular LLMs. The results show a substantial increase in the token acceptance rate by 0.1 to 0.65, which translates into 1.22$\times$ to 3.06$\times$ latency reduction. Code is available at \texttt{https://github.com/LiuXiaoxuanPKU/OSD}.



\end{abstract}
\input{1-introduction}
\input{2-relatedwork}
\input{3-methodology}
\input{4-experiment}
\input{5-conclusion}
\bibliography{iclr2024_conference}
\bibliographystyle{iclr2024_conference}

\input{6-appendix}


\end{document}
