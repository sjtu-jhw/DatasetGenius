\section{Related Work}
LLMs have become pervasive in today's AI applications, underscoring the importance of optimizing LLM inference. 
Numerous system optimizations have been developed to optimize the throughput of LLM serving~\citep{yu2022orca,kwon2023efficient}. This paper particularly concentrates on a significant strand of research, speculative decoding, aimed at reducing the latency of LLM inference.

\noindent \textbf{Speculative decoding.}
Speculative decoding~\citep{leviathan2023fast,chen2023accelerating} accelerates LLM decoding by employing a (small) draft model to predict the outputs of the larger target model, which are then verified by the target model. Typically, the draft model, while having fewer parameters, is pretrained using the same training data as the target mode, resulting in a negotiable inference cost but with compromised capability. 
If the draft model can correctly predict more than one token per verification step, the memory I/O for accessing the model weights and KV cache at inference is amortized across multiple output tokens, thereby reduces latency, especially since LLM inference is often constrained by GPU HBM bandwidth.
The efficacy of speculative decoding largely hinges on the draft model's ability to accurately predict the target model's outputs. Existing work improves the speculation accuracy by using multiple collectively boosted~\citep{miao2023specinfer} or staged~\citep{spector2023accelerating} draft models, or retraining the target model with auxiliary prediction heads as a draft model~\citep{medusa, stern2018blockwise}. These methods predominantly assume a static draft model post-deployment. In contrast, our work introduces a framework that actively adapts the draft model to the evolving user query distribution on the fly, irrespective of the draft model's construction.

\noindent \textbf{Distillation for auto-regressive models.}
Knowledge distillation (KD) is a framework to generate smaller models that emulate the performance of larger models. However, KD in its conventional form has been observed to be less effective for LLMs. \cite{gu2023knowledge} extend KD to autoregressive LLMs by decoding from the student model and optimizing the reserve KL divergence between students and teachers.
% Optimizing the reserve KL diversity prompt the student to seek major modes in the teacher's distribution, wh 
Further, \cite{agarwal2023gkd} introduce generalized knowledge distillation (GKD) to optimize a linear combination of the forward KL and reverse KL between teacher and student, using a blend of teacher- and student-sampled data. 
Drawing inspiration from both works, our paper applies KD to speculative decoding for LLMs. We empirically determine the most effective KD variant for maximizing the draft model's accuracy, and extend it to dynamically generate draft models for online LLM services.

% \noindent \textbf{Other optimizations for LLM inference.} Besides speculative decoding, there are a variety of system optimizations for LLM serving. Notable methods include: continuous batching~\cite{}, paged attention~\cite{}, chunked prefill~\cite{}. Essentially, these methods improve the system scheduling and memory allocation, which will translate into improved throughput. Our method, on the other hand, reduces the latency of LLM inference by reducing the HBM memory access, which is considered as a major bottleneck at LLM decoding.