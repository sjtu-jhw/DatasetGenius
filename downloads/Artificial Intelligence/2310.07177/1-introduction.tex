\section{Introduction}

Large language models (LLMs) such as GPT-4~\citep{openai2023gpt}, Claude~\citep{bai2022training}, and Llama~\citep{touvron2023llama,touvron2023llama2} are rapidly reinventing today's applications. 
% \pb{one might argue that they haven't fully penetrated it YET, but perhaps instead that they are ``rapidly reinventing'' or something like that} 
Many companies are racing to deploy LLMs in their vertical domains, such as search, chatbots, and virtual assistants. Since most of these applications demand low latency, optimizing LLM serving latency is of vital importance and can directly translate into better quality of service and cost reduction.

% Paragraph 2: describe an LLM service and speculative decoding.
The latency of today's LLM service is unfortunately very high. This is primarily because serving a user query requires multiple serial evaluations of the LLM, each generating only one token of the response.
An emerging solution to reduce the latency is speculative decoding. Speculative decoding employs a smaller model to speculate multiple output tokens of the target (large) model, then lets the target LLM verify these speculations in parallel. Then, if the verification of a token fails, the large model must recompute from that point. Therefore, the performance of speculative decoding primarily depends on the speculation accuracy of the small model. In the presence of diverse text inputs, %\alvin{what does diverse mean? give examples} 
the accuracy of existing speculative decoding methods is unfortunately not very high, due to the capability gap between the draft and target model. Employing a larger, more accurate model however defeats the purpose of speculative decoding as it potentially increases latency.

% Our idea and motivation
To address this challenge, we introduce a novel method, \emph{online speculative decoding}, specifically designed for online LLM services.
The method leverages the abundant redundant compute, termed as ``spare flops,'' available in a typical LLM serving cluster to continuously retrain (multiple) small draft models through online learning on query data posted to the LLM service. Our approach is simple and offers several significant advantages.
% 1. argue it is easy to learn
First, user queries to a specific LLM service often exhibit a common domain-specific distribution~\citep{zheng2023lmsys}, reflecting shared usage patterns.
While accurately speculating the larger model's outputs on \emph{any diverse input} is challenging, it is feasible to enhance the draft model's prediction accuracy, \emph{only for similar inputs posted to the service}, characterized by the query distribution. This can be achieved by finetuning the draft model on user query distribution or finetuning multiple draft models, each on a cluster of the query distribution, and selecting the appropriately specialized draft model to speculate based on the class of inputs they are trained on. As shown in~\S\ref{sec:eval:online_evaluation}, we show that it is possible to train multiple draft models, each for a different language or topic. 
% 2. argue it is free
Second, the primary bottleneck for transformer-based LLM inference is the accelerator's memory bandwidth, as generating each word requires loading the model weights from HBM to SRAM as well as reading the KV cache on all previous words. 
This results in a substantial amount of unused compute, especially during non-spike traffic hours~\citep{spector2023accelerating,chen2023accelerating,kwon2023efficient}, in an LLM serving cluster. We demonstrate that these spare FLOPs can be effectively repurposed for online retraining of draft models, with inconspicuous retraining cost (\S\ref{sec:analysis}).
% \alvin{what does hidden mean? overlapped with inference?}
% 3. argue it can handle distribution shift.
Third, since tuning is performed online, the draft models continuously evolve over time based on the observed query data, which ensures high speculation accuracy even when faced with shifts in query distribution.
% Based on these observations, this paper develops a new framework, \emph{online speculative sampling}, to improve the efficiency of online LLM serving. The key idea is simple: In an online LLM service, we can leverage the rich spare flops to continuously retrain the draft model to emulate the outputs of the target model on query distributions. Due to online retraining, the draft model evolves over time based on observed user query data so far. Hence, it can effectively capture the temporal shift on query distributions, resulting in improved speculation accuracy compared to existing speculative sampling methods with static draft models.



% generates responses serially one word at a time. Since generating each word needs to access the transformers' KV cache of all previous words,  today's LLM serving is bottlenecked by the accelerators' memory bandwidth, instead of arithmetic computations \ion{reference?}. As a consequence, concurrent LLM serving clusters often have many \emph{unused compute}, which we call ``spare flops.'' 

% Existing work has proposed \emph{speculative sampling}(SpS) to utilize these sparse flops, by speculating the outputs of the target model using a small draft model, which will later be verified by the target model. While SpS opportunistically reduces the latency, it assumes the draft model is constructed once before deployment, and fixed throughout serving.

% The aforementioned characteristic, that an LLM serving cluster often has many spare flops, is also a key motivation for our paper. \pb{consider a stronger topic sentence â€“ like ``In this work, we show how to utilize these spare flops to even greater effect by leveraging two key observations about today's LLM serving system.''}\ion{I think a more direct motivation is: ouput generation is slow because it is one-at-a-time LLMs are expensive to evaluate; applications want lower latency; one solution emerging is speculative decoding; use a smaller model to generate output tokens, and the large model to verify a bunch of output tokens \emph{in parallel}; however, if verification of a token fails, then the large model has to compute it; so the performance of speculative decoding depends on the accuracy of the small model; in the presence of diverse inputs this accuracy is unfortunately not very high; using a larger more accurate model defeats the purpose of speculative decoding; in this paper we propose a solution to address this challenge; tune multiple small models on similar inputs (this increases the accuracy of the model for that input); select the model to perform the speculation based on the input belonging to the class of inputs it was trained on; in a simple case we can tune a per-user or ....  model.} We, however, make two further observations about today's LLM serving systems. First, as LLMs are increasingly deployed in more vertical applications, the user queries posted to a specific service will follow specific query distributions~\cite{}, reflecting the common usage patterns shared across users of the same service. The query distribution may also shift over time depending on usage. While it is difficult to build a small draft model to accurately predict a larger model's output on \emph{any input} due to the capacity difference between the draft and target models, it is possible to let the draft model emulate the target model's outputs \emph{only on inputs posted by a similar group of users}, characterized by the query distribution. 
% Second, despite the introduction of draft models, the spare flops in an LLM serving system are still substantially underutilized, especially in non-spike traffic windows -- those flops are sufficiently ample to even \emph{train} potentially draft model(s) (see section~\ref{})

Based on these insights, we develop an online speculative decoding framework to improve the efficiency of online LLM serving. 
% The key idea is simple: In an online LLM service, we can leverage the rich spare flops to continuously retrain the draft model to emulate the outputs of the target model on query distributions. Due to online retraining, the draft model evolves over time based on observed user query data so far. Hence, it can effectively capture the temporal shift on query distributions, resulting in improved speculation accuracy compared to existing speculative sampling methods with static draft models. 
% Figure~\ref{} illustrates its high-level workflow. 
To align the draft model with the target model on a newly observed user query, we develop a new online learning algorithm based on Generalized Knowledge Distillation (GKD)~\citep{gu2023knowledge,agarwal2023gkd}. The algorithm keeps track of the recent queries that the draft model has speculated incorrectly, and forces the draft model to emulate the target model's outputs on these queries. The algorithm performs GKD-based gradient update opportunistically only when spare flops are available, hiding the overhead. 
% \pb{consider ``minimized'' to ``hidden''}

\begin{figure}[h]       
    \centering
    \label{fig:arch}
    \includegraphics[width=0.8\linewidth]{figures/arch.pdf}
    \caption{Online speculative decoding overview. For each prompt, the draft model suggests multiple tokens in a single step. The target model then verifies these tokens, accepting some and rejecting others. If the student proposes incorrect tokens, both the draft and target distributions are stored in a buffer. Once the buffer exceeds a specified threshold, the draft model is updated by calculating the loss between the draft and target distributions using various distance metrics.}
\end{figure}

In summary, this paper makes the following contributions:
\begin{itemize}[leftmargin=*]
    \item We introduce online speculative decoding to reduce LLM serving latency by adapting (multiple) draft models on the fly using query data and knowledge distillation.
    \item We explore various GKD methods for constructing draft models and identify the most effective variants, suggesting them as superior alternatives to existing finetuning methods in offline settings.
    \item Our method demonstrates a significant improvement in token acceptance rate by 10-65\% on diverse datasets, translating to 1.2-3.1$\times$ reduction in latency theoretically, with a negligible additional cost. It surpasses existing methods which construct static draft models using fine-tuning or distillation on offline datasets, and matches the hypothetical accuracy achieved if all query data were available a priori.
    % \item As the amount of spare flops allows, we show that maintaining multiple draft models, each tailored to a query distribution mode, can reduce latency \alvin{by up to XX$\times$}.
\end{itemize}



% paragraph 2: 
% speculative decoding is a promising method to break the memory bandwidth bottleneck;
% Describe speculative decoding
% State: its performance hinges on two things: (1) how to obtain a 

% paragraph 3
% motivation


% paragraph 4