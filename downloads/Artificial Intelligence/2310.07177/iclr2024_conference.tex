
\documentclass{article} % For LaTeX2e
\usepackage{iclr2024_conference,times}

% Optional math commands from https://github.com/goodfeli/dlbook_notation.
\input{math_commands.tex}

\usepackage{hyperref}
\usepackage{url}
\usepackage{enumitem}
\usepackage{amssymb}
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage[capitalize,noabbrev]{cleveref}

\usepackage{graphicx}
\usepackage{booktabs}
\usepackage{multirow}
\usepackage{listings}
\usepackage{xspace}
\usepackage{inconsolata}
\lstset{
        language=Python,
        captionpos=b,
        xleftmargin=.01in,
        keywordstyle=\color{blue},
        showstringspaces=false,
		breaklines=true,
		numbersep=3pt, 
        numbers=left,
		tabsize=2,  
		numberstyle=\tiny\color{gray},
        commentstyle=\color{brown},
        basicstyle=\scriptsize\ttfamily,
        escapeinside={@}{@}
}


\newcommand{\tool}{OSD\xspace} % we should change this later
\newcommand{\zhijie}[1]{\textcolor{green}{zhijie:#1}}
\newcommand{\lily}[1]{\textcolor{orange}{lily:#1}}
\newcommand{\pb}[1]{\textcolor{blue}{pb:#1}}
\newcommand{\ion}[1]{\textcolor{red}{ion:#1}}
\newcommand{\hao}[1]{\textcolor{orange}{hao:#1}}
\newcommand{\alvin}[1]{\textcolor{brown}{alvin: #1}}
\newcommand{\woosuk}[1]{\textcolor{brown}{woosuk: #1}}
\newcommand{\zhuohan}[1]{\textcolor{brown}{zhuohan: #1}}


\title{Online Speculative Decoding}

% Authors must not appear in the submitted version. They should be hidden
% as long as the \iclrfinalcopy macro remains commented out below.
% Non-anonymous submissions will be rejected without review.

% \author{Xiaoxuan Liu
% \quad
% Lanxiang Hu
% % \texttt{\{robot,net\}@wits.ac.za} \\
% \quad
% Peter Bailis 
% \quad
% Ion Stoica
% \\
% Zhijie Deng
% \quad
% Alvin Cheung
% \quad
% Hao Zhang
% }

\author{
    \centerline{\textbf{Xiaoxuan Liu$\,\:$$^{1}$ \hspace{.03mm}
    $\qquad$
    Lanxiang Hu$^{2}$$\qquad$
    Peter Bailis$^{3}$$\qquad$
    Ion Stoica$^{1}$
    }}\\\vspace{1mm}
    \centerline{\textbf{Zhijie Deng$^{4}$$\thanks{Corresponding author}$ $\qquad$
    Alvin Cheung$^{1}$$\qquad$ Hao Zhang$^{2*}$}}\\
    \centerline{$^{1}$~UC Berkeley\qquad $^{2}$~UCSD\qquad $^{3}$~Sisu Data\qquad $^{4}$~SJTU}\\
    \centerline{\texttt{\footnotesize{\{xiaoxuanliu, istoica, akcheung\}@cs.berkeley.edu}}} \\
    \centerline{\texttt{\footnotesize{\{lah003, haozhang\}@ucsd.edu, peter@sisudata.com, zhijied@sjtu.edu.cn}}}
}

% The \author macro works with any number of authors. There are two commands
% used to separate the names and addresses of multiple authors: \And and \AND.
%
% Using \And between authors leaves it to \LaTeX{} to determine where to break
% the lines. Using \AND forces a linebreak at that point. So, if \LaTeX{}
% puts 3 of 4 authors names on the first line, and the last on the second
% line, try using \AND instead of \And before the third author name.

\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}

\newcommand\todo[1]{\ifthenelse{\equal{\showcomments}{yes}}{{\color{red} TODO: #1}}{\ignorespaces}}

\iclrfinalcopy % Uncomment for camera-ready version, but NOT for submission.
\begin{document}


\maketitle

\begin{abstract}
Speculative decoding is a pivotal technique to accelerate the inference of large language models (LLMs) by employing a smaller draft model to predict the target model's outputs. However, its efficacy can be limited due to the low predictive accuracy of the draft model, particularly when faced with diverse text inputs and a significant capability gap between the draft and target models. 
We introduce online speculative decoding to address this challenge. 
The main idea is to continually update (multiple) draft model(s) on observed user query data using the abundant excess computational power in an LLM serving cluster.
% , and route new queries to draft models based on the class of the inputs they have been trained with. 
Given that LLM inference is memory-bounded, the surplus computational power in a typical LLM serving cluster can be repurposed for online retraining of draft models, thereby making the training cost-neutral.
% It demonstrates three key advantages: First, since LLM inference is memory-bounded, there is an abundant surplus of computational power available in a typical LLM serving cluster; these flops can be repurposed for online retraining draft models --- rendering the training cost-neutral. 
Since the query distribution of an LLM service is relatively simple, retraining on query distribution enables the draft model to more accurately predict the target model's outputs, particularly on data originating from query distributions.
% Because the query distribution (of an LLM service) is less complex, it is easier for the draft model to predict target model outputs, \emph{only on data from query distributions}. 
As the draft model evolves online, it aligns with the query distribution in real time, mitigating distribution shifts. 
We develop a prototype of online speculative decoding based on online knowledge distillation and evaluate it using both synthetic and real query data on several popular LLMs. The results show a substantial increase in the token acceptance rate by 0.1 to 0.65, which translates into 1.22$\times$ to 3.06$\times$ latency reduction. Code is available at \texttt{https://github.com/LiuXiaoxuanPKU/OSD}.
%Remarkably, we show that, on real LLM service trace, this latency reduction is achieved without additional expense. %\hao{TODO: fill numbers}



% but hinges on if the draft model can correctly predict the outputs of the target large models. 
% Speculative decoding serves as a crucial technique for accelerating inference in large language models. It capitalizes on the insight that certain tokens are simpler to generate than others. To leverage this, approximation models, which are much smaller than the original large model, can be employed to suggest possible tokens. The larger models work in parallel to validate these suggestions for accuracy. 
% While prior research has primarily centered on offline scenarios—where a smaller model is fine-tuned on a specific dataset to enhance token acceptance rates—such setups often prove impractical in real-world applications where the dataset may not be readily accessible. Furthermore, we contend that the focus should shift from label-based fine-tuning to utilizing knowledge distillation for improved token acceptance. To address these limitations, we introduce Online Speculative Decoding. In this methodology, the smaller model undergoes real-time distillation, leveraging unused computational resources ("spare flops") during the serving phase. This enables the model to learn directly from the query distribution and adapt dynamically to shifts in that distribution. Through extensive experimentation across multiple datasets and diverse model architectures, our approach demonstrates a significant improvement in token acceptance rates.
\end{abstract}
\input{1-introduction}
\input{2-relatedwork}
\input{3-methodology}
\input{4-experiment}
\input{5-conclusion}
\bibliography{iclr2024_conference}
\bibliographystyle{iclr2024_conference}

\input{6-appendix}


\end{document}